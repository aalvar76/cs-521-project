{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import all dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, merge, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## param list\n",
    "\n",
    "## parameter list\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "DROPOUT_RATE    = 0.4\n",
    "HIDDEN_UNITS    = 200\n",
    "\n",
    "# NB_CLASSES      = 2\n",
    "\n",
    "# LEARNING\n",
    "# BATCH_SIZE      = 100\n",
    "# NB_EPOCHS       = 10\n",
    "# RUNS            = 5\n",
    "# VAL_SIZE        = 0.2\n",
    "\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data preprocessing\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string,re\n",
    "\n",
    "dataset=pd.read_csv('train.tsv',delimiter='\\t',encoding='utf-8')\n",
    "dataset.columns=['ID','Label','Statement','Subject','speaker','job_title',\n",
    "           'state_info','pantry_affiliation','barely_true_cnt','false_cnt',\n",
    "           'half_true_cnt','mostly_true_cnt','pants_on_fire_cnt','Context']\n",
    "\n",
    "def preprocessing_txt(dataset):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    rm_punct = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    corpus=[]\n",
    "    for elm in range(0, len(dataset.index)):\n",
    "        res=' '.join([i for i in dataset['Statement'][elm].split() if i not in stop_words])\n",
    "        res=rm_punct.sub(' ', res)\n",
    "        corpus.append(res)\n",
    "    return corpus\n",
    "\n",
    "corpus=preprocessing_txt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n",
      "Shape of data tensor: (10239, 16)\n",
      "Shape of label tensor: (10239, 2)\n"
     ]
    }
   ],
   "source": [
    "## import local dependencies\n",
    "import util\n",
    "from nlp_util import NLP_Task\n",
    "import preprocessing\n",
    "import importlib\n",
    "importlib.reload(preprocessing)\n",
    "\n",
    "\n",
    "## label\n",
    "multi_labels = {'false':0, 'true':1,'pants-fire':2,'barely-true':3,'half-true':4,'mostly-true':5}\n",
    "binary_labels = {'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}\n",
    "\n",
    "dataset['b_label'] = np.array(preprocessing.create_labels(labels=dataset['Label'].values,label_values=binary_labels))\n",
    "dataset['m_label'] = np.array(preprocessing.create_labels(labels=dataset['Label'].values,label_values=multi_labels))\n",
    "\n",
    "texts = corpus\n",
    "labels = dataset['b_label']\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', dataset.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "#\n",
    "# indices = np.arange(dataset.shape[0])\n",
    "# np.random.shuffle(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set \n",
      "[1673. 6518.]\n",
      "[ 441. 1607.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# cv = CountVectorizer()\n",
    "# corpus = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "X_train, X_val, y_train, y_val  = train_test_split(corpus,\n",
    "                                                   labels,train_size=0.8, random_state=123)\n",
    "\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set ')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOVE_DIR = \"/ext/home/analyst/Testground/data/glove\"\n",
    "# embeddings_index = {}\n",
    "# f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))\n",
    "\n",
    "# embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# embedding_layer = Embedding(len(word_index) + 1,\n",
    "#                             EMBEDDING_DIM,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                             trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "t = Tokenizer(num_words=5000) # create tokenizer with a max number of words to take into account according to frequency\n",
    "t.fit_on_texts(X_train) # fit tokenizer with train data\n",
    "X_train = t.texts_to_sequences(X_train)\n",
    "X_dev = t.texts_to_sequences(X_val)\n",
    "X_train = pad_sequences(X_train,maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "X_dev = pad_sequences(X_dev, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    embeddings_index = {}\n",
    "    f = open(gloveFile, encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-300])\n",
    "        coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    }
   ],
   "source": [
    "glove_model = loadGloveModel('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.7628e-01,  1.3999e-01,  9.8519e-02, -6.4019e-01,  3.1988e-02,\n",
       "        1.0066e-01, -1.8673e-01, -3.7129e-01,  5.9740e-01, -2.0405e+00,\n",
       "        2.2368e-01, -2.6314e-02,  7.2408e-01, -4.3829e-01,  4.8886e-01,\n",
       "       -3.5486e-03, -1.0006e-01, -3.0587e-01, -1.5621e-01, -6.8136e-02,\n",
       "        2.1104e-01,  2.9287e-01, -8.8861e-02, -2.0462e-01, -5.7602e-01,\n",
       "        3.4526e-01,  4.1390e-01,  1.7917e-01,  2.5143e-01, -2.2678e-01,\n",
       "       -1.0103e-01,  1.4576e-01,  2.0127e-01,  3.1810e-01, -7.8907e-01,\n",
       "       -2.2194e-01, -2.4833e-01, -1.5103e-02, -2.0050e-01, -2.6441e-02,\n",
       "        1.8551e-01,  3.3782e-01, -3.3543e-01,  8.6117e-01, -4.7083e-02,\n",
       "       -1.7009e-01,  3.0438e-01,  9.4119e-02,  3.2435e-01, -8.1171e-01,\n",
       "        8.8966e-01, -3.9149e-01,  1.6828e-01,  1.4316e-01,  3.6339e-03,\n",
       "       -6.4557e-02,  4.5777e-02, -3.2248e-01,  4.8943e-02,  1.6817e-01,\n",
       "        6.8344e-02,  5.4227e-01,  1.2493e-01,  6.9742e-01, -3.7194e-02,\n",
       "        3.3080e-01, -4.2194e-01,  3.3970e-01,  2.7646e-01, -1.6003e-02,\n",
       "       -2.1827e-01,  4.4535e-01,  3.5379e-01, -2.2089e-02,  2.1375e-01,\n",
       "        4.3267e-01, -3.2897e-01,  9.6165e-02,  3.1265e-01, -3.0528e-01,\n",
       "        2.6126e-01, -6.5364e-01, -7.8014e-01, -2.3154e-01,  1.2113e-01,\n",
       "        3.4896e-01, -5.5444e-01,  4.6619e-01, -1.6520e-01,  1.1611e-01,\n",
       "       -7.6676e-01,  6.9502e-01, -1.5698e-01, -1.2490e-01,  5.6505e-01,\n",
       "        6.4499e-01, -5.7403e-01, -3.3549e-02,  3.2898e-01, -1.4025e+00,\n",
       "       -3.1143e-01,  6.4549e-01, -6.1534e-02, -6.9295e-01,  6.0894e-04,\n",
       "       -5.6544e-01,  1.9181e-01, -1.9208e-01, -6.2673e-01, -9.7473e-03,\n",
       "       -5.5040e-01, -5.6128e-01, -1.9603e-01,  2.9254e-01,  9.8576e-02,\n",
       "       -5.9395e-02,  3.3616e-03,  1.9515e-01, -6.0703e-01,  3.4262e-01,\n",
       "        9.5211e-02, -7.9411e-02,  1.4305e-01, -5.6569e-01, -6.5887e-02,\n",
       "        1.5167e-01, -1.3505e-01,  1.9571e-01,  2.2812e-01,  3.5346e-02,\n",
       "       -2.2509e-01,  1.8910e-01, -3.7348e-01,  1.2505e-01,  4.6249e-01,\n",
       "       -3.2219e-01,  9.0643e-01,  1.1595e-01,  1.1628e-01,  2.2961e-01,\n",
       "        2.4010e-01, -6.1609e-02,  3.9325e-01, -6.5066e-02,  4.2257e-01,\n",
       "        5.6880e-01,  4.9804e-01, -6.1308e-01,  4.1468e-01, -1.3448e-01,\n",
       "        6.0430e-01, -6.5462e-02, -8.5376e-02,  1.9115e-01,  3.9925e-01,\n",
       "        3.7495e-01, -1.8492e-01,  6.1751e-02, -3.8747e-01, -3.0335e-01,\n",
       "       -3.8211e-01,  2.8221e-01, -1.0286e-01, -5.8660e-01,  8.2922e-01,\n",
       "        2.5131e-01,  2.4772e-01,  8.7482e-01, -3.1359e-01,  8.1621e-01,\n",
       "       -9.0081e-01, -7.7933e-01, -1.0090e+00,  3.6472e-01, -1.1562e-01,\n",
       "       -2.4841e-01,  9.4527e-02, -4.2266e-01,  6.0392e-02, -1.5365e-01,\n",
       "       -6.9604e-02,  5.1292e-03,  3.9572e-01, -1.5692e-01,  3.5708e-01,\n",
       "       -3.5165e-01,  3.5296e-01, -5.2220e-01,  5.1400e-01, -1.7764e-01,\n",
       "       -1.0272e-01, -3.9640e-01,  3.0418e-01,  7.3659e-02, -1.1685e-01,\n",
       "        1.4299e-01, -3.6810e-01,  2.7642e-01, -4.6683e-01, -3.2633e-01,\n",
       "        5.1107e-01,  2.3945e-02,  1.1723e-01,  2.1761e-01, -1.7389e-01,\n",
       "       -6.1193e-01, -5.9449e-01,  4.7749e-01, -5.9008e-01, -3.6092e-01,\n",
       "       -9.9574e-02, -4.3098e-02, -1.5106e-01, -1.4336e-01, -3.1135e-02,\n",
       "        1.7887e-01, -6.4221e-01,  1.7242e-01,  3.3916e-01,  8.7181e-01,\n",
       "       -7.7230e-01,  5.3195e-01, -5.2763e-01,  1.7510e-01,  3.1043e-01,\n",
       "       -1.5177e-01, -2.2706e-01,  1.0803e-01,  4.4919e-01,  7.0016e-02,\n",
       "        2.0851e-01,  2.1517e-01, -6.1712e-01, -9.9970e-02,  5.5020e-03,\n",
       "        7.6786e-02,  2.8046e-01,  4.2331e-01, -5.8925e-01,  7.0554e-02,\n",
       "        3.9923e-01,  9.0201e-02,  1.7139e-01, -1.7282e-01, -5.3675e-01,\n",
       "       -4.6439e-01, -5.7850e-01, -6.8311e-01,  5.9383e-02,  1.2427e-01,\n",
       "       -1.4558e-01,  5.7687e-01, -5.7499e-01, -5.1645e-02,  3.8410e-01,\n",
       "        1.3047e-01,  3.3786e-01,  3.3204e-01,  4.0119e-01,  2.6389e-01,\n",
       "       -3.6953e-01, -2.9797e-01, -6.6816e-01, -1.1883e-01,  5.0133e-01,\n",
       "        2.0603e-01, -3.2558e-01, -1.2242e-01,  5.0666e-01,  1.6353e-01,\n",
       "       -1.0672e-01,  2.2364e-01,  2.3915e-01, -5.5509e-01, -4.8432e-01,\n",
       "       -1.2165e-02, -1.7992e+00,  3.2310e-01, -2.6309e-01, -3.2538e-01,\n",
       "       -5.8270e-01,  1.5099e-01,  3.3838e-01,  1.2007e-01,  4.1395e-01,\n",
       "       -1.5553e-01, -1.9301e-01,  5.8860e-02, -5.2420e-01, -3.7170e-01,\n",
       "        5.6205e-01, -6.5801e-01, -4.9796e-01,  2.4347e-01,  1.2873e-01,\n",
       "        3.3665e-01, -7.2609e-02, -1.5686e-01, -1.4187e-01, -2.6488e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build glove-vec embedding layer\n",
    "def build_glove_embedding_layers():\n",
    "    \"\"\"\"embed_index={}\n",
    "    with open('glove.840B.300d.txt'.format(EMBEDDING_DIM), encoding=\"utf8\",) as file:\n",
    "        for line in file:\n",
    "            values=line.split()\n",
    "            word=values[0]\n",
    "            coefs=np.asarray(values[1:], dtype='float32')\n",
    "            embed_index[word]=coefs\"\"\"\n",
    "    embed_matrix=np.zeros((MAX_NB_WORDS, EMBEDDING_DIM))\n",
    "    for word, indx in t.word_index.items():\n",
    "        if indx >= MAX_NB_WORDS:\n",
    "            continue\n",
    "        if word in glove_model:\n",
    "            embed_vec=glove_model[word]\n",
    "            if embed_vec is not None:\n",
    "                embed_matrix[indx]=embed_vec\n",
    "    return Embedding(input_dim=MAX_NB_WORDS,\n",
    "                     output_dim=EMBEDDING_DIM,\n",
    "                     input_length=MAX_SEQUENCE_LENGTH,\n",
    "                     weights=[embed_matrix],\n",
    "                     trainable=False, name='word embedding')\n",
    "\n",
    "##\n",
    "embedding_layers=build_glove_embedding_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x238922aa668>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate\n",
    "#concatenate([x1, x2], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying complex convolutional approach\n",
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layers(sequence_input)\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "    l_pool = MaxPooling1D(5)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "    \n",
    "l_merge = concatenate(convs, axis=1)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(30)(l_cov2)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "l_flat = Dense(HIDDEN_UNITS)(l_flat)\n",
    "l_flat = Dropout(DROPOUT_RATE)(l_flat)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "\n",
    "preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - more complex convolutional neural network\")\n",
    "model.summary()\n",
    "model.fit(X_train, y_train, validation_data=(X_dev, y_val),\n",
    "          nb_epoch=3, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
