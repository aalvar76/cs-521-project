{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import all dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, merge, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## param list\n",
    "\n",
    "## parameter list\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 200\n",
    "DROPOUT_RATE    = 0.4\n",
    "HIDDEN_UNITS    = 200\n",
    "\n",
    "# NB_CLASSES      = 2\n",
    "\n",
    "# LEARNING\n",
    "# BATCH_SIZE      = 100\n",
    "# NB_EPOCHS       = 10\n",
    "# RUNS            = 5\n",
    "# VAL_SIZE        = 0.2\n",
    "\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data preprocessing\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string,re\n",
    "\n",
    "dataset=pd.read_csv('train.tsv',delimiter='\\t',encoding='utf-8')\n",
    "dataset.columns=['ID','Label','Statement','Subject','speaker','job_title',\n",
    "           'state_info','pantry_affiliation','barely_true_cnt','false_cnt',\n",
    "           'half_true_cnt','mostly_true_cnt','pants_on_fire_cnt','Context']\n",
    "\n",
    "def preprocessing_txt(dataset):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    rm_punct = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    corpus=[]\n",
    "    for elm in range(0, len(dataset.index)):\n",
    "        res=' '.join([i for i in dataset['Statement'][elm].split() if i not in stop_words])\n",
    "        res=rm_punct.sub(' ', res)\n",
    "        corpus.append(res)\n",
    "    return corpus\n",
    "\n",
    "corpus=preprocessing_txt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (10239, 16)\n",
      "Shape of label tensor: (10239, 2)\n"
     ]
    }
   ],
   "source": [
    "## import local dependencies\n",
    "import util\n",
    "from nlp_util import NLP_Task\n",
    "import preprocessing\n",
    "import importlib\n",
    "importlib.reload(preprocessing)\n",
    "\n",
    "\n",
    "## label\n",
    "multi_labels = {'false':0, 'true':1,'pants-fire':2,'barely-true':3,'half-true':4,'mostly-true':5}\n",
    "binary_labels = {'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}\n",
    "\n",
    "dataset['b_label'] = np.array(preprocessing.create_labels(labels=dataset['Label'].values,label_values=binary_labels))\n",
    "dataset['m_label'] = np.array(preprocessing.create_labels(labels=dataset['Label'].values,label_values=multi_labels))\n",
    "\n",
    "texts = corpus\n",
    "labels = dataset['b_label']\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', dataset.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "#\n",
    "# indices = np.arange(dataset.shape[0])\n",
    "# np.random.shuffle(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jvrat\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set \n",
      "[1673. 6518.]\n",
      "[ 441. 1607.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# cv = CountVectorizer()\n",
    "# corpus = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "X_train, X_val, y_train, y_val  = train_test_split(corpus,\n",
    "                                                   labels,train_size=0.8, random_state=123)\n",
    "\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set ')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOVE_DIR = \"/ext/home/analyst/Testground/data/glove\"\n",
    "# embeddings_index = {}\n",
    "# f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))\n",
    "\n",
    "# embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# embedding_layer = Embedding(len(word_index) + 1,\n",
    "#                             EMBEDDING_DIM,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                             trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EMBEDDING_DIM' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cd9841e82d08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0membedding_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuild_glove_embedding_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-cd9841e82d08>\u001b[0m in \u001b[0;36mbuild_glove_embedding_layers\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbuild_glove_embedding_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0membed_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'glove.6B.300d.txt'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EMBEDDING_DIM' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "## build glove-vec embedding layer\n",
    "def build_glove_embedding_layers():\n",
    "    embed_index={}\n",
    "    with open('glove.6B.300d.txt'.format(EMBEDDING_DIM), encoding=\"utf8\",) as file:\n",
    "        for line in file:\n",
    "            values=line.split()\n",
    "            word=values[0]\n",
    "            coefs=np.asarray(values[1:], dtype='float32')\n",
    "            embed_index[word]=coefs\n",
    "    embed_matrix=np.zeros((MAX_NB_WORDS, EMBEDDING_DIM))\n",
    "    for word, indx in tokenized.word_index.items():\n",
    "        if indx >= MAX_NB_WORDS:\n",
    "            continue\n",
    "        embed_vec=embed_index.get(word)\n",
    "        if embed_vec is not None:\n",
    "            embed_matrix[indx]=embed_vec\n",
    "    return Embedding(input_dim=MAX_NB_WORDS,\n",
    "                     output_dim=EMBEDDING_DIM,\n",
    "                     input_length=MAX_SEQUENCE_LENGTH,\n",
    "                     weights=[embed_matrix],\n",
    "                     trainable=True, name='word embedding')\n",
    "\n",
    "##\n",
    "embedding_layers=build_glove_embedding_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_layes' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d025edcae8b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membedding_layes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_layes' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "embedding_layes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying complex convolutional approach\n",
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "    l_pool = MaxPooling1D(5)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "    \n",
    "l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(30)(l_cov2)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "l_flat = Dense(HIDDEN_UNITS)(l_flat)\n",
    "l_flat = Dropout(DROPOUT_RATE)(l_flat)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "\n",
    "preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - more complex convolutional neural network\")\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=20, batch_size=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
