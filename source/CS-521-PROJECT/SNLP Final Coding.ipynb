{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all code together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utility libraries\n",
    "import util\n",
    "import preprocessing\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_task = nlp_util.NLP_Task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "tr_file, va_file, te_file = util.load_files()\n",
    "tr_dict = util.tsv_to_dict(tsv_file=tr_file)\n",
    "va_dict = util.tsv_to_dict(tsv_file=va_file)\n",
    "te_dict = util.tsv_to_dict(tsv_file=te_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_file['preprocessed'] = preprocessing.preprocessing_txt(dataset=tr_file)\n",
    "va_file['preprocessed'] = preprocessing.preprocessing_txt(dataset=va_file)\n",
    "te_file['preprocessed'] = preprocessing.preprocessing_txt(dataset=te_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training = pd.DataFrame()\n",
    "features_validation = pd.DataFrame()\n",
    "features_testing = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training['text'] = tr_file['preprocessed']\n",
    "features_validation['text'] = va_file['preprocessed']\n",
    "features_testing['text'] = te_file['preprocessed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotting_util\n",
    "importlib.reload(plotting_util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_label = [ len(tr_file['Label'].values[tr_file['Label'] == 'pants-fire']),\n",
    "    len(tr_file['Label'].values[tr_file['Label'] == 'false']),\n",
    "                 len(tr_file['Label'].values[tr_file['Label'] == 'barely-true']),\n",
    "                 len(tr_file['Label'].values[tr_file['Label'] == 'half-true']),\n",
    "                 len(tr_file['Label'].values[tr_file['Label'] == 'mostly-true']),\n",
    "                len(tr_file['Label'].values[tr_file['Label'] == 'true'])\n",
    "                ]\n",
    "data_labels = ['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true','true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar_plot = plotting_util.plot_bar_chart(chartname='Liar liar dataset distribution', barnames=data_labels, barvalues=data_by_label,\n",
    "#                             barcolors=['skyblue'])\n",
    "#plt.plot()\n",
    "\n",
    "#pie_plot = plotting_util.plot_pie_chart(chartname='myplot', labels=data_labels, values=data_by_label)\n",
    "sizes = data_by_label\n",
    "explode = (0.1, 0.1, 0.1, 0.1, 0.1, 0)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=data_labels, autopct='%1.1f%%', explode= explode,\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart\n",
    "labels = ['Pants-fire', 'False', 'Barely-true', 'Half-true', 'Mostly-true','True']\n",
    "sizes = data_by_label\n",
    "#colors\n",
    "colors = ['#ff4d4d','#ff794d','#ffa64d','#d2ff4d','#a6ff4d','#4dff4d']\n",
    "#explsion\n",
    "explode = (0.05,0.05,0.05,0.05,0.05,0.05)\n",
    "\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(6,6)) \n",
    "ax1.pie(sizes, colors = colors, labels=labels, autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode, textprops={'fontsize': 17})\n",
    "#draw circle\n",
    "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle\n",
    "ax1.axis('equal') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting POS tags grouped by unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS extracted from no preprocessed data for training, validation and testing files\n",
    "unigram_pos, bigrams_pos, trigram_pos = preprocessing.extract_POS(statements=tr_dict['statement'])\n",
    "unigram_pos_va, bigrams_pos_va, trigram_pos_va = preprocessing.extract_POS(statements=va_dict['statement'])\n",
    "unigram_pos_te, bigrams_pos_te, trigram_pos_te = preprocessing.extract_POS(statements=te_dict['statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pos extracted from preprocessed data for training, validation and testing files\n",
    "unigram_pos_p, bigrams_pos_p, trigram_pos_p = preprocessing.extract_POS(tr_file['preprocessed'].values)\n",
    "unigram_pos_p_va, bigrams_pos_p_va, trigram_pos_p_va = preprocessing.extract_POS(va_file['preprocessed'].values)\n",
    "unigram_pos_p_te, bigrams_pos_p_te, trigram_pos_p_te = preprocessing.extract_POS(te_file['preprocessed'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_remove = ['NNP','CD']\n",
    "# Training\n",
    "removed_pos = nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_p)\n",
    "removed_pos_va =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_p_va)\n",
    "removed_pos_te =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_p_te)\n",
    "\n",
    "removed_pos_bigrams = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos)\n",
    "removed_pos_bigrams_va = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_va)\n",
    "removed_pos_bigrams_te = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_te)\n",
    "\n",
    "removed_pos_trigrams = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos)\n",
    "removed_pos_trigrams_va = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_va)\n",
    "removed_pos_trigrams_te = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_remove = ['NNP','CD']\n",
    "features_training['pos'] = [\" \".join(x).replace('<s>','').replace('$','dollar').strip() for x in removed_pos]\n",
    "features_validation['pos'] = [\" \".join(x).replace('<s>','').replace('$','dollar').strip() for x in removed_pos_va]\n",
    "features_testing['pos'] = [\" \".join(x).replace('<s>','').replace('$','dollar').strip() for x in removed_pos_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_remove = ['NNP','CD']\n",
    "features_training['pos_bi'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_bigrams]\n",
    "features_validation['pos_bi'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_bigrams_va]\n",
    "features_testing['pos_bi'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_bigrams_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_remove = ['NNP','CD']\n",
    "features_training['pos_tri'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_trigrams]\n",
    "features_validation['pos_tri'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_trigrams_va]\n",
    "features_testing['pos_tri'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_trigrams_te]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unique values for unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique list of unigrams, bigrams and trigrams for no preprocessed data\n",
    "unigram_list_tr = nlp_task.UniquePosTags(unigram_pos)\n",
    "bigram_list_tr = nlp_task.UniquePosTags(bigrams_pos)\n",
    "trigram_list_tr = nlp_task.UniquePosTags(trigram_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique list of unigrams, bigrams and trigrams for preprocessed data\n",
    "unigram_list_tr_processed = nlp_task.UniquePosTags(unigram_pos_p)\n",
    "bigram_list_tr_processed = nlp_task.UniquePosTags(bigrams_pos_p)\n",
    "trigram_list_tr_processed = nlp_task.UniquePosTags(trigram_pos_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicated POS in bigrams and trigrams [NNP and CD]\n",
    "\n",
    "For example: The/DT economy/NN bled/VBD $/$ 24/CD billion/CD due/JJ to/TO the/DT government/NN shutdown/NN ./. <br>\n",
    "In this case having CD_CD is the same as having only CD<br>\n",
    "Same with: <br>\n",
    "U.S./NNP Rep./NNP Ron/NNP Kind/NNP ,/, D-Wis./NNP ,/, and/CC his/PRP$ fellow/JJ Democrats/NNS went/VBD on/IN a/DT spending/NN spree/NN and/CC now/RB their/PRP$ credit/NN card/NN is/VBZ maxed/VBN out/RP <br>\n",
    "We don't need all those NNPs to find a pattern and it might be noisy to the ML algorithm<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For raw data\n",
    "\n",
    "list_to_remove = ['NNP','CD']\n",
    "# Training\n",
    "removed_pos = nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos)\n",
    "removed_pos_bigrams = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos)\n",
    "removed_pos_trigrams = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos)\n",
    "\n",
    "# Validdation\n",
    "removed_pos_va =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_va)\n",
    "removed_pos_bigrams_va = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_va)\n",
    "removed_pos_trigrams_va = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_va)\n",
    "\n",
    "# Testing\n",
    "removed_pos_te =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_te)\n",
    "removed_pos_bigrams_te = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_te)\n",
    "removed_pos_trigrams_te = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_te)\n",
    "\n",
    "#LIST OF UNIQUE BIGRAMS AND TRIGRAMS AFTER REMOVING CONSECUTIVE SAME TAGS\n",
    "removed_unique_bigrams = nlp_task.UniquePosTags(postags=removed_pos_bigrams)\n",
    "removed_unique_trigrams = nlp_task.UniquePosTags(postags=removed_pos_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For processed data\n",
    "list_to_remove = ['NNP','CD']\n",
    "# Training\n",
    "removed_pos_p = nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_p)\n",
    "removed_pos_bigrams_p = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_p)\n",
    "removed_pos_trigrams_p = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_p)\n",
    "\n",
    "# Validdation\n",
    "removed_pos_va_p =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_p_va)\n",
    "removed_pos_bigrams_va_p = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_va_p)\n",
    "removed_pos_trigrams_va_p = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_va_p)\n",
    "\n",
    "# Testing\n",
    "removed_pos_te_p =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_p_te)\n",
    "removed_pos_bigrams_te_p = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_te_p)\n",
    "removed_pos_trigrams_te_p = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_te_p)\n",
    "\n",
    "#LIST OF UNIQUE BIGRAMS AND TRIGRAMS AFTER REMOVING CONSECUTIVE SAME TAGS\n",
    "removed_unique_bigrams_p = nlp_task.UniquePosTags(postags=removed_pos_bigrams_p)\n",
    "removed_unique_trigrams_p = nlp_task.UniquePosTags(postags=removed_pos_trigrams_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unigram_list_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add numerical labels for each of the sentence in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for multiclassification and binary classification tasks\n",
    "multi_labels = {'false':0, 'true':1,'pants-fire':2,'barely-true':3,'half-true':4,'mostly-true':5}\n",
    "binary_labels = {'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}\n",
    "\n",
    "\n",
    "tr_file['b_label'] = np.array(preprocessing.create_labels(labels=tr_file['Label'].values,label_values=binary_labels))\n",
    "va_file['b_label'] = np.array(preprocessing.create_labels(labels=va_file['Label'].values,label_values=binary_labels))\n",
    "te_file['b_label'] = np.array(preprocessing.create_labels(labels=te_file['Label'].values,label_values=binary_labels))\n",
    "\n",
    "tr_file['m_label'] = np.array(preprocessing.create_labels(labels=tr_file['Label'].values,label_values=multi_labels))\n",
    "va_file['m_label'] = np.array(preprocessing.create_labels(labels=va_file['Label'].values,label_values=multi_labels))\n",
    "te_file['m_label'] = np.array(preprocessing.create_labels(labels=te_file['Label'].values,label_values=multi_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training['b_label']= np.array(preprocessing.create_labels(labels=tr_file['Label'].values,label_values=binary_labels))\n",
    "features_validation['b_label']= np.array(preprocessing.create_labels(labels=va_file['Label'].values,label_values=binary_labels))\n",
    "features_testing['b_label']= np.array(preprocessing.create_labels(labels=te_file['Label'].values,label_values=binary_labels))\n",
    "\n",
    "features_training['m_label']= np.array(preprocessing.create_labels(labels=tr_file['Label'].values,label_values=multi_labels))\n",
    "features_validation['m_label']= np.array(preprocessing.create_labels(labels=va_file['Label'].values,label_values=multi_labels))\n",
    "features_testing['m_label']= np.array(preprocessing.create_labels(labels=te_file['Label'].values,label_values=multi_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecesary columns from dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once this code is executed and you try to re run it again it is going to show an error because columns already were remoded\n",
    "unnecesary_columns = ['BT', 'FC', 'HT', 'MT', 'PF']\n",
    "tr_file = tr_file.drop(unnecesary_columns, axis=1)\n",
    "va_file = va_file.drop(unnecesary_columns, axis=1)\n",
    "te_file = te_file.drop(unnecesary_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract POS features from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having ran some experiments with decision trees regarding POS we came up with a list of POS unigrams, bigrams and trigrams that were more relevant for classifying mostly ture and mostly false news.\n",
    "<br>\n",
    "POS unigrams: ['VBZ', 'DT', 'NNPS', 'VBP', 'JJ', 'IN', 'WRB', 'VBD', 'PRP', 'RP', 'WDT', 'VB', 'NNP', 'VBG', 'PRP$', 'VBN', 'CD', 'RB', 'WP', 'JJS', 'JJR', 'EX', 'RBS', 'FW', 'LS']\n",
    " <br>\n",
    "POS brigrams: ['NNPS_VBP', 'VB_NNP', 'IN_DT', 'VB_JJ', 'JJ_CD', 'CD_NNS', 'DT_JJS', 'JJR_IN', 'IN_CD', 'CC_IN', 'RB_VBD', 'CD_NN', 'NN_TO', 'JJR_JJ', 'VB_CD'] <br>\n",
    "POS trigrams: ['VBD_VBN_IN', 'IN_DT_JJ', 'CD_NN_IN', 'IN_CD_NNS', 'IN_DT_NN', 'DT_JJ_CD', 'MD_VB_IN', 'JJS_JJ_NN', 'CC_JJ_NNS', 'JJ_NNS_VBP', 'VBP_CD_NN', 'NNS_,_CD', 'sos_JJR_IN', 'IN_DT_NNS','JJ_NN_MD'] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_relevant_unigrams =  ['VBZ', 'DT', 'NNPS', 'VBP', 'JJ', 'IN', 'WRB', 'VBD', 'PRP', 'RP', 'WDT', 'VB', 'NNP', 'VBG', 'PRP$', 'VBN', 'CD', 'RB', 'WP', 'JJS', 'JJR', 'EX', 'RBS', 'FW', 'LS'] \n",
    "pos_relevant_bigrams = ['NNPS_VBP', 'VB_NNP', 'IN_DT', 'VB_JJ', 'JJ_CD', 'CD_NNS', 'DT_JJS', 'JJR_IN', 'IN_CD', 'CC_IN', 'RB_VBD', 'CD_NN', 'NN_TO', 'JJR_JJ', 'VB_CD'] \n",
    "pos_relevant_trigrams = ['VBD_VBN_IN', 'IN_DT_JJ', 'CD_NN_IN', 'IN_CD_NNS', 'IN_DT_NN', 'DT_JJ_CD', 'MD_VB_IN', 'JJS_JJ_NN', 'CC_JJ_NNS', 'JJ_NNS_VBP', 'VBP_CD_NN', 'sos_JJR_IN', 'IN_DT_NNS','JJ_NN_MD'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_onehot_unigram, Xtr_count_unigram, Xtr_tfidf_unigram, Xval_onehot_unigram, Xval_count_unigram, Xval_tfidf_unigram, Xte_onehot_unigram, Xte_count_unigram, Xte_tfidf_unigram = util.GetFeaturesFromPOS(training_data=unigram_pos_p, validation_data=unigram_pos_p_va, testing_data=unigram_pos_p_te, user_defined_vocabulary=pos_relevant_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Xtr_onehot_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_onehot_bigrams, Xtr_count_bigrams, Xtr_tfidf_bigrams, Xval_onehot_bigrams, Xval_count_bigrams, Xval_tfidf_bigrams, Xte_onehot_bigrams, Xte_count_bigrams, Xte_tfidf_bigrams = util.GetFeaturesFromPOS(training_data=removed_pos_bigrams_p, validation_data=removed_pos_bigrams_va_p, testing_data=removed_pos_bigrams_te_p, user_defined_vocabulary=pos_relevant_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Xtr_onehot_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_onehot_trigram, Xtr_count_trigram, Xtr_tfidf_trigram, Xval_onehot_trigram, Xval_count_trigram, Xval_tfidf_trigram, Xte_onehot_trigram, Xte_count_trigram, Xte_tfidf_trigram = util.GetFeaturesFromPOS(training_data=trigram_pos, validation_data=trigram_pos_va, testing_data=trigram_pos_te, user_defined_vocabulary=pos_relevant_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Xtr_onehot_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(Xtr_onehot_trigram[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving vectors representing bigrams\n",
    "tr_file['pos_unigrams_1hot'] =  [str(x) for x in Xtr_onehot_unigram]\n",
    "tr_file['pos_bigrams_1hot'] = [str(x) for x in Xtr_onehot_bigrams]\n",
    "tr_file['pos_trigrams_1hot'] = [str(x) for x in Xtr_onehot_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "va_file['pos_unigrams_1hot'] =  [str(x) for x in Xval_onehot_unigram]\n",
    "va_file['pos_bigrams_1hot'] = [str(x) for x in Xval_onehot_bigrams]\n",
    "va_file['pos_trigrams_1hot'] = [str(x) for x in Xval_onehot_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "te_file['pos_unigrams_1hot'] =  [str(x) for x in Xte_onehot_unigram]\n",
    "te_file['pos_bigrams_1hot'] = [str(x) for x in Xte_onehot_bigrams]\n",
    "te_file['pos_trigrams_1hot'] = [str(x) for x in Xte_onehot_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "tr_file['pos_unigrams_count'] =  [str(x) for x in Xtr_count_unigram]\n",
    "tr_file['pos_bigrams_count'] = [str(x) for x in Xtr_count_bigrams]\n",
    "tr_file['pos_trigrams_count'] = [str(x) for x in Xtr_count_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "va_file['pos_unigrams_count'] =  [str(x) for x in Xval_count_unigram]\n",
    "va_file['pos_bigrams_count'] = [str(x) for x in Xval_count_bigrams]\n",
    "va_file['pos_trigrams_count'] = [str(x) for x in Xval_count_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "te_file['pos_unigrams_count'] =  [str(x) for x in Xte_count_unigram]\n",
    "te_file['pos_bigrams_count'] = [str(x) for x in Xte_count_bigrams]\n",
    "te_file['pos_trigrams_count'] = [str(x) for x in Xte_count_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "tr_file['pos_unigrams_tfidf'] =  [str(x) for x in Xtr_tfidf_unigram]\n",
    "tr_file['pos_bigrams_tfidf'] = [str(x) for x in Xtr_tfidf_bigrams]\n",
    "tr_file['pos_trigrams_tfidf'] = [str(x) for x in Xtr_tfidf_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "va_file['pos_unigrams_tfidf'] =  [str(x) for x in Xval_tfidf_unigram]\n",
    "va_file['pos_bigrams_tfidf'] = [str(x) for x in Xval_tfidf_bigrams]\n",
    "va_file['pos_trigrams_tfidf'] = [str(x) for x in Xval_tfidf_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "te_file['pos_unigrams_tfidf'] =  [str(x) for x in Xte_tfidf_unigram]\n",
    "te_file['pos_bigrams_tfidf'] = [str(x) for x in Xte_tfidf_bigrams]\n",
    "te_file['pos_trigrams_tfidf'] = [str(x) for x in Xte_tfidf_trigram]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training['key_words'] = preprocessing.get_keywords(tr_file)\n",
    "features_validation['key_words'] = preprocessing.get_keywords(va_file)\n",
    "features_testing['key_words'] = preprocessing.get_keywords(te_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing.bigphrase_tfidf_feats(tr_file[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIWC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LIWC analysis files\n",
    "liwc_tr = pd.read_csv('..\\\\dataset\\\\{0}'.format('train_liwc.csv'))\n",
    "liwc_va = pd.read_csv('..\\\\dataset\\\\{0}'.format('valid_liwc.csv'))\n",
    "liwc_te = pd.read_csv('..\\\\dataset\\\\{0}'.format('test_liwc.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features so there are equally treated in terms of measure units\n",
    "scaler = MinMaxScaler()\n",
    "liwc_features_tr = scaler.fit_transform(liwc_tr.iloc[1:,3:])\n",
    "liwc_features_va = scaler.transform(liwc_va.iloc[:,14:])\n",
    "liwc_features_te = scaler.transform(liwc_te.iloc[:,14:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_feature_names = liwc_tr.columns[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, cname in enumerate(liwc_feature_names):\n",
    "    features_training[cname] = liwc_features_tr.T[index]\n",
    "    features_validation[cname] = liwc_features_va.T[index]\n",
    "    features_testing[cname] = liwc_features_te.T[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MI compute the information between the variables and the target\n",
    "feature_selected = mutual_info_classif(liwc_features_tr[(tr_file['b_label']!=0)],tr_file['b_label'][(tr_file['b_label']!=0)] , random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= liwc_tr.columns[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the more relevant LIWC features\n",
    "relevant_liwc_features = list()\n",
    "for index, f in enumerate(features):\n",
    "    if feature_selected[index]>0.005:\n",
    "        relevant_liwc_features.append(f)\n",
    "        print(index, f, feature_selected[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LIWC features to the dataframe\n",
    "for index,col in enumerate(liwc_tr.columns[3:]):\n",
    "    if col in relevant_liwc_features:\n",
    "        tr_file[col] = liwc_features_tr.T[index]\n",
    "        \n",
    "for index,col in enumerate(liwc_va.columns[14:]):\n",
    "    if col in relevant_liwc_features:\n",
    "        va_file[col] = liwc_features_va.T[index]\n",
    "        \n",
    "for index,col in enumerate(liwc_te.columns[14:]):\n",
    "    if col in relevant_liwc_features:\n",
    "        te_file[col] = liwc_features_te.T[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Blob Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Blobber\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.np_extractors import ConllExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTextBlobFeatures(corpus):\n",
    "    extractor = ConllExtractor()\n",
    "    text_blob_features = np.zeros((len(corpus),4))\n",
    "    blob_sentiment_analyzer = Blobber(analyzer=NaiveBayesAnalyzer())\n",
    "    for i,each_text in enumerate(corpus):\n",
    "        #print('analyzing: ',i)\n",
    "        #blob_sentiment_analyzer = TextBlob(each_text, analyzer=NaiveBayesAnalyzer())\n",
    "        text_blob_features[i,0]=blob_sentiment_analyzer(each_text).sentiment[1]\n",
    "        text_blob_features[i,1]=blob_sentiment_analyzer(each_text).sentiment[2]\n",
    "        #text_blob_features[i,2]= TextBlob(each_text).polarity\n",
    "        text_blob_features[i,2]= TextBlob(each_text).subjectivity\n",
    "        noun_phrase_extractor = TextBlob(each_text, np_extractor=extractor)\n",
    "        text_blob_features[i,3]= len(noun_phrase_extractor.noun_phrases)\n",
    "    return text_blob_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_tb_features = extractTextBlobFeatures(tr_file['preprocessed'])\n",
    "va_tb_features = extractTextBlobFeatures(va_file['preprocessed'])\n",
    "te_tb_features = extractTextBlobFeatures(te_file['preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_feature_names = ['sentiment_pos', 'sentiment_neg','subjectivity','noun_phrases_count']\n",
    "for i in range(0,4):\n",
    "    tr_file[tb_feature_names[i]] = tr_tb_features.T[i]\n",
    "    va_file[tb_feature_names[i]] = va_tb_features.T[i]\n",
    "    te_file[tb_feature_names[i]] = te_tb_features.T[i]\n",
    "\n",
    "    \n",
    "phrases_scaler = MinMaxScaler()\n",
    "tr_file['noun_phrases_count'] = phrases_scaler.fit_transform(tr_file['noun_phrases_count'].values.reshape(-1, 1))\n",
    "va_file['noun_phrases_count']= phrases_scaler.transform(va_file['noun_phrases_count'].values.reshape(-1, 1))\n",
    "te_file['noun_phrases_count'] = phrases_scaler.transform(te_file['noun_phrases_count'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_feature_names = ['sentiment_pos', 'sentiment_neg','subjectivity','noun_phrases_count']\n",
    "for i in range(0,4):\n",
    "    features_training[tb_feature_names[i]]  = tr_tb_features.T[i]\n",
    "    features_validation[tb_feature_names[i]] = va_tb_features.T[i]\n",
    "    features_testing[tb_feature_names[i]] = te_tb_features.T[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_scaler = MinMaxScaler()\n",
    "features_training['noun_phrases_count'] = phrases_scaler.fit_transform(tr_file['noun_phrases_count'].values.reshape(-1, 1))\n",
    "features_validation['noun_phrases_count']= phrases_scaler.transform(va_file['noun_phrases_count'].values.reshape(-1, 1))\n",
    "features_testing['noun_phrases_count'] = phrases_scaler.transform(te_file['noun_phrases_count'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting context based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check trainig statements\n",
    "tr_file['all_cap'] = [1 if len(x)>0 else 0 for x in [re.findall('([A-Z]+\\s)', x) for x in tr_file['Statement'].values]]\n",
    "tr_file['quotation'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[\"|\\']([^\"]*)[\"|\\']', x) for x in tr_file['Statement'].values]]\n",
    "tr_file['parenthesis'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[(]([^\"]*)[)]', x) for x in tr_file['Statement'].values]]\n",
    "\n",
    "# Check validation statements\n",
    "va_file['all_cap'] = [1 if len(x)>0 else 0 for x in [re.findall('([A-Z]+\\s)', x) for x in va_file['Statement'].values]]\n",
    "va_file['quotation'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[\"|\\']([^\"]*)[\"|\\']', x) for x in va_file['Statement'].values]]\n",
    "va_file['parenthesis'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[(]([^\"]*)[)]', x) for x in va_file['Statement'].values]]\n",
    "\n",
    "# Check testing statements\n",
    "te_file['all_cap'] = [1 if len(x)>0 else 0 for x in [re.findall('([A-Z]+\\s)', x) for x in te_file['Statement'].values]]\n",
    "te_file['quotation'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[\"|\\']([^\"]*)[\"|\\']', x) for x in te_file['Statement'].values]]\n",
    "te_file['parenthesis'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[(]([^\"]*)[)]', x) for x in te_file['Statement'].values]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training['all_cap'] = tr_file['all_cap']\n",
    "features_validation['all_cap']= va_file['all_cap'] \n",
    "features_testing['all_cap'] =te_file['all_cap'] \n",
    "\n",
    "features_training['quotation'] = tr_file['quotation']\n",
    "features_validation['quotation']= va_file['quotation'] \n",
    "features_testing['quotation'] =te_file['quotation'] \n",
    "\n",
    "features_training['parenthesis'] = tr_file['parenthesis']\n",
    "features_validation['parenthesis']= va_file['parenthesis'] \n",
    "features_testing['parenthesis'] =te_file['parenthesis'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run machine learning models on feature extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords # nltk stop words\n",
    "from nltk.tokenize import word_tokenize # nltk word tokenizer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords # nltk stop words\n",
    "from nltk.tokenize import word_tokenize # nltk word tokenizer\n",
    "import string\n",
    "import re\n",
    "## Function to clean text data\n",
    "def lemmatize_remove_stop_words(corpus):\n",
    "    print('Processing data...')\n",
    "    result = list()\n",
    "    # define a lemmatizer\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # tokenize and remove stopwords from every sentence\n",
    "    stop_words = stopwords.words('english')\n",
    "    # iterate over every sentence and apply the three filters to remove stopwords, extract lemmas for verbs and nouns\n",
    "    for index, sentence in enumerate(corpus):\n",
    "        #print('Cleaning sentence number:', index, end=\"##\")\n",
    "        #print('Making text all lower case...',end=\"--\")\n",
    "        clean_text = sentence.lower() #  make all words lower case\n",
    "        #print('Removing punctuation...', end=\"--\")\n",
    "        clean_text = re.sub(r'[{0}]'.format(string.punctuation),'',clean_text)# remove punctuation\n",
    "        #print('Removing numberic data and symbols...', end=\"--\")\n",
    "        clean_text = re.sub(r'\\w*\\d\\w*','',clean_text) # remove alpha numerics\n",
    "        #print('Splitting into tokens...', end=\"--\")\n",
    "        tokenized_sentence = word_tokenize(clean_text) # split sentence into tokens\n",
    "        #print('Lemmatizing verbs...',end=\"--\")\n",
    "        filter_one = [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in tokenized_sentence] # lemmatize verbs\n",
    "        #print('Lemmatizing nouns...',end=\"--\")\n",
    "        filter_two = [wordnet_lemmatizer.lemmatize(word, pos=\"n\") for word in filter_one] # lemmatize nouns\n",
    "        #print('Removing stop words...',end=\"--\")\n",
    "        filter_three = [w for w in filter_two if w not in stop_words] # remove stop words\n",
    "        #print('Removing extra white spaces',end=\"--\")\n",
    "        all_clean = (' '.join(filter_three)).strip() # remove extra white spaces\n",
    "        #print('All clean for sentence: ', index,end=\"\\r\")\n",
    "        result.append(all_clean) # append data to result\n",
    "    print('Finished!')\n",
    "    return np.array(result) # convert to numpy array and return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import different models that we want to train\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RANDOM_STATE_V = 45\n",
    "models = [DummyClassifier,\n",
    "          DecisionTreeClassifier, \n",
    "          Perceptron, \n",
    "          LogisticRegression, \n",
    "          MultinomialNB, \n",
    "          BernoulliNB, \n",
    "          SGDClassifier, \n",
    "          SVC\n",
    "         ]#, \n",
    "          #SVC]\n",
    "defaults = [{'strategy':'most_frequent', 'random_state': _RANDOM_STATE_V}, #Baseline\n",
    "    {'max_depth': 3, 'criterion':'entropy','random_state': _RANDOM_STATE_V}, #DT\n",
    "            {'penalty':'l2','early_stopping': True,'random_state': _RANDOM_STATE_V}, #Perceptron\n",
    "            {'penalty':'l2','tol':0.0001, 'C':1.0,'max_iter':100,'random_state': _RANDOM_STATE_V}, #Linear Regression\n",
    "            {}, #MultinomialNB\n",
    "            {}, #BernoulliNB\n",
    "            {'loss':'hinge', 'penalty':'l2', 'alpha':0.0001,'random_state': _RANDOM_STATE_V}, #SGDClassifier,\n",
    "            {'C':1.0, 'kernel':'linear', 'degree':3, 'gamma':1,'random_state': _RANDOM_STATE_V}\n",
    "            #{'n_neighbors':5},\n",
    "           #{}\n",
    "           ]#, #KNeighborsClassifier\n",
    "            #{'C':1.0, 'kernel':'rbf', 'degree':3, 'gamma':1,'random_state': _RANDOM_STATE_V}, #SVC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making string of the data\n",
    "training_str = [\" \".join(x) for x in removed_pos_bigrams_p]\n",
    "validation_str = [\" \".join(x) for x in removed_pos_bigrams_va]\n",
    "\n",
    "#replace $ by dollar\n",
    "training_str = [x.replace('$', 'dollar').replace('<s>','') for x in training_str]\n",
    "validation_str = [x.replace('$', 'dollar').replace('<s>','') for x in validation_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Xtr = lemmatize_remove_stop_words(tr_file['preprocessed'][(tr_file['b_label']!=0)])\n",
    "#Xde = lemmatize_remove_stop_words(va_file['preprocessed'][(va_file['b_label']!=0)])\n",
    "Xtr = tr_file['preprocessed'][(tr_file['b_label']!=0)]\n",
    "Xde = va_file['preprocessed'][(va_file['b_label']!=0)]\n",
    "#Xtr = tr_file['preprocessed'][(tr_file['b_label']!=0)]\n",
    "#Xde = va_file['preprocessed'][(va_file['b_label']!=0)]\n",
    "#Ytr = tr_file['b_label'][(tr_file['b_label']!=0)]\n",
    "#Yde = va_file['b_label'][(va_file['b_label']!=0)]\n",
    "#Xtr= Xtr_onehot_unigram[(tr_file['b_label']!=0)]\n",
    "#Xde= Xval_onehot_unigram[(va_file['b_label']!=0)]\n",
    "#Xtr = Xtr_onehot_bigrams#tr_tb_features#liwc_features_tr#tr_file['preprocessed']+' '+training_str\n",
    "#Xde = Xval_onehot_bigrams#va_tb_features#liwc_features_va#va_file['preprocessed']+' '+validation_str\n",
    "Ytr = tr_file['b_label'][(tr_file['b_label']!=0)][(tr_file['b_label']!=0)]\n",
    "Yde = va_file['b_label'][(va_file['b_label']!=0)]\n",
    "\n",
    "report_list = list()\n",
    "for index, model in enumerate(models):\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', model(**defaults[index])),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    #text_clf = model(**defaults[index])\n",
    "    print('Training {0}'.format(model.__name__))\n",
    "    text_clf.fit(Xtr, Ytr)\n",
    "    pred = text_clf.predict(Xde)\n",
    "    #print('Accuracy: ', np.mean(pred==Yde))\n",
    "    print('Accuracy: ', accuracy_score(pred,Yde))\n",
    "    print(classification_report(pred, Yde))\n",
    "    #report_list.append((model.__name__, np.mean(pred==Yde), f1_score(pred, Yde, average='binary', pos_label=1),f1_score(pred, Yde, average='binary', pos_label=-1)))\n",
    "    report_list.append((model.__name__, accuracy_score(pred,Yde)))\n",
    "for e in report_list:\n",
    "    #print('{0},{1},{2}'.format(e[1], e[2], e[3]))\n",
    "    print('{0},{1}'.format(e[0], e[1]))                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, \n",
    "#random_state=None, solver=’warn’, max_iter=1000, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None\n",
    "\n",
    "parameters = {\n",
    "    #'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__max_iter': (10,20,30,50,100),\n",
    "    #'clf__alpha': (1e-5,1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1),\n",
    "    #'clf__penalty': 'l2',\n",
    "    'clf__penalty': ('l2','l1'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "    #alpha=1e-5,\n",
    "    'clf__C' :(0.01,0.05,0.1,0.2,0.3,0.4,0.5,1,5,8,12) ,\n",
    "    #'clf__class_weight' : ({1:1},{1:2},{1:3},{1:4},{1:5}\n",
    "    #'clf__alpha':(0.2,0.5,1,2,3,10,15,25,50)\n",
    "    \n",
    "    #'clf__loss': 'hinge' #, 'log', 'modified_huber', 'squared_hinge', 'perceptron'\n",
    "}\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(penalty='l2', random_state=_RANDOM_STATE_V, max_iter=1000)),\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(text_clf, parameters, cv=5,n_jobs=1, verbose=1, scoring='accuracy')\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in text_clf.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "#validation_set = PredefinedSplit(test_fold=validation_text)\n",
    "grid_search.fit(Xtr, Ytr)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "#print('Training {0}'.format(SGDClassifier.__name__))\n",
    "#text_clf.fit(X_train_oversamples, Y_train_resampled)\n",
    "#pred = text_clf.predict(validation_text)\n",
    "#print(classification_report(pred, validation_target))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(penalty='l2', random_state=_RANDOM_STATE_V, max_iter=1000, C=0.5)),\n",
    "])\n",
    "\n",
    "text_clf.fit(Xtr,Ytr)\n",
    "pred = text_clf.predict(Xde)\n",
    "print('Accuracy: ', accuracy_score(pred, Yde))\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adj Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import spacy\n",
    "import en_core_web_sm\n",
    "nlp=en_core_web_sm.load()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def get_noun_adj_pairs(corpus):\n",
    "    result = list()\n",
    "    for c in corpus:\n",
    "        parsed=nlp(c)\n",
    "        noun_adj_pairs_res=[]\n",
    "        for i, tok in enumerate(parsed):\n",
    "            if tok.pos_ not in ('NOUN','PRON'):\n",
    "                continue\n",
    "            for j in range(i+1, len(parsed)):\n",
    "                if parsed[j].pos_=='ADJ':\n",
    "                    noun_adj_pairs_res.append((tok, parsed[j]))\n",
    "                    break\n",
    "        result.append(noun_adj_pairs_res)\n",
    "    return result\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Xtr[5]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"get_noun_adj_pairs(tr_file['preprocessed'])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    embeddings_index = {}\n",
    "    f = open(gloveFile, encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-300])\n",
    "        coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glove_model = loadGloveModel('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build glove-vec embedding layer\n",
    "def build_glove_embedding_layers():\n",
    "    embed_matrix=np.zeros((max_features, embedding_dims))\n",
    "    for word, indx in t.word_index.items():\n",
    "        if indx >= max_features:\n",
    "            continue\n",
    "        if word in glove_model:\n",
    "            embed_vec=glove_model[word]\n",
    "            if embed_vec is not None:\n",
    "                embed_matrix[indx]=embed_vec\n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "embedding_weights=build_glove_embedding_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(num_words=5000) # create tokenizer with a max number of words to take into account according to frequency\n",
    "t.fit_on_texts(tr_file['Statement']) # fit tokenizer with train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences # To make vectors the same size. \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPool1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 300\n",
    "x_train = t.texts_to_sequences(tr_file['Statement'])\n",
    "x_dev = t.texts_to_sequences(va_file['Statement'])\n",
    "x_train = pad_sequences(x_train,maxlen=maxlen,padding='post')\n",
    "x_dev = pad_sequences(x_dev, maxlen=maxlen, padding='post')\n",
    "x_test = t.texts_to_sequences(te_file['Statement'])\n",
    "x_test =  pad_sequences(x_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(tr_file['m_label'], num_classes=6)\n",
    "y_dev = to_categorical(va_file['m_label'], num_classes=6)\n",
    "y_test =  to_categorical(te_file['m_label'], num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = len(t.word_index)+1\n",
    "filters = 128\n",
    "kernel_size = 2\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "embedding_dims = 300\n",
    "model = Sequential()\n",
    "maxlen = 300\n",
    "hidden_dims = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#Simple one hidden layer NN with a Convolutional layer for filtering and GlobalMaxPooling 1D \n",
    "model.add(Embedding(input_dim=max_features, \n",
    "                           output_dim=embedding_dims, \n",
    "                        input_length=maxlen,\n",
    "                         weights=[embedding_weights],\n",
    "                         trainable=False,\n",
    "                   ))\n",
    "model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPool1D(5))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool1D(5))\n",
    "model.add(Conv1D(filters=128, kernel_size=4, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.8))\n",
    "#model.add(Dense(hidden_dims, activation='relu'))\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_dev,y_dev),\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_dev, y_dev, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation ==> Loss: {0}, Accuracy: {1}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test ==Loss: {0}, Accuracy: {1}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training.iloc[6117:6120,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altered behavior of NLTK so CoreNLP performs sentence splits\n",
    "def constituency_parse(sentences, return_parse_obj=False):\n",
    "    core_nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "    \"\"\"Creates parse strings for each sentence.  \n",
    "    Each parse string can be fed into Tree.fromstring() to create NLTK Tree objects.\n",
    "\n",
    "    parser (CoreNLPParser): parser to parse sentences\n",
    "    sentences (str): essay text\n",
    "    return_parse_obj (bool): return parse object or string of trees\n",
    "    RETURNS (list): a list of parses in string form\n",
    "    \"\"\"\n",
    "    result = list()\n",
    "    index= 0\n",
    "    for s in sentences:\n",
    "        print(index)\n",
    "        default_properties = {'outputFormat': 'json', 'annotators': 'tokenize,pos,parse'}\n",
    "        parsed_data = core_nlp.annotate(s, properties=default_properties)\n",
    "        if return_parse_obj:\n",
    "            return parsed_data\n",
    "        else:\n",
    "            parses = list()\n",
    "            dependencies = list()\n",
    "            for parsed_sent in parsed_data['sentences']:\n",
    "                parse = parsed_sent['parse']\n",
    "                # Compress whitespace\n",
    "                parse = re.sub('[\\s]+', ' ', parse)\n",
    "                parses.append(parse)\n",
    "                if 'enhancedDependencies' in parsed_sent:\n",
    "                    for dep in parsed_sent['enhancedDependencies']:\n",
    "                        dependencies.append(dep['dep'])\n",
    "            result.append((parses, dependencies))\n",
    "        index += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for e in constituency_parse(tr_file['Statement'].values, return_parse_obj=True)['sentences'][0]['enhancedDependencies']:\n",
    "#    print(e['dep'])\n",
    "parsed_sentences=constituency_parse(features_training['text'].values, return_parse_obj=False)\n",
    "parsed_sentences_validation=constituency_parse(features_validation['text'].values, return_parse_obj=False)\n",
    "parsed_sentences_test =constituency_parse(features_testing['text'].values, return_parse_obj=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training['parenthesis'] = tr_file['parenthesis']\n",
    "features_validation['parenthesis']= va_file['parenthesis'] \n",
    "features_testing['parenthesis'] =te_file['parenthesis'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = ['ROOT','advcl','det','predet','preconj','vmod','mwe','mark','advmod','neg','rcmod','quantmod','nn','npadvmod','tmod','num','number','prep','poss','possessive','prt','parataxis','goeswith','punct','ref','sdep','xsubj','root','dep','aux','auxpass','cop','arg','agent','comp','acomp','ccomp','xcomp','obj','dobj','iobj','pobj','subj','nsubj','nsubjpass','csubj','csubjpass','cc','conj','expl','mod','amod','appos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_parsed = list()\n",
    "for each in parsed_sentences:\n",
    "     for each_p in each[1]:\n",
    "        if each_p.split(':')[0] not in unique_parsed:\n",
    "            unique_parsed.append(each_p.split(':')[0])\n",
    "    #print(each[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_parse = np.zeros((len(parsed_sentences), len(unique_parsed))) \n",
    "for x,each in enumerate(parsed_sentences):\n",
    "     for y,up in enumerate(unique_parsed):\n",
    "            if up in each[1]:\n",
    "                vector_parse[x,y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_parse_validation = np.zeros((len(parsed_sentences_validation), len(unique_parsed))) \n",
    "for x,each in enumerate(parsed_sentences_validation):\n",
    "     for y,up in enumerate(unique_parsed):\n",
    "            if up in each[1]:\n",
    "                vector_parse_validation[x,y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mi_parsed = mutual_info_classif(vector_parse,tr_file['b_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#filtered_unique_parser = list()\n",
    "#for index, each in enumerate(mi_parsed):\n",
    "#    if each>=0.005:\n",
    "#        filtered_unique_parser.append(unique_parsed[index])\n",
    "#        print(index, unique_parsed[index], each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstring = [\" \".join(x[1]) for x in parsed_sentences]\n",
    "xstringva = [\" \".join(x[1]) for x in parsed_sentences_validation]\n",
    "\n",
    "xstring = np.array([x.replace(':','_').replace('ROOT','') for x in xstring])\n",
    "xstringva = np.array([x.replace(':','_').replace('ROOT','') for x in xstringva])\n",
    "xtrain = xstring[(tr_file['b_label']!=0)]\n",
    "ytrain = tr_file['b_label'][(tr_file['b_label']!=0)]\n",
    "xtest = xstringva[(va_file['b_label']!=0)]\n",
    "ytest = va_file['b_label'][(va_file['b_label']!=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(penalty='l2', random_state=_RANDOM_STATE_V, max_iter=1000, C=0.5)),\n",
    "])\n",
    "\n",
    "text_clf.fit(xtrain,ytrain)\n",
    "pred = text_clf.predict(xtest)\n",
    "print('Accuracy: ', accuracy_score(pred, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DummyClassifier(strategy='most_frequent').fit(xtrain,ytrain)\n",
    "accuracy_score(dc.predict(xtest),ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for depth in range(1,20):\n",
    "dt_clf = DecisionTreeClassifier(max_depth = 3).fit(xtrain,ytrain)\n",
    "print(depth, accuracy_score(dt_clf.predict(xtest),ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, values in enumerate(dt_clf.feature_importances_):\n",
    "    if values>=0.05:\n",
    "        print(unique_parsed[index], values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({'id':tr_file['Id'],'label':tr_file['Label'],'parsed_sentences': parsed_sentences,'b_labels':tr_file['b_label'] ,'m_labels':tr_file['m_label']}, index= None).to_csv('parsed_sentences.csv',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"parameters = {'batch_size': [25, 32],\n",
    "          'epochs': [100, 500],\n",
    "          'optimizer': ['adam', 'rmsprop']}\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                       param_grid = parameters,\n",
    "                       scoring = 'accuracy',\n",
    "                       cv = 10)\n",
    "grid_search = grid_search.fit(x_train, y_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "_model = KerasClassifier(build_fn=model, verbose=0)\n",
    "\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=_model, param_grid=param_grid, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid.fit(x_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we can go ahead and set the parameter space\n",
    "p = {'lr': (0.5, 5, 10),\n",
    "     'first_neuron':[4, 8, 16, 32, 64],\n",
    "     'hidden_layers':[0, 1, 2],\n",
    "     'batch_size': (2, 30, 10, 64),\n",
    "     'epochs': [150],\n",
    "     'dropout': (0, 0.5,0.8, 5),\n",
    "     'weight_regulizer':[None],\n",
    "     'emb_output_dims': [None],\n",
    "     'shape':['brick','long_funnel'],\n",
    "     #'optimizer': [Adam, Nadam, RMSprop],\n",
    "     #'losses': [logcosh, binary_crossentropy],\n",
    "     #'activation':[relu, elu],\n",
    "     #'last_activation': 'sigmoid'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_model(xtrain, xdev, ytrain, ydev, params):\n",
    "    model = Sequential()\n",
    "    #Simple one hidden layer NN with a Convolutional layer for filtering and GlobalMaxPooling 1D \n",
    "    model.add(Embedding(input_dim=max_features, \n",
    "                               output_dim=embedding_dims, \n",
    "                               input_length=maxlen,\n",
    "                        weights=[embedding_weights],\n",
    "                         trainable=False,\n",
    "                       ))\n",
    "    model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPool1D(5))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPool1D(5))\n",
    "    model.add(Conv1D(filters=128, kernel_size=4, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    #model.add(Dense(hidden_dims, activation='relu'))\n",
    "    model.add(Dense(6, activation='sigmoid'))\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    history = model.fit(xtrain, ytrain,\n",
    "                        epochs=params['epochs'],\n",
    "                        validation_data=[xdev,ydev],\n",
    "                        batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = talos.Scan(x_train, y_train,\n",
    "          params=p,\n",
    "          dataset_name='first_test',\n",
    "          experiment_no='2',\n",
    "          model=nlp_model,\n",
    "          grid_downsample=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords # nltk stop words\n",
    "from nltk.tokenize import word_tokenize # nltk word tokenizer\n",
    "import string\n",
    "import re\n",
    "## Function to clean text data\n",
    "def lemmatize_remove_stop_words(corpus):\n",
    "    print('Processing data...')\n",
    "    result = list()\n",
    "    # define a lemmatizer\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # tokenize and remove stopwords from every sentence\n",
    "    stop_words = stopwords.words('english')\n",
    "    # iterate over every sentence and apply the three filters to remove stopwords, extract lemmas for verbs and nouns\n",
    "    for index, sentence in enumerate(corpus):\n",
    "        #print('Cleaning sentence number:', index, end=\"##\")\n",
    "        #print('Making text all lower case...',end=\"--\")\n",
    "        clean_text = sentence.lower() #  make all words lower case\n",
    "        #print('Removing punctuation...', end=\"--\")\n",
    "        clean_text = re.sub(r'[{0}]'.format(string.punctuation),'',clean_text)# remove punctuation\n",
    "        #print('Removing numberic data and symbols...', end=\"--\")\n",
    "        clean_text = re.sub(r'\\w*\\d\\w*','',clean_text) # remove alpha numerics\n",
    "        #print('Splitting into tokens...', end=\"--\")\n",
    "        tokenized_sentence = word_tokenize(clean_text) # split sentence into tokens\n",
    "        #print('Lemmatizing verbs...',end=\"--\")\n",
    "        filter_one = [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in tokenized_sentence] # lemmatize verbs\n",
    "        #print('Lemmatizing nouns...',end=\"--\")\n",
    "        filter_two = [wordnet_lemmatizer.lemmatize(word, pos=\"n\") for word in filter_one] # lemmatize nouns\n",
    "        #print('Removing stop words...',end=\"--\")\n",
    "        filter_three = [w for w in filter_two if w not in stop_words] # remove stop words\n",
    "        #print('Removing extra white spaces',end=\"--\")\n",
    "        all_clean = (' '.join(filter_three)).strip() # remove extra white spaces\n",
    "        #print('All clean for sentence: ', index,end=\"\\r\")\n",
    "        result.append(all_clean) # append data to result\n",
    "    print('Finished!')\n",
    "    return np.array(result) # convert to numpy array and return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING ML MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training\n",
    "features_validation\n",
    "features_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = lemmatize_remove_stop_words(features_training['text'])\n",
    "Xdev = lemmatize_remove_stop_words(features_validation['text'])\n",
    "Xtest = lemmatize_remove_stop_words(features_testing['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import different models that we want to train\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RANDOM_STATE_V = 45\n",
    "models = [DummyClassifier,\n",
    "          LogisticRegression, \n",
    "          SVC,\n",
    "          ExtraTreesClassifier,\n",
    "          RandomForestClassifier,\n",
    "          AdaBoostClassifier,\n",
    "          GradientBoostingClassifier\n",
    "         ]#, \n",
    "          #SVC]\n",
    "defaults = [{'strategy':'most_frequent', 'random_state': _RANDOM_STATE_V}, #Baseline\n",
    "            {'penalty':'l2','tol':0.0001, 'C':1.0,'max_iter':100,'random_state': _RANDOM_STATE_V}, #Linear Regression\n",
    "            {'C':1.0, 'kernel':'linear', 'degree':3, 'gamma':1,'random_state': _RANDOM_STATE_V},\n",
    "            {'n_estimators':10, 'criterion': 'entropy', 'max_depth':2, 'min_samples_split':2,'random_state': _RANDOM_STATE_V},\n",
    "            {'n_estimators':10, 'criterion': 'entropy', 'max_depth':2, 'min_samples_split':2,'random_state': _RANDOM_STATE_V},\n",
    "            {'n_estimators':50, 'learning_rate':1.0, 'algorithm':'SAMME.R', 'random_state': _RANDOM_STATE_V},\n",
    "            {'loss':'deviance', 'learning_rate':0.1, 'n_estimators':100}\n",
    "            #{'n_neighbors':5},\n",
    "           #{}\n",
    "           ]#, #KNeighborsClassifier\n",
    "            #{'C':1.0, 'kernel':'rbf', 'degree':3, 'gamma':1,'random_state': _RANDOM_STATE_V}, #SVC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = Xtrain[(features_training['b_label']!=0)]\n",
    "Ytr = features_training['b_label'][features_training['b_label']!=0]\n",
    "Xde = Xdev[(features_validation['b_label']!=0)]\n",
    "Yde = features_validation['b_label'][features_validation['b_label']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModels(Xtr, Ytr, Xde, Yde, vocab= None, verbose= True):\n",
    "    report_list = list()\n",
    "    for index, model in enumerate(models):\n",
    "        text_clf = Pipeline([\n",
    "            ('vect', CountVectorizer(vocabulary=vocab)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', model(**defaults[index])),\n",
    "        ])\n",
    "        #text_clf = model(**defaults[index])\n",
    "        if verbose:\n",
    "            print('Training {0}'.format(model.__name__))\n",
    "        text_clf.fit(Xtr, Ytr)\n",
    "        pred = text_clf.predict(Xde)\n",
    "        #print('Accuracy: ', np.mean(pred==Yde))\n",
    "        if verbose:\n",
    "            print('Accuracy: ', accuracy_score(pred,Yde))\n",
    "            print(classification_report(pred, Yde))\n",
    "        #report_list.append((model.__name__, np.mean(pred==Yde), f1_score(pred, Yde, average='binary', pos_label=1),f1_score(pred, Yde, average='binary', pos_label=-1)))\n",
    "        report_list.append((model.__name__, accuracy_score(pred,Yde)))\n",
    "    if verbose:\n",
    "        for e in report_list:\n",
    "            #print('{0},{1},{2}'.format(e[1], e[2], e[3]))\n",
    "            print('{0}: {1}'.format(e[0], e[1]))          \n",
    "    return report_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModels(Xtr, Ytr, Xde, Yde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_training = features_training['pos']\n",
    "pos_validation = features_validation['pos']\n",
    "pos_testing = features_testing['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list()\n",
    "for pos in pos_training:\n",
    "    for ep in pos.split(' '):\n",
    "        if ep not in vocab:\n",
    "            vocab.append(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_str =  [x.lower().replace('$','dollar') for x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Report for every individual tag\n",
    "report_pos = list()\n",
    "for i in range(0,len(vocab)-1):\n",
    "    vocab_filtered = vocab_str[i:i+1]\n",
    "    print(vocab_filtered)\n",
    "    report = trainModels(pos_training[(tr_file['b_label']!=0)], Ytr, pos_validation[(va_file['b_label']!=0)], Yde, vocab=vocab_filtered, \n",
    "                verbose=False)\n",
    "    report_pos.append((vocab_filtered, report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['vbz'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['dt'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['nnps'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['vbp'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['jj'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['nn'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['nns'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['in'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['wrb'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['vbd'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['prp'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['rp'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['wdt'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['to'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['vb'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['nnp'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5868725868725869),\n",
       "   ('SVC', 0.5868725868725869),\n",
       "   ('ExtraTreesClassifier', 0.5868725868725869),\n",
       "   ('RandomForestClassifier', 0.5868725868725869),\n",
       "   ('AdaBoostClassifier', 0.5868725868725869),\n",
       "   ('GradientBoostingClassifier', 0.5868725868725869)]),\n",
       " (['vbg'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['prpdollar'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['vbn'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['rbr'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5965250965250966),\n",
       "   ('SVC', 0.5965250965250966),\n",
       "   ('ExtraTreesClassifier', 0.5965250965250966),\n",
       "   ('RandomForestClassifier', 0.5965250965250966),\n",
       "   ('AdaBoostClassifier', 0.5965250965250966),\n",
       "   ('GradientBoostingClassifier', 0.5965250965250966)]),\n",
       " (['cd'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5868725868725869),\n",
       "   ('SVC', 0.5868725868725869),\n",
       "   ('ExtraTreesClassifier', 0.5868725868725869),\n",
       "   ('RandomForestClassifier', 0.5868725868725869),\n",
       "   ('AdaBoostClassifier', 0.5868725868725869),\n",
       "   ('GradientBoostingClassifier', 0.5868725868725869)]),\n",
       " (['rb'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['wp'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['jjs'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5994208494208494),\n",
       "   ('SVC', 0.5994208494208494),\n",
       "   ('ExtraTreesClassifier', 0.5994208494208494),\n",
       "   ('RandomForestClassifier', 0.5994208494208494),\n",
       "   ('AdaBoostClassifier', 0.5994208494208494),\n",
       "   ('GradientBoostingClassifier', 0.5994208494208494)]),\n",
       " (['cc'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['jjr'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.6206563706563707),\n",
       "   ('SVC', 0.6206563706563707),\n",
       "   ('ExtraTreesClassifier', 0.6206563706563707),\n",
       "   ('RandomForestClassifier', 0.6206563706563707),\n",
       "   ('AdaBoostClassifier', 0.6206563706563707),\n",
       "   ('GradientBoostingClassifier', 0.6206563706563707)]),\n",
       " (['md'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['pdt'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5926640926640927),\n",
       "   ('SVC', 0.5926640926640927),\n",
       "   ('ExtraTreesClassifier', 0.5926640926640927),\n",
       "   ('RandomForestClassifier', 0.5926640926640927),\n",
       "   ('AdaBoostClassifier', 0.5926640926640927),\n",
       "   ('GradientBoostingClassifier', 0.5926640926640927)]),\n",
       " (['ex'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5955598455598455),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['rbs'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5916988416988417),\n",
       "   ('SVC', 0.5916988416988417),\n",
       "   ('ExtraTreesClassifier', 0.5916988416988417),\n",
       "   ('RandomForestClassifier', 0.5916988416988417),\n",
       "   ('AdaBoostClassifier', 0.5916988416988417),\n",
       "   ('GradientBoostingClassifier', 0.5916988416988417)]),\n",
       " (['uh'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['wpdollar'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5955598455598455),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5955598455598455),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['fw'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)])]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vbz', 'dt']\n",
      "['vbz', 'nnps']\n",
      "['vbz', 'vbp']\n",
      "['vbz', 'jj']\n",
      "['vbz', 'nn']\n",
      "['vbz', 'nns']\n",
      "['vbz', 'in']\n",
      "['vbz', 'wrb']\n",
      "['vbz', 'vbd']\n",
      "['vbz', 'prp']\n",
      "['vbz', 'rp']\n",
      "['vbz', 'wdt']\n",
      "['vbz', 'to']\n",
      "['vbz', 'vb']\n",
      "['vbz', 'nnp']\n",
      "['vbz', 'vbg']\n",
      "['vbz', 'prpdollar']\n",
      "['vbz', 'vbn']\n",
      "['vbz', 'rbr']\n",
      "['vbz', 'cd']\n",
      "['vbz', 'rb']\n",
      "['vbz', 'wp']\n",
      "['vbz', 'jjs']\n",
      "['vbz', 'cc']\n",
      "['vbz', 'jjr']\n",
      "['vbz', 'md']\n",
      "['vbz', 'pdt']\n",
      "['vbz', 'ex']\n",
      "['vbz', 'rbs']\n",
      "['vbz', 'uh']\n",
      "['vbz', 'wpdollar']\n",
      "['vbz', 'fw']\n",
      "['vbz', 'ls']\n",
      "['dt', 'nnps']\n",
      "['dt', 'vbp']\n",
      "['dt', 'jj']\n",
      "['dt', 'nn']\n",
      "['dt', 'nns']\n",
      "['dt', 'in']\n",
      "['dt', 'wrb']\n",
      "['dt', 'vbd']\n",
      "['dt', 'prp']\n",
      "['dt', 'rp']\n",
      "['dt', 'wdt']\n",
      "['dt', 'to']\n",
      "['dt', 'vb']\n",
      "['dt', 'nnp']\n",
      "['dt', 'vbg']\n",
      "['dt', 'prpdollar']\n",
      "['dt', 'vbn']\n",
      "['dt', 'rbr']\n",
      "['dt', 'cd']\n",
      "['dt', 'rb']\n",
      "['dt', 'wp']\n",
      "['dt', 'jjs']\n",
      "['dt', 'cc']\n",
      "['dt', 'jjr']\n",
      "['dt', 'md']\n",
      "['dt', 'pdt']\n",
      "['dt', 'ex']\n",
      "['dt', 'rbs']\n",
      "['dt', 'uh']\n",
      "['dt', 'wpdollar']\n",
      "['dt', 'fw']\n",
      "['dt', 'ls']\n",
      "['nnps', 'vbp']\n",
      "['nnps', 'jj']\n",
      "['nnps', 'nn']\n",
      "['nnps', 'nns']\n",
      "['nnps', 'in']\n",
      "['nnps', 'wrb']\n",
      "['nnps', 'vbd']\n",
      "['nnps', 'prp']\n",
      "['nnps', 'rp']\n",
      "['nnps', 'wdt']\n",
      "['nnps', 'to']\n",
      "['nnps', 'vb']\n",
      "['nnps', 'nnp']\n",
      "['nnps', 'vbg']\n",
      "['nnps', 'prpdollar']\n",
      "['nnps', 'vbn']\n",
      "['nnps', 'rbr']\n",
      "['nnps', 'cd']\n",
      "['nnps', 'rb']\n",
      "['nnps', 'wp']\n",
      "['nnps', 'jjs']\n",
      "['nnps', 'cc']\n",
      "['nnps', 'jjr']\n",
      "['nnps', 'md']\n",
      "['nnps', 'pdt']\n",
      "['nnps', 'ex']\n",
      "['nnps', 'rbs']\n",
      "['nnps', 'uh']\n",
      "['nnps', 'wpdollar']\n",
      "['nnps', 'fw']\n",
      "['nnps', 'ls']\n",
      "['vbp', 'jj']\n",
      "['vbp', 'nn']\n",
      "['vbp', 'nns']\n",
      "['vbp', 'in']\n",
      "['vbp', 'wrb']\n",
      "['vbp', 'vbd']\n",
      "['vbp', 'prp']\n",
      "['vbp', 'rp']\n",
      "['vbp', 'wdt']\n",
      "['vbp', 'to']\n",
      "['vbp', 'vb']\n",
      "['vbp', 'nnp']\n",
      "['vbp', 'vbg']\n",
      "['vbp', 'prpdollar']\n",
      "['vbp', 'vbn']\n",
      "['vbp', 'rbr']\n",
      "['vbp', 'cd']\n",
      "['vbp', 'rb']\n",
      "['vbp', 'wp']\n",
      "['vbp', 'jjs']\n",
      "['vbp', 'cc']\n",
      "['vbp', 'jjr']\n",
      "['vbp', 'md']\n",
      "['vbp', 'pdt']\n",
      "['vbp', 'ex']\n",
      "['vbp', 'rbs']\n",
      "['vbp', 'uh']\n",
      "['vbp', 'wpdollar']\n",
      "['vbp', 'fw']\n",
      "['vbp', 'ls']\n",
      "['jj', 'nn']\n",
      "['jj', 'nns']\n",
      "['jj', 'in']\n",
      "['jj', 'wrb']\n",
      "['jj', 'vbd']\n",
      "['jj', 'prp']\n",
      "['jj', 'rp']\n",
      "['jj', 'wdt']\n",
      "['jj', 'to']\n",
      "['jj', 'vb']\n",
      "['jj', 'nnp']\n",
      "['jj', 'vbg']\n",
      "['jj', 'prpdollar']\n",
      "['jj', 'vbn']\n",
      "['jj', 'rbr']\n",
      "['jj', 'cd']\n",
      "['jj', 'rb']\n",
      "['jj', 'wp']\n",
      "['jj', 'jjs']\n",
      "['jj', 'cc']\n",
      "['jj', 'jjr']\n",
      "['jj', 'md']\n",
      "['jj', 'pdt']\n",
      "['jj', 'ex']\n",
      "['jj', 'rbs']\n",
      "['jj', 'uh']\n",
      "['jj', 'wpdollar']\n",
      "['jj', 'fw']\n",
      "['jj', 'ls']\n",
      "['nn', 'nns']\n",
      "['nn', 'in']\n",
      "['nn', 'wrb']\n",
      "['nn', 'vbd']\n",
      "['nn', 'prp']\n",
      "['nn', 'rp']\n",
      "['nn', 'wdt']\n",
      "['nn', 'to']\n",
      "['nn', 'vb']\n",
      "['nn', 'nnp']\n",
      "['nn', 'vbg']\n",
      "['nn', 'prpdollar']\n",
      "['nn', 'vbn']\n",
      "['nn', 'rbr']\n",
      "['nn', 'cd']\n",
      "['nn', 'rb']\n",
      "['nn', 'wp']\n",
      "['nn', 'jjs']\n",
      "['nn', 'cc']\n",
      "['nn', 'jjr']\n",
      "['nn', 'md']\n",
      "['nn', 'pdt']\n",
      "['nn', 'ex']\n",
      "['nn', 'rbs']\n",
      "['nn', 'uh']\n",
      "['nn', 'wpdollar']\n",
      "['nn', 'fw']\n",
      "['nn', 'ls']\n",
      "['nns', 'in']\n",
      "['nns', 'wrb']\n",
      "['nns', 'vbd']\n",
      "['nns', 'prp']\n",
      "['nns', 'rp']\n",
      "['nns', 'wdt']\n",
      "['nns', 'to']\n",
      "['nns', 'vb']\n",
      "['nns', 'nnp']\n",
      "['nns', 'vbg']\n",
      "['nns', 'prpdollar']\n",
      "['nns', 'vbn']\n",
      "['nns', 'rbr']\n",
      "['nns', 'cd']\n",
      "['nns', 'rb']\n",
      "['nns', 'wp']\n",
      "['nns', 'jjs']\n",
      "['nns', 'cc']\n",
      "['nns', 'jjr']\n",
      "['nns', 'md']\n",
      "['nns', 'pdt']\n",
      "['nns', 'ex']\n",
      "['nns', 'rbs']\n",
      "['nns', 'uh']\n",
      "['nns', 'wpdollar']\n",
      "['nns', 'fw']\n",
      "['nns', 'ls']\n",
      "['in', 'wrb']\n",
      "['in', 'vbd']\n",
      "['in', 'prp']\n",
      "['in', 'rp']\n",
      "['in', 'wdt']\n",
      "['in', 'to']\n",
      "['in', 'vb']\n",
      "['in', 'nnp']\n",
      "['in', 'vbg']\n",
      "['in', 'prpdollar']\n",
      "['in', 'vbn']\n",
      "['in', 'rbr']\n",
      "['in', 'cd']\n",
      "['in', 'rb']\n",
      "['in', 'wp']\n",
      "['in', 'jjs']\n",
      "['in', 'cc']\n",
      "['in', 'jjr']\n",
      "['in', 'md']\n",
      "['in', 'pdt']\n",
      "['in', 'ex']\n",
      "['in', 'rbs']\n",
      "['in', 'uh']\n",
      "['in', 'wpdollar']\n",
      "['in', 'fw']\n",
      "['in', 'ls']\n",
      "['wrb', 'vbd']\n",
      "['wrb', 'prp']\n",
      "['wrb', 'rp']\n",
      "['wrb', 'wdt']\n",
      "['wrb', 'to']\n",
      "['wrb', 'vb']\n",
      "['wrb', 'nnp']\n",
      "['wrb', 'vbg']\n",
      "['wrb', 'prpdollar']\n",
      "['wrb', 'vbn']\n",
      "['wrb', 'rbr']\n",
      "['wrb', 'cd']\n",
      "['wrb', 'rb']\n",
      "['wrb', 'wp']\n",
      "['wrb', 'jjs']\n",
      "['wrb', 'cc']\n",
      "['wrb', 'jjr']\n",
      "['wrb', 'md']\n",
      "['wrb', 'pdt']\n",
      "['wrb', 'ex']\n",
      "['wrb', 'rbs']\n",
      "['wrb', 'uh']\n",
      "['wrb', 'wpdollar']\n",
      "['wrb', 'fw']\n",
      "['wrb', 'ls']\n",
      "['vbd', 'prp']\n",
      "['vbd', 'rp']\n",
      "['vbd', 'wdt']\n",
      "['vbd', 'to']\n",
      "['vbd', 'vb']\n",
      "['vbd', 'nnp']\n",
      "['vbd', 'vbg']\n",
      "['vbd', 'prpdollar']\n",
      "['vbd', 'vbn']\n",
      "['vbd', 'rbr']\n",
      "['vbd', 'cd']\n",
      "['vbd', 'rb']\n",
      "['vbd', 'wp']\n",
      "['vbd', 'jjs']\n",
      "['vbd', 'cc']\n",
      "['vbd', 'jjr']\n",
      "['vbd', 'md']\n",
      "['vbd', 'pdt']\n",
      "['vbd', 'ex']\n",
      "['vbd', 'rbs']\n",
      "['vbd', 'uh']\n",
      "['vbd', 'wpdollar']\n",
      "['vbd', 'fw']\n",
      "['vbd', 'ls']\n",
      "['prp', 'rp']\n",
      "['prp', 'wdt']\n",
      "['prp', 'to']\n",
      "['prp', 'vb']\n",
      "['prp', 'nnp']\n",
      "['prp', 'vbg']\n",
      "['prp', 'prpdollar']\n",
      "['prp', 'vbn']\n",
      "['prp', 'rbr']\n",
      "['prp', 'cd']\n",
      "['prp', 'rb']\n",
      "['prp', 'wp']\n",
      "['prp', 'jjs']\n",
      "['prp', 'cc']\n",
      "['prp', 'jjr']\n",
      "['prp', 'md']\n",
      "['prp', 'pdt']\n",
      "['prp', 'ex']\n",
      "['prp', 'rbs']\n",
      "['prp', 'uh']\n",
      "['prp', 'wpdollar']\n",
      "['prp', 'fw']\n",
      "['prp', 'ls']\n",
      "['rp', 'wdt']\n",
      "['rp', 'to']\n",
      "['rp', 'vb']\n",
      "['rp', 'nnp']\n",
      "['rp', 'vbg']\n",
      "['rp', 'prpdollar']\n",
      "['rp', 'vbn']\n",
      "['rp', 'rbr']\n",
      "['rp', 'cd']\n",
      "['rp', 'rb']\n",
      "['rp', 'wp']\n",
      "['rp', 'jjs']\n",
      "['rp', 'cc']\n",
      "['rp', 'jjr']\n",
      "['rp', 'md']\n",
      "['rp', 'pdt']\n",
      "['rp', 'ex']\n",
      "['rp', 'rbs']\n",
      "['rp', 'uh']\n",
      "['rp', 'wpdollar']\n",
      "['rp', 'fw']\n",
      "['rp', 'ls']\n",
      "['wdt', 'to']\n",
      "['wdt', 'vb']\n",
      "['wdt', 'nnp']\n",
      "['wdt', 'vbg']\n",
      "['wdt', 'prpdollar']\n",
      "['wdt', 'vbn']\n",
      "['wdt', 'rbr']\n",
      "['wdt', 'cd']\n",
      "['wdt', 'rb']\n",
      "['wdt', 'wp']\n",
      "['wdt', 'jjs']\n",
      "['wdt', 'cc']\n",
      "['wdt', 'jjr']\n",
      "['wdt', 'md']\n",
      "['wdt', 'pdt']\n",
      "['wdt', 'ex']\n",
      "['wdt', 'rbs']\n",
      "['wdt', 'uh']\n",
      "['wdt', 'wpdollar']\n",
      "['wdt', 'fw']\n",
      "['wdt', 'ls']\n",
      "['to', 'vb']\n",
      "['to', 'nnp']\n",
      "['to', 'vbg']\n",
      "['to', 'prpdollar']\n",
      "['to', 'vbn']\n",
      "['to', 'rbr']\n",
      "['to', 'cd']\n",
      "['to', 'rb']\n",
      "['to', 'wp']\n",
      "['to', 'jjs']\n",
      "['to', 'cc']\n",
      "['to', 'jjr']\n",
      "['to', 'md']\n",
      "['to', 'pdt']\n",
      "['to', 'ex']\n",
      "['to', 'rbs']\n",
      "['to', 'uh']\n",
      "['to', 'wpdollar']\n",
      "['to', 'fw']\n",
      "['to', 'ls']\n",
      "['vb', 'nnp']\n",
      "['vb', 'vbg']\n",
      "['vb', 'prpdollar']\n",
      "['vb', 'vbn']\n",
      "['vb', 'rbr']\n",
      "['vb', 'cd']\n",
      "['vb', 'rb']\n",
      "['vb', 'wp']\n",
      "['vb', 'jjs']\n",
      "['vb', 'cc']\n",
      "['vb', 'jjr']\n",
      "['vb', 'md']\n",
      "['vb', 'pdt']\n",
      "['vb', 'ex']\n",
      "['vb', 'rbs']\n",
      "['vb', 'uh']\n",
      "['vb', 'wpdollar']\n",
      "['vb', 'fw']\n",
      "['vb', 'ls']\n",
      "['nnp', 'vbg']\n",
      "['nnp', 'prpdollar']\n",
      "['nnp', 'vbn']\n",
      "['nnp', 'rbr']\n",
      "['nnp', 'cd']\n",
      "['nnp', 'rb']\n",
      "['nnp', 'wp']\n",
      "['nnp', 'jjs']\n",
      "['nnp', 'cc']\n",
      "['nnp', 'jjr']\n",
      "['nnp', 'md']\n",
      "['nnp', 'pdt']\n",
      "['nnp', 'ex']\n",
      "['nnp', 'rbs']\n",
      "['nnp', 'uh']\n",
      "['nnp', 'wpdollar']\n",
      "['nnp', 'fw']\n",
      "['nnp', 'ls']\n",
      "['vbg', 'prpdollar']\n",
      "['vbg', 'vbn']\n",
      "['vbg', 'rbr']\n",
      "['vbg', 'cd']\n",
      "['vbg', 'rb']\n",
      "['vbg', 'wp']\n",
      "['vbg', 'jjs']\n",
      "['vbg', 'cc']\n",
      "['vbg', 'jjr']\n",
      "['vbg', 'md']\n",
      "['vbg', 'pdt']\n",
      "['vbg', 'ex']\n",
      "['vbg', 'rbs']\n",
      "['vbg', 'uh']\n",
      "['vbg', 'wpdollar']\n",
      "['vbg', 'fw']\n",
      "['vbg', 'ls']\n",
      "['prpdollar', 'vbn']\n",
      "['prpdollar', 'rbr']\n",
      "['prpdollar', 'cd']\n",
      "['prpdollar', 'rb']\n",
      "['prpdollar', 'wp']\n",
      "['prpdollar', 'jjs']\n",
      "['prpdollar', 'cc']\n",
      "['prpdollar', 'jjr']\n",
      "['prpdollar', 'md']\n",
      "['prpdollar', 'pdt']\n",
      "['prpdollar', 'ex']\n",
      "['prpdollar', 'rbs']\n",
      "['prpdollar', 'uh']\n",
      "['prpdollar', 'wpdollar']\n",
      "['prpdollar', 'fw']\n",
      "['prpdollar', 'ls']\n",
      "['vbn', 'rbr']\n",
      "['vbn', 'cd']\n",
      "['vbn', 'rb']\n",
      "['vbn', 'wp']\n",
      "['vbn', 'jjs']\n",
      "['vbn', 'cc']\n",
      "['vbn', 'jjr']\n",
      "['vbn', 'md']\n",
      "['vbn', 'pdt']\n",
      "['vbn', 'ex']\n",
      "['vbn', 'rbs']\n",
      "['vbn', 'uh']\n",
      "['vbn', 'wpdollar']\n",
      "['vbn', 'fw']\n",
      "['vbn', 'ls']\n",
      "['rbr', 'cd']\n",
      "['rbr', 'rb']\n",
      "['rbr', 'wp']\n",
      "['rbr', 'jjs']\n",
      "['rbr', 'cc']\n",
      "['rbr', 'jjr']\n",
      "['rbr', 'md']\n",
      "['rbr', 'pdt']\n",
      "['rbr', 'ex']\n",
      "['rbr', 'rbs']\n",
      "['rbr', 'uh']\n",
      "['rbr', 'wpdollar']\n",
      "['rbr', 'fw']\n",
      "['rbr', 'ls']\n",
      "['cd', 'rb']\n",
      "['cd', 'wp']\n",
      "['cd', 'jjs']\n",
      "['cd', 'cc']\n",
      "['cd', 'jjr']\n",
      "['cd', 'md']\n",
      "['cd', 'pdt']\n",
      "['cd', 'ex']\n",
      "['cd', 'rbs']\n",
      "['cd', 'uh']\n",
      "['cd', 'wpdollar']\n",
      "['cd', 'fw']\n",
      "['cd', 'ls']\n",
      "['rb', 'wp']\n",
      "['rb', 'jjs']\n",
      "['rb', 'cc']\n",
      "['rb', 'jjr']\n",
      "['rb', 'md']\n",
      "['rb', 'pdt']\n",
      "['rb', 'ex']\n",
      "['rb', 'rbs']\n",
      "['rb', 'uh']\n",
      "['rb', 'wpdollar']\n",
      "['rb', 'fw']\n",
      "['rb', 'ls']\n",
      "['wp', 'jjs']\n",
      "['wp', 'cc']\n",
      "['wp', 'jjr']\n",
      "['wp', 'md']\n",
      "['wp', 'pdt']\n",
      "['wp', 'ex']\n",
      "['wp', 'rbs']\n",
      "['wp', 'uh']\n",
      "['wp', 'wpdollar']\n",
      "['wp', 'fw']\n",
      "['wp', 'ls']\n",
      "['jjs', 'cc']\n",
      "['jjs', 'jjr']\n",
      "['jjs', 'md']\n",
      "['jjs', 'pdt']\n",
      "['jjs', 'ex']\n",
      "['jjs', 'rbs']\n",
      "['jjs', 'uh']\n",
      "['jjs', 'wpdollar']\n",
      "['jjs', 'fw']\n",
      "['jjs', 'ls']\n",
      "['cc', 'jjr']\n",
      "['cc', 'md']\n",
      "['cc', 'pdt']\n",
      "['cc', 'ex']\n",
      "['cc', 'rbs']\n",
      "['cc', 'uh']\n",
      "['cc', 'wpdollar']\n",
      "['cc', 'fw']\n",
      "['cc', 'ls']\n",
      "['jjr', 'md']\n",
      "['jjr', 'pdt']\n",
      "['jjr', 'ex']\n",
      "['jjr', 'rbs']\n",
      "['jjr', 'uh']\n",
      "['jjr', 'wpdollar']\n",
      "['jjr', 'fw']\n",
      "['jjr', 'ls']\n",
      "['md', 'pdt']\n",
      "['md', 'ex']\n",
      "['md', 'rbs']\n",
      "['md', 'uh']\n",
      "['md', 'wpdollar']\n",
      "['md', 'fw']\n",
      "['md', 'ls']\n",
      "['pdt', 'ex']\n",
      "['pdt', 'rbs']\n",
      "['pdt', 'uh']\n",
      "['pdt', 'wpdollar']\n",
      "['pdt', 'fw']\n",
      "['pdt', 'ls']\n",
      "['ex', 'rbs']\n",
      "['ex', 'uh']\n",
      "['ex', 'wpdollar']\n",
      "['ex', 'fw']\n",
      "['ex', 'ls']\n",
      "['rbs', 'uh']\n",
      "['rbs', 'wpdollar']\n",
      "['rbs', 'fw']\n",
      "['rbs', 'ls']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uh', 'wpdollar']\n",
      "['uh', 'fw']\n",
      "['uh', 'ls']\n",
      "['wpdollar', 'fw']\n",
      "['wpdollar', 'ls']\n",
      "['fw', 'ls']\n"
     ]
    }
   ],
   "source": [
    "# Sequentially Adding pos.\n",
    "report_pos_sequential = list()\n",
    "for x in range(0,len(vocab)):\n",
    "    for y in range(x+1, len(vocab)):\n",
    "        vocab_filtered = [vocab_str[x],vocab_str[y]]\n",
    "        print(vocab_filtered)\n",
    "        report = trainModels(pos_training[(tr_file['b_label']!=0)], Ytr, pos_validation[(va_file['b_label']!=0)], Yde, vocab=vocab_filtered, \n",
    "                    verbose=False)\n",
    "        report_pos_sequential.append((vocab_filtered, report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_jjr_list = list()\n",
    "for x in report_pos_sequential:\n",
    "    for y in x[1]:\n",
    "        if y[1]>0.6207:\n",
    "            #print(x[0])\n",
    "            #print(y)\n",
    "            filtered_jjr_list.append(x[0][0])\n",
    "            filtered_jjr_list.append(x[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_jjr_list = np.unique(filtered_jjr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp',\n",
       "       'nns', 'prp', 'vb', 'vbd', 'vbg', 'vbp'], dtype='<U3')"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_jjr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'vbz']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'nnps']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'nn']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'wrb']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'rp']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'wdt']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'to']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'prpdollar']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'vbn']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'rbr']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'rb']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'wp']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'md']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'pdt']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'rbs']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'uh']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'wpdollar']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'fw']\n"
     ]
    }
   ],
   "source": [
    "# Sequentially Adding pos.\n",
    "report_pos_sequential_jjr = list()\n",
    "for x in range(0,len(vocab_str)):\n",
    "    #for y in range(x+1, len(vocab)):\n",
    "    if vocab_str[x] not in filtered_jjr_list:\n",
    "        vocab_filtered = np.append(filtered_jjr_list, vocab_str[x])\n",
    "        print(vocab_filtered)\n",
    "        report = trainModels(pos_training[(tr_file['b_label']!=0)], Ytr, pos_validation[(va_file['b_label']!=0)], Yde, vocab=vocab_filtered, \n",
    "                    verbose=False)\n",
    "        report_pos_sequential_jjr.append((vocab_filtered, report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered_jjr_list_2 = list()\n",
    "for x in report_pos_sequential_jjr:\n",
    "    for y in x[1]:\n",
    "        if y[1]>0.635:\n",
    "            for vv in x[0]:\n",
    "                filtered_jjr_list_2.append(vv)\n",
    "            #print(x[0])\n",
    "            #print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_jjr_list_2 = np.unique(filtered_jjr_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'vbz']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'nnps']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'nn']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wrb']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wdt']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'to']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'vbn']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'rbr']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wp']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'md']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'rbs']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'uh']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'fw']\n"
     ]
    }
   ],
   "source": [
    "# Sequentially Adding pos.\n",
    "report_pos_sequential_jjr_2 = list()\n",
    "for x in range(0,len(vocab_str)):\n",
    "    #for y in range(x+1, len(vocab)):\n",
    "    if vocab_str[x] not in filtered_jjr_list_2:\n",
    "        vocab_filtered = np.append(filtered_jjr_list_2, vocab_str[x])\n",
    "        print(vocab_filtered)\n",
    "        report = trainModels(pos_training[(tr_file['b_label']!=0)], Ytr, pos_validation[(va_file['b_label']!=0)], Yde, vocab=vocab_filtered, \n",
    "                    verbose=False)\n",
    "        report_pos_sequential_jjr_2.append((vocab_filtered, report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6438\n",
    "filtered_jjr_list_3 = list()\n",
    "for x in report_pos_sequential_jjr_2:\n",
    "    for y in x[1]:\n",
    "        if y[1]>0.6438:\n",
    "             for vv in x[0]:\n",
    "                filtered_jjr_list_3.append(vv)\n",
    "            #print(x[0])\n",
    "            #print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_jjr_list_3 = np.unique(filtered_jjr_list_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'vbz']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'nnps']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'nn']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wrb']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wdt']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'to']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'vbn']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'rbr']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wp']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'rbs']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'uh']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'fw']\n"
     ]
    }
   ],
   "source": [
    "# Sequentially Adding pos.\n",
    "report_pos_sequential_jjr_3 = list()\n",
    "for x in range(0,len(vocab_str)):\n",
    "    #for y in range(x+1, len(vocab)):\n",
    "    if vocab_str[x] not in filtered_jjr_list_3:\n",
    "        vocab_filtered = np.append(filtered_jjr_list_2, vocab_str[x])\n",
    "        print(vocab_filtered)\n",
    "        report = trainModels(pos_training[(tr_file['b_label']!=0)], Ytr, pos_validation[(va_file['b_label']!=0)], Yde, vocab=vocab_filtered, \n",
    "                    verbose=False)\n",
    "        report_pos_sequential_jjr_3.append((vocab_filtered, report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_pos_sequential_jjr_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'vbn']\n",
      "('GradientBoostingClassifier', 0.6428571428571429)\n"
     ]
    }
   ],
   "source": [
    "# 0.6438\n",
    "filtered_jjr_list_4 = list()\n",
    "for x in report_pos_sequential_jjr_3:\n",
    "    for y in x[1]:\n",
    "        if y[1]>0.64:\n",
    "            for vv in x[0]:\n",
    "                filtered_jjr_list_4.append(vv)\n",
    "            print(x[0])\n",
    "            print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_jjr_list_4 = np.unique(filtered_jjr_list_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'vbz']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'nnps']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'nn']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wrb']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wdt']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'to']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'rbr']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wp']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'md']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'rbs']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'uh']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'fw']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-243-fcc93d9f550e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_filtered\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         report = trainModels(pos_training[(tr_file['b_label']!=0)], Ytr, pos_validation[(va_file['b_label']!=0)], Yde, vocab=vocab_filtered, \n\u001b[1;32m----> 9\u001b[1;33m                     verbose=False)\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mreport_pos_sequential_jjr_4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_filtered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-170-6675eb3f80d6>\u001b[0m in \u001b[0;36mtrainModels\u001b[1;34m(Xtr, Ytr, Xde, Yde, vocab, verbose)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mtext_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXde\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m#print('Accuracy: ', np.mean(pred==Yde))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_sparse_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshrinking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m                 random_seed)\n\u001b[0m\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\svm\\libsvm_sparse.pyx\u001b[0m in \u001b[0;36msklearn.svm.libsvm_sparse.libsvm_sparse_train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;34m\"\"\"base matrix class for compressed row and column oriented matrices\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0m_data_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Sequentially Adding pos.\n",
    "report_pos_sequential_jjr_4 = list()\n",
    "for x in range(0,len(vocab_str)):\n",
    "    #for y in range(x+1, len(vocab)):\n",
    "    if vocab_str[x] not in filtered_jjr_list_4:\n",
    "        vocab_filtered = np.append(filtered_jjr_list_2, vocab_str[x])\n",
    "        print(vocab_filtered)\n",
    "        report = trainModels(pos_training[(tr_file['b_label']!=0)], Ytr, pos_validation[(va_file['b_label']!=0)], Yde, vocab=vocab_filtered, \n",
    "                    verbose=False)\n",
    "        report_pos_sequential_jjr_4.append((vocab_filtered, report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalList =  list()\n",
    "for each in filtered_jjr_list_4:\n",
    "    finalList.append(each)\n",
    "    \n",
    "for each in vocab_str:\n",
    "    if each not in finalList:\n",
    "        finalList.append(each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'update'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-313-ba1c978504b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdummy_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport_pos_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport_pos_final\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogistic_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mxticks\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1722\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_xticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1723\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1724\u001b[1;33m         \u001b[0mlocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1725\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_xticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mset_xticks\u001b[1;34m(self, ticks, minor)\u001b[0m\n\u001b[0;32m   3206\u001b[0m             \u001b[0mDefault\u001b[0m \u001b[1;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3207\u001b[0m         \"\"\"\n\u001b[1;32m-> 3208\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3209\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3210\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mset_ticks\u001b[1;34m(self, ticks, minor)\u001b[0m\n\u001b[0;32m   1676\u001b[0m         \"\"\"\n\u001b[0;32m   1677\u001b[0m         \u001b[1;31m# XXX if the user changes units, the information will be lost here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1678\u001b[1;33m         \u001b[0mticks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1679\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1680\u001b[0m             \u001b[0mxleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_view_interval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mconvert_units\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1524\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1526\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1527\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\category.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(value, unit, axis)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m# force an update so it also does type checking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0munit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         str2idx = np.vectorize(unit._mapping.__getitem__,\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'update'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,10)) \n",
    "ax = plt.axes()\n",
    "x = np.arange(len(dummy_values))\n",
    "ax.set_ylim(0,100)\n",
    "plt.xticks()\n",
    "ax.plot(x, dummy_values)\n",
    "ax.plot(x, logistic_values)\n",
    "#ax.plot(x, svm_values)\n",
    "#ax.plot(x, extra_trees_values)\n",
    "#ax.plot(x, random_forest_values)\n",
    "#ax.plot(x, gradientboosting_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cc']\n",
      "['cc', 'cd']\n",
      "['cc', 'cd', 'dt']\n",
      "['cc', 'cd', 'dt', 'ex']\n",
      "['cc', 'cd', 'dt', 'ex', 'in']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg', 'vbn']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg', 'vbn', 'vbp']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'wpdollar']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'wpdollar', 'vbz']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'wpdollar', 'vbz', 'nnps']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'wpdollar', 'vbz', 'nnps', 'nn']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'wpdollar', 'vbz', 'nnps', 'nn', 'wrb']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'wpdollar', 'vbz', 'nnps', 'nn', 'wrb', 'wdt']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'wpdollar', 'vbz', 'nnps', 'nn', 'wrb', 'wdt', 'to']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'wpdollar', 'vbz', 'nnps', 'nn', 'wrb', 'wdt', 'to', 'rbr']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'wpdollar', 'vbz', 'nnps', 'nn', 'wrb', 'wdt', 'to', 'rbr', 'wp']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'wpdollar', 'vbz', 'nnps', 'nn', 'wrb', 'wdt', 'to', 'rbr', 'wp', 'md']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'wpdollar', 'vbz', 'nnps', 'nn', 'wrb', 'wdt', 'to', 'rbr', 'wp', 'md', 'rbs']\n",
      "['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp', 'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg', 'vbn', 'vbp', 'wpdollar', 'vbz', 'nnps', 'nn', 'wrb', 'wdt', 'to', 'rbr', 'wp', 'md', 'rbs', 'uh']\n"
     ]
    }
   ],
   "source": [
    "report_pos_final = list()\n",
    "for i in range(0,len(finalList)-1):\n",
    "    vocab_filtered = finalList[0:i+1]\n",
    "    print(vocab_filtered)\n",
    "    report = trainModels(pos_training[(tr_file['b_label']!=0)], Ytr, pos_validation[(va_file['b_label']!=0)], Yde, vocab=vocab_filtered, \n",
    "                verbose=False)\n",
    "    report_pos_final.append((vocab_filtered, report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_values = list()\n",
    "logistic_values = list()\n",
    "svm_values = list()\n",
    "extra_trees_values = list()\n",
    "random_forest_values = list()\n",
    "adaboost_classifier_values = list()\n",
    "gradientboosting_values = list()\n",
    "for x in report_pos_final:\n",
    "    dummy_values.append(x[1][0][1]*100)\n",
    "    logistic_values.append(x[1][1][1]*100)\n",
    "    svm_values.append(x[1][2][1]*100)\n",
    "    extra_trees_values.append(x[1][3][1]*100)\n",
    "    random_forest_values.append(x[1][4][1]*100)\n",
    "    adaboost_classifier_values.append(x[1][5][1]*100)\n",
    "    gradientboosting_values.append(x[1][6][1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946,\n",
       " 59.45945945945946]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cc',\n",
       " 'cd',\n",
       " 'dt',\n",
       " 'ex',\n",
       " 'in',\n",
       " 'jj',\n",
       " 'jjr',\n",
       " 'jjs',\n",
       " 'ls',\n",
       " 'nnp',\n",
       " 'nns',\n",
       " 'pdt',\n",
       " 'prp',\n",
       " 'prpdollar',\n",
       " 'rb',\n",
       " 'rp',\n",
       " 'vb',\n",
       " 'vbd',\n",
       " 'vbg',\n",
       " 'vbn',\n",
       " 'vbp',\n",
       " 'wpdollar',\n",
       " 'vbz',\n",
       " 'nnps',\n",
       " 'nn',\n",
       " 'wrb',\n",
       " 'wdt',\n",
       " 'to',\n",
       " 'rbr',\n",
       " 'wp',\n",
       " 'md',\n",
       " 'rbs',\n",
       " 'uh']"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_pos_final[len(report_pos_final)-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
