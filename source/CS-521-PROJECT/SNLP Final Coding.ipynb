{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all code together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utility libraries\n",
    "import util\n",
    "import preprocessing\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_task = nlp_util.NLP_Task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "tr_file, va_file, te_file = util.load_files()\n",
    "tr_dict = util.tsv_to_dict(tsv_file=tr_file)\n",
    "va_dict = util.tsv_to_dict(tsv_file=va_file)\n",
    "te_dict = util.tsv_to_dict(tsv_file=te_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_file['preprocessed'] = preprocessing.preprocessing_txt(dataset=tr_file)\n",
    "va_file['preprocessed'] = preprocessing.preprocessing_txt(dataset=va_file)\n",
    "te_file['preprocessed'] = preprocessing.preprocessing_txt(dataset=te_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training = pd.DataFrame()\n",
    "features_validation = pd.DataFrame()\n",
    "features_testing = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training['text'] = tr_file['preprocessed']\n",
    "features_validation['text'] = va_file['preprocessed']\n",
    "features_testing['text'] = te_file['preprocessed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotting_util\n",
    "importlib.reload(plotting_util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_label = [ len(tr_file['Label'].values[tr_file['Label'] == 'pants-fire']),\n",
    "    len(tr_file['Label'].values[tr_file['Label'] == 'false']),\n",
    "                 len(tr_file['Label'].values[tr_file['Label'] == 'barely-true']),\n",
    "                 len(tr_file['Label'].values[tr_file['Label'] == 'half-true']),\n",
    "                 len(tr_file['Label'].values[tr_file['Label'] == 'mostly-true']),\n",
    "                len(tr_file['Label'].values[tr_file['Label'] == 'true'])\n",
    "                ]\n",
    "data_labels = ['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true','true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar_plot = plotting_util.plot_bar_chart(chartname='Liar liar dataset distribution', barnames=data_labels, barvalues=data_by_label,\n",
    "#                             barcolors=['skyblue'])\n",
    "#plt.plot()\n",
    "\n",
    "#pie_plot = plotting_util.plot_pie_chart(chartname='myplot', labels=data_labels, values=data_by_label)\n",
    "sizes = data_by_label\n",
    "explode = (0.1, 0.1, 0.1, 0.1, 0.1, 0)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=data_labels, autopct='%1.1f%%', explode= explode,\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart\n",
    "labels = ['Pants-fire', 'False', 'Barely-true', 'Half-true', 'Mostly-true','True']\n",
    "sizes = data_by_label\n",
    "#colors\n",
    "colors = ['#ff4d4d','#ff794d','#ffa64d','#d2ff4d','#a6ff4d','#4dff4d']\n",
    "#explsion\n",
    "explode = (0.05,0.05,0.05,0.05,0.05,0.05)\n",
    "\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(6,6)) \n",
    "ax1.pie(sizes, colors = colors, labels=labels, autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode, textprops={'fontsize': 17})\n",
    "#draw circle\n",
    "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle\n",
    "ax1.axis('equal') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting POS tags grouped by unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS extracted from no preprocessed data for training, validation and testing files\n",
    "unigram_pos, bigrams_pos, trigram_pos = preprocessing.extract_POS(statements=tr_dict['statement'])\n",
    "unigram_pos_va, bigrams_pos_va, trigram_pos_va = preprocessing.extract_POS(statements=va_dict['statement'])\n",
    "unigram_pos_te, bigrams_pos_te, trigram_pos_te = preprocessing.extract_POS(statements=te_dict['statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pos extracted from preprocessed data for training, validation and testing files\n",
    "unigram_pos_p, bigrams_pos_p, trigram_pos_p = preprocessing.extract_POS(tr_file['preprocessed'].values)\n",
    "unigram_pos_p_va, bigrams_pos_p_va, trigram_pos_p_va = preprocessing.extract_POS(va_file['preprocessed'].values)\n",
    "unigram_pos_p_te, bigrams_pos_p_te, trigram_pos_p_te = preprocessing.extract_POS(te_file['preprocessed'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_remove = ['NNP','CD']\n",
    "# Training\n",
    "removed_pos = nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_p)\n",
    "removed_pos_va =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_p_va)\n",
    "removed_pos_te =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_p_te)\n",
    "\n",
    "removed_pos_bigrams = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos)\n",
    "removed_pos_bigrams_va = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_va)\n",
    "removed_pos_bigrams_te = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_te)\n",
    "\n",
    "removed_pos_trigrams = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos)\n",
    "removed_pos_trigrams_va = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_va)\n",
    "removed_pos_trigrams_te = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_remove = ['NNP','CD']\n",
    "features_training['pos'] = [\" \".join(x).replace('<s>','').replace('$','dollar').strip() for x in removed_pos]\n",
    "features_validation['pos'] = [\" \".join(x).replace('<s>','').replace('$','dollar').strip() for x in removed_pos_va]\n",
    "features_testing['pos'] = [\" \".join(x).replace('<s>','').replace('$','dollar').strip() for x in removed_pos_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_remove = ['NNP','CD']\n",
    "features_training['pos_bi'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_bigrams]\n",
    "features_validation['pos_bi'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_bigrams_va]\n",
    "features_testing['pos_bi'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_bigrams_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_remove = ['NNP','CD']\n",
    "features_training['pos_tri'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_trigrams]\n",
    "features_validation['pos_tri'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_trigrams_va]\n",
    "features_testing['pos_tri'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_trigrams_te]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unique values for unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique list of unigrams, bigrams and trigrams for no preprocessed data\n",
    "unigram_list_tr = nlp_task.UniquePosTags(unigram_pos)\n",
    "bigram_list_tr = nlp_task.UniquePosTags(bigrams_pos)\n",
    "trigram_list_tr = nlp_task.UniquePosTags(trigram_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique list of unigrams, bigrams and trigrams for preprocessed data\n",
    "unigram_list_tr_processed = nlp_task.UniquePosTags(unigram_pos_p)\n",
    "bigram_list_tr_processed = nlp_task.UniquePosTags(bigrams_pos_p)\n",
    "trigram_list_tr_processed = nlp_task.UniquePosTags(trigram_pos_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicated POS in bigrams and trigrams [NNP and CD]\n",
    "\n",
    "For example: The/DT economy/NN bled/VBD $/$ 24/CD billion/CD due/JJ to/TO the/DT government/NN shutdown/NN ./. <br>\n",
    "In this case having CD_CD is the same as having only CD<br>\n",
    "Same with: <br>\n",
    "U.S./NNP Rep./NNP Ron/NNP Kind/NNP ,/, D-Wis./NNP ,/, and/CC his/PRP$ fellow/JJ Democrats/NNS went/VBD on/IN a/DT spending/NN spree/NN and/CC now/RB their/PRP$ credit/NN card/NN is/VBZ maxed/VBN out/RP <br>\n",
    "We don't need all those NNPs to find a pattern and it might be noisy to the ML algorithm<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For raw data\n",
    "\n",
    "list_to_remove = ['NNP','CD']\n",
    "# Training\n",
    "removed_pos = nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos)\n",
    "removed_pos_bigrams = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos)\n",
    "removed_pos_trigrams = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos)\n",
    "\n",
    "# Validdation\n",
    "removed_pos_va =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_va)\n",
    "removed_pos_bigrams_va = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_va)\n",
    "removed_pos_trigrams_va = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_va)\n",
    "\n",
    "# Testing\n",
    "removed_pos_te =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_te)\n",
    "removed_pos_bigrams_te = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_te)\n",
    "removed_pos_trigrams_te = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_te)\n",
    "\n",
    "#LIST OF UNIQUE BIGRAMS AND TRIGRAMS AFTER REMOVING CONSECUTIVE SAME TAGS\n",
    "removed_unique_bigrams = nlp_task.UniquePosTags(postags=removed_pos_bigrams)\n",
    "removed_unique_trigrams = nlp_task.UniquePosTags(postags=removed_pos_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For processed data\n",
    "list_to_remove = ['NNP','CD']\n",
    "# Training\n",
    "removed_pos_p = nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_p)\n",
    "removed_pos_bigrams_p = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_p)\n",
    "removed_pos_trigrams_p = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_p)\n",
    "\n",
    "# Validdation\n",
    "removed_pos_va_p =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_p_va)\n",
    "removed_pos_bigrams_va_p = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_va_p)\n",
    "removed_pos_trigrams_va_p = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_va_p)\n",
    "\n",
    "# Testing\n",
    "removed_pos_te_p =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_p_te)\n",
    "removed_pos_bigrams_te_p = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_te_p)\n",
    "removed_pos_trigrams_te_p = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_te_p)\n",
    "\n",
    "#LIST OF UNIQUE BIGRAMS AND TRIGRAMS AFTER REMOVING CONSECUTIVE SAME TAGS\n",
    "removed_unique_bigrams_p = nlp_task.UniquePosTags(postags=removed_pos_bigrams_p)\n",
    "removed_unique_trigrams_p = nlp_task.UniquePosTags(postags=removed_pos_trigrams_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unigram_list_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add numerical labels for each of the sentence in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for multiclassification and binary classification tasks\n",
    "multi_labels = {'false':0, 'true':1,'pants-fire':2,'barely-true':3,'half-true':4,'mostly-true':5}\n",
    "binary_labels = {'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}\n",
    "\n",
    "\n",
    "tr_file['b_label'] = np.array(preprocessing.create_labels(labels=tr_file['Label'].values,label_values=binary_labels))\n",
    "va_file['b_label'] = np.array(preprocessing.create_labels(labels=va_file['Label'].values,label_values=binary_labels))\n",
    "te_file['b_label'] = np.array(preprocessing.create_labels(labels=te_file['Label'].values,label_values=binary_labels))\n",
    "\n",
    "tr_file['m_label'] = np.array(preprocessing.create_labels(labels=tr_file['Label'].values,label_values=multi_labels))\n",
    "va_file['m_label'] = np.array(preprocessing.create_labels(labels=va_file['Label'].values,label_values=multi_labels))\n",
    "te_file['m_label'] = np.array(preprocessing.create_labels(labels=te_file['Label'].values,label_values=multi_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training['b_label']= np.array(preprocessing.create_labels(labels=tr_file['Label'].values,label_values=binary_labels))\n",
    "features_validation['b_label']= np.array(preprocessing.create_labels(labels=va_file['Label'].values,label_values=binary_labels))\n",
    "features_testing['b_label']= np.array(preprocessing.create_labels(labels=te_file['Label'].values,label_values=binary_labels))\n",
    "\n",
    "features_training['m_label']= np.array(preprocessing.create_labels(labels=tr_file['Label'].values,label_values=multi_labels))\n",
    "features_validation['m_label']= np.array(preprocessing.create_labels(labels=va_file['Label'].values,label_values=multi_labels))\n",
    "features_testing['m_label']= np.array(preprocessing.create_labels(labels=te_file['Label'].values,label_values=multi_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecesary columns from dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once this code is executed and you try to re run it again it is going to show an error because columns already were remoded\n",
    "unnecesary_columns = ['BT', 'FC', 'HT', 'MT', 'PF']\n",
    "tr_file = tr_file.drop(unnecesary_columns, axis=1)\n",
    "va_file = va_file.drop(unnecesary_columns, axis=1)\n",
    "te_file = te_file.drop(unnecesary_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract POS features from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having ran some experiments with decision trees regarding POS we came up with a list of POS unigrams, bigrams and trigrams that were more relevant for classifying mostly ture and mostly false news.\n",
    "<br>\n",
    "POS unigrams: ['VBZ', 'DT', 'NNPS', 'VBP', 'JJ', 'IN', 'WRB', 'VBD', 'PRP', 'RP', 'WDT', 'VB', 'NNP', 'VBG', 'PRP$', 'VBN', 'CD', 'RB', 'WP', 'JJS', 'JJR', 'EX', 'RBS', 'FW', 'LS']\n",
    " <br>\n",
    "POS brigrams: ['NNPS_VBP', 'VB_NNP', 'IN_DT', 'VB_JJ', 'JJ_CD', 'CD_NNS', 'DT_JJS', 'JJR_IN', 'IN_CD', 'CC_IN', 'RB_VBD', 'CD_NN', 'NN_TO', 'JJR_JJ', 'VB_CD'] <br>\n",
    "POS trigrams: ['VBD_VBN_IN', 'IN_DT_JJ', 'CD_NN_IN', 'IN_CD_NNS', 'IN_DT_NN', 'DT_JJ_CD', 'MD_VB_IN', 'JJS_JJ_NN', 'CC_JJ_NNS', 'JJ_NNS_VBP', 'VBP_CD_NN', 'NNS_,_CD', 'sos_JJR_IN', 'IN_DT_NNS','JJ_NN_MD'] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_relevant_unigrams =  ['VBZ', 'DT', 'NNPS', 'VBP', 'JJ', 'IN', 'WRB', 'VBD', 'PRP', 'RP', 'WDT', 'VB', 'NNP', 'VBG', 'PRP$', 'VBN', 'CD', 'RB', 'WP', 'JJS', 'JJR', 'EX', 'RBS', 'FW', 'LS'] \n",
    "pos_relevant_bigrams = ['NNPS_VBP', 'VB_NNP', 'IN_DT', 'VB_JJ', 'JJ_CD', 'CD_NNS', 'DT_JJS', 'JJR_IN', 'IN_CD', 'CC_IN', 'RB_VBD', 'CD_NN', 'NN_TO', 'JJR_JJ', 'VB_CD'] \n",
    "pos_relevant_trigrams = ['VBD_VBN_IN', 'IN_DT_JJ', 'CD_NN_IN', 'IN_CD_NNS', 'IN_DT_NN', 'DT_JJ_CD', 'MD_VB_IN', 'JJS_JJ_NN', 'CC_JJ_NNS', 'JJ_NNS_VBP', 'VBP_CD_NN', 'sos_JJR_IN', 'IN_DT_NNS','JJ_NN_MD'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_onehot_unigram, Xtr_count_unigram, Xtr_tfidf_unigram, Xval_onehot_unigram, Xval_count_unigram, Xval_tfidf_unigram, Xte_onehot_unigram, Xte_count_unigram, Xte_tfidf_unigram = util.GetFeaturesFromPOS(training_data=unigram_pos_p, validation_data=unigram_pos_p_va, testing_data=unigram_pos_p_te, user_defined_vocabulary=pos_relevant_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Xtr_onehot_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_onehot_bigrams, Xtr_count_bigrams, Xtr_tfidf_bigrams, Xval_onehot_bigrams, Xval_count_bigrams, Xval_tfidf_bigrams, Xte_onehot_bigrams, Xte_count_bigrams, Xte_tfidf_bigrams = util.GetFeaturesFromPOS(training_data=removed_pos_bigrams_p, validation_data=removed_pos_bigrams_va_p, testing_data=removed_pos_bigrams_te_p, user_defined_vocabulary=pos_relevant_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Xtr_onehot_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_onehot_trigram, Xtr_count_trigram, Xtr_tfidf_trigram, Xval_onehot_trigram, Xval_count_trigram, Xval_tfidf_trigram, Xte_onehot_trigram, Xte_count_trigram, Xte_tfidf_trigram = util.GetFeaturesFromPOS(training_data=trigram_pos, validation_data=trigram_pos_va, testing_data=trigram_pos_te, user_defined_vocabulary=pos_relevant_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Xtr_onehot_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(Xtr_onehot_trigram[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving vectors representing bigrams\n",
    "tr_file['pos_unigrams_1hot'] =  [str(x) for x in Xtr_onehot_unigram]\n",
    "tr_file['pos_bigrams_1hot'] = [str(x) for x in Xtr_onehot_bigrams]\n",
    "tr_file['pos_trigrams_1hot'] = [str(x) for x in Xtr_onehot_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "va_file['pos_unigrams_1hot'] =  [str(x) for x in Xval_onehot_unigram]\n",
    "va_file['pos_bigrams_1hot'] = [str(x) for x in Xval_onehot_bigrams]\n",
    "va_file['pos_trigrams_1hot'] = [str(x) for x in Xval_onehot_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "te_file['pos_unigrams_1hot'] =  [str(x) for x in Xte_onehot_unigram]\n",
    "te_file['pos_bigrams_1hot'] = [str(x) for x in Xte_onehot_bigrams]\n",
    "te_file['pos_trigrams_1hot'] = [str(x) for x in Xte_onehot_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "tr_file['pos_unigrams_count'] =  [str(x) for x in Xtr_count_unigram]\n",
    "tr_file['pos_bigrams_count'] = [str(x) for x in Xtr_count_bigrams]\n",
    "tr_file['pos_trigrams_count'] = [str(x) for x in Xtr_count_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "va_file['pos_unigrams_count'] =  [str(x) for x in Xval_count_unigram]\n",
    "va_file['pos_bigrams_count'] = [str(x) for x in Xval_count_bigrams]\n",
    "va_file['pos_trigrams_count'] = [str(x) for x in Xval_count_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "te_file['pos_unigrams_count'] =  [str(x) for x in Xte_count_unigram]\n",
    "te_file['pos_bigrams_count'] = [str(x) for x in Xte_count_bigrams]\n",
    "te_file['pos_trigrams_count'] = [str(x) for x in Xte_count_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "tr_file['pos_unigrams_tfidf'] =  [str(x) for x in Xtr_tfidf_unigram]\n",
    "tr_file['pos_bigrams_tfidf'] = [str(x) for x in Xtr_tfidf_bigrams]\n",
    "tr_file['pos_trigrams_tfidf'] = [str(x) for x in Xtr_tfidf_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "va_file['pos_unigrams_tfidf'] =  [str(x) for x in Xval_tfidf_unigram]\n",
    "va_file['pos_bigrams_tfidf'] = [str(x) for x in Xval_tfidf_bigrams]\n",
    "va_file['pos_trigrams_tfidf'] = [str(x) for x in Xval_tfidf_trigram]\n",
    "\n",
    "#saving vectors representing bigrams\n",
    "te_file['pos_unigrams_tfidf'] =  [str(x) for x in Xte_tfidf_unigram]\n",
    "te_file['pos_bigrams_tfidf'] = [str(x) for x in Xte_tfidf_bigrams]\n",
    "te_file['pos_trigrams_tfidf'] = [str(x) for x in Xte_tfidf_trigram]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training['key_words'] = preprocessing.get_keywords(tr_file)\n",
    "features_validation['key_words'] = preprocessing.get_keywords(va_file)\n",
    "features_testing['key_words'] = preprocessing.get_keywords(te_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing.bigphrase_tfidf_feats(tr_file[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIWC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LIWC analysis files\n",
    "liwc_tr = pd.read_csv('..\\\\dataset\\\\{0}'.format('train_liwc.csv'))\n",
    "liwc_va = pd.read_csv('..\\\\dataset\\\\{0}'.format('valid_liwc.csv'))\n",
    "liwc_te = pd.read_csv('..\\\\dataset\\\\{0}'.format('test_liwc.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features so there are equally treated in terms of measure units\n",
    "scaler = MinMaxScaler()\n",
    "liwc_features_tr = scaler.fit_transform(liwc_tr.iloc[1:,3:])\n",
    "liwc_features_va = scaler.transform(liwc_va.iloc[:,14:])\n",
    "liwc_features_te = scaler.transform(liwc_te.iloc[:,14:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_feature_names = liwc_tr.columns[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, cname in enumerate(liwc_feature_names):\n",
    "    features_training[cname] = liwc_features_tr.T[index]\n",
    "    features_validation[cname] = liwc_features_va.T[index]\n",
    "    features_testing[cname] = liwc_features_te.T[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MI compute the information between the variables and the target\n",
    "feature_selected = mutual_info_classif(liwc_features_tr[(tr_file['b_label']!=0)],tr_file['b_label'][(tr_file['b_label']!=0)] , random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= liwc_tr.columns[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the more relevant LIWC features\n",
    "relevant_liwc_features = list()\n",
    "for index, f in enumerate(features):\n",
    "    if feature_selected[index]>0.005:\n",
    "        relevant_liwc_features.append(f)\n",
    "        print(index, f, feature_selected[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LIWC features to the dataframe\n",
    "for index,col in enumerate(liwc_tr.columns[3:]):\n",
    "    if col in relevant_liwc_features:\n",
    "        tr_file[col] = liwc_features_tr.T[index]\n",
    "        \n",
    "for index,col in enumerate(liwc_va.columns[14:]):\n",
    "    if col in relevant_liwc_features:\n",
    "        va_file[col] = liwc_features_va.T[index]\n",
    "        \n",
    "for index,col in enumerate(liwc_te.columns[14:]):\n",
    "    if col in relevant_liwc_features:\n",
    "        te_file[col] = liwc_features_te.T[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Blob Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Blobber\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.np_extractors import ConllExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTextBlobFeatures(corpus):\n",
    "    extractor = ConllExtractor()\n",
    "    text_blob_features = np.zeros((len(corpus),4))\n",
    "    blob_sentiment_analyzer = Blobber(analyzer=NaiveBayesAnalyzer())\n",
    "    for i,each_text in enumerate(corpus):\n",
    "        #print('analyzing: ',i)\n",
    "        #blob_sentiment_analyzer = TextBlob(each_text, analyzer=NaiveBayesAnalyzer())\n",
    "        text_blob_features[i,0]=blob_sentiment_analyzer(each_text).sentiment[1]\n",
    "        text_blob_features[i,1]=blob_sentiment_analyzer(each_text).sentiment[2]\n",
    "        #text_blob_features[i,2]= TextBlob(each_text).polarity\n",
    "        text_blob_features[i,2]= TextBlob(each_text).subjectivity\n",
    "        noun_phrase_extractor = TextBlob(each_text, np_extractor=extractor)\n",
    "        text_blob_features[i,3]= len(noun_phrase_extractor.noun_phrases)\n",
    "    return text_blob_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_tb_features = extractTextBlobFeatures(tr_file['preprocessed'])\n",
    "va_tb_features = extractTextBlobFeatures(va_file['preprocessed'])\n",
    "te_tb_features = extractTextBlobFeatures(te_file['preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_feature_names = ['sentiment_pos', 'sentiment_neg','subjectivity','noun_phrases_count']\n",
    "for i in range(0,4):\n",
    "    tr_file[tb_feature_names[i]] = tr_tb_features.T[i]\n",
    "    va_file[tb_feature_names[i]] = va_tb_features.T[i]\n",
    "    te_file[tb_feature_names[i]] = te_tb_features.T[i]\n",
    "\n",
    "    \n",
    "phrases_scaler = MinMaxScaler()\n",
    "tr_file['noun_phrases_count'] = phrases_scaler.fit_transform(tr_file['noun_phrases_count'].values.reshape(-1, 1))\n",
    "va_file['noun_phrases_count']= phrases_scaler.transform(va_file['noun_phrases_count'].values.reshape(-1, 1))\n",
    "te_file['noun_phrases_count'] = phrases_scaler.transform(te_file['noun_phrases_count'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_feature_names = ['sentiment_pos', 'sentiment_neg','subjectivity','noun_phrases_count']\n",
    "for i in range(0,4):\n",
    "    features_training[tb_feature_names[i]]  = tr_tb_features.T[i]\n",
    "    features_validation[tb_feature_names[i]] = va_tb_features.T[i]\n",
    "    features_testing[tb_feature_names[i]] = te_tb_features.T[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_scaler = MinMaxScaler()\n",
    "features_training['noun_phrases_count'] = phrases_scaler.fit_transform(tr_file['noun_phrases_count'].values.reshape(-1, 1))\n",
    "features_validation['noun_phrases_count']= phrases_scaler.transform(va_file['noun_phrases_count'].values.reshape(-1, 1))\n",
    "features_testing['noun_phrases_count'] = phrases_scaler.transform(te_file['noun_phrases_count'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting context based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check trainig statements\n",
    "tr_file['all_cap'] = [1 if len(x)>0 else 0 for x in [re.findall('([A-Z]+\\s)', x) for x in tr_file['Statement'].values]]\n",
    "tr_file['quotation'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[\"|\\']([^\"]*)[\"|\\']', x) for x in tr_file['Statement'].values]]\n",
    "tr_file['parenthesis'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[(]([^\"]*)[)]', x) for x in tr_file['Statement'].values]]\n",
    "\n",
    "# Check validation statements\n",
    "va_file['all_cap'] = [1 if len(x)>0 else 0 for x in [re.findall('([A-Z]+\\s)', x) for x in va_file['Statement'].values]]\n",
    "va_file['quotation'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[\"|\\']([^\"]*)[\"|\\']', x) for x in va_file['Statement'].values]]\n",
    "va_file['parenthesis'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[(]([^\"]*)[)]', x) for x in va_file['Statement'].values]]\n",
    "\n",
    "# Check testing statements\n",
    "te_file['all_cap'] = [1 if len(x)>0 else 0 for x in [re.findall('([A-Z]+\\s)', x) for x in te_file['Statement'].values]]\n",
    "te_file['quotation'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[\"|\\']([^\"]*)[\"|\\']', x) for x in te_file['Statement'].values]]\n",
    "te_file['parenthesis'] = [1 if len(x)>0 else 0 for x in [re.findall(r'[(]([^\"]*)[)]', x) for x in te_file['Statement'].values]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training['all_cap'] = tr_file['all_cap']\n",
    "features_validation['all_cap']= va_file['all_cap'] \n",
    "features_testing['all_cap'] =te_file['all_cap'] \n",
    "\n",
    "features_training['quotation'] = tr_file['quotation']\n",
    "features_validation['quotation']= va_file['quotation'] \n",
    "features_testing['quotation'] =te_file['quotation'] \n",
    "\n",
    "features_training['parenthesis'] = tr_file['parenthesis']\n",
    "features_validation['parenthesis']= va_file['parenthesis'] \n",
    "features_testing['parenthesis'] =te_file['parenthesis'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run machine learning models on feature extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords # nltk stop words\n",
    "from nltk.tokenize import word_tokenize # nltk word tokenizer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords # nltk stop words\n",
    "from nltk.tokenize import word_tokenize # nltk word tokenizer\n",
    "import string\n",
    "import re\n",
    "## Function to clean text data\n",
    "def lemmatize_remove_stop_words(corpus):\n",
    "    print('Processing data...')\n",
    "    result = list()\n",
    "    # define a lemmatizer\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # tokenize and remove stopwords from every sentence\n",
    "    stop_words = stopwords.words('english')\n",
    "    # iterate over every sentence and apply the three filters to remove stopwords, extract lemmas for verbs and nouns\n",
    "    for index, sentence in enumerate(corpus):\n",
    "        #print('Cleaning sentence number:', index, end=\"##\")\n",
    "        #print('Making text all lower case...',end=\"--\")\n",
    "        clean_text = sentence.lower() #  make all words lower case\n",
    "        #print('Removing punctuation...', end=\"--\")\n",
    "        clean_text = re.sub(r'[{0}]'.format(string.punctuation),'',clean_text)# remove punctuation\n",
    "        #print('Removing numberic data and symbols...', end=\"--\")\n",
    "        clean_text = re.sub(r'\\w*\\d\\w*','',clean_text) # remove alpha numerics\n",
    "        #print('Splitting into tokens...', end=\"--\")\n",
    "        tokenized_sentence = word_tokenize(clean_text) # split sentence into tokens\n",
    "        #print('Lemmatizing verbs...',end=\"--\")\n",
    "        filter_one = [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in tokenized_sentence] # lemmatize verbs\n",
    "        #print('Lemmatizing nouns...',end=\"--\")\n",
    "        filter_two = [wordnet_lemmatizer.lemmatize(word, pos=\"n\") for word in filter_one] # lemmatize nouns\n",
    "        #print('Removing stop words...',end=\"--\")\n",
    "        filter_three = [w for w in filter_two if w not in stop_words] # remove stop words\n",
    "        #print('Removing extra white spaces',end=\"--\")\n",
    "        all_clean = (' '.join(filter_three)).strip() # remove extra white spaces\n",
    "        #print('All clean for sentence: ', index,end=\"\\r\")\n",
    "        result.append(all_clean) # append data to result\n",
    "    print('Finished!')\n",
    "    return np.array(result) # convert to numpy array and return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import different models that we want to train\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RANDOM_STATE_V = 45\n",
    "models = [DummyClassifier,\n",
    "          DecisionTreeClassifier, \n",
    "          Perceptron, \n",
    "          LogisticRegression, \n",
    "          MultinomialNB, \n",
    "          BernoulliNB, \n",
    "          SGDClassifier, \n",
    "          SVC\n",
    "         ]#, \n",
    "          #SVC]\n",
    "defaults = [{'strategy':'most_frequent', 'random_state': _RANDOM_STATE_V}, #Baseline\n",
    "    {'max_depth': 3, 'criterion':'entropy','random_state': _RANDOM_STATE_V}, #DT\n",
    "            {'penalty':'l2','early_stopping': True,'random_state': _RANDOM_STATE_V}, #Perceptron\n",
    "            {'penalty':'l2','tol':0.0001, 'C':1.0,'max_iter':100,'random_state': _RANDOM_STATE_V}, #Linear Regression\n",
    "            {}, #MultinomialNB\n",
    "            {}, #BernoulliNB\n",
    "            {'loss':'hinge', 'penalty':'l2', 'alpha':0.0001,'random_state': _RANDOM_STATE_V}, #SGDClassifier,\n",
    "            {'C':1.0, 'kernel':'linear', 'degree':3, 'gamma':1,'random_state': _RANDOM_STATE_V}\n",
    "            #{'n_neighbors':5},\n",
    "           #{}\n",
    "           ]#, #KNeighborsClassifier\n",
    "            #{'C':1.0, 'kernel':'rbf', 'degree':3, 'gamma':1,'random_state': _RANDOM_STATE_V}, #SVC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making string of the data\n",
    "training_str = [\" \".join(x) for x in removed_pos_bigrams_p]\n",
    "validation_str = [\" \".join(x) for x in removed_pos_bigrams_va]\n",
    "\n",
    "#replace $ by dollar\n",
    "training_str = [x.replace('$', 'dollar').replace('<s>','') for x in training_str]\n",
    "validation_str = [x.replace('$', 'dollar').replace('<s>','') for x in validation_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Xtr = lemmatize_remove_stop_words(tr_file['preprocessed'][(tr_file['b_label']!=0)])\n",
    "#Xde = lemmatize_remove_stop_words(va_file['preprocessed'][(va_file['b_label']!=0)])\n",
    "Xtr = tr_file['preprocessed'][(tr_file['b_label']!=0)]\n",
    "Xde = va_file['preprocessed'][(va_file['b_label']!=0)]\n",
    "#Xtr = tr_file['preprocessed'][(tr_file['b_label']!=0)]\n",
    "#Xde = va_file['preprocessed'][(va_file['b_label']!=0)]\n",
    "#Ytr = tr_file['b_label'][(tr_file['b_label']!=0)]\n",
    "#Yde = va_file['b_label'][(va_file['b_label']!=0)]\n",
    "#Xtr= Xtr_onehot_unigram[(tr_file['b_label']!=0)]\n",
    "#Xde= Xval_onehot_unigram[(va_file['b_label']!=0)]\n",
    "#Xtr = Xtr_onehot_bigrams#tr_tb_features#liwc_features_tr#tr_file['preprocessed']+' '+training_str\n",
    "#Xde = Xval_onehot_bigrams#va_tb_features#liwc_features_va#va_file['preprocessed']+' '+validation_str\n",
    "Ytr = tr_file['b_label'][(tr_file['b_label']!=0)][(tr_file['b_label']!=0)]\n",
    "Yde = va_file['b_label'][(va_file['b_label']!=0)]\n",
    "\n",
    "report_list = list()\n",
    "for index, model in enumerate(models):\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', model(**defaults[index])),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    #text_clf = model(**defaults[index])\n",
    "    print('Training {0}'.format(model.__name__))\n",
    "    text_clf.fit(Xtr, Ytr)\n",
    "    pred = text_clf.predict(Xde)\n",
    "    #print('Accuracy: ', np.mean(pred==Yde))\n",
    "    print('Accuracy: ', accuracy_score(pred,Yde))\n",
    "    print(classification_report(pred, Yde))\n",
    "    #report_list.append((model.__name__, np.mean(pred==Yde), f1_score(pred, Yde, average='binary', pos_label=1),f1_score(pred, Yde, average='binary', pos_label=-1)))\n",
    "    report_list.append((model.__name__, accuracy_score(pred,Yde)))\n",
    "for e in report_list:\n",
    "    #print('{0},{1},{2}'.format(e[1], e[2], e[3]))\n",
    "    print('{0},{1}'.format(e[0], e[1]))                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, \n",
    "#random_state=None, solver=’warn’, max_iter=1000, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None\n",
    "\n",
    "parameters = {\n",
    "    #'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__max_iter': (10,20,30,50,100),\n",
    "    #'clf__alpha': (1e-5,1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1),\n",
    "    #'clf__penalty': 'l2',\n",
    "    'clf__penalty': ('l2','l1'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "    #alpha=1e-5,\n",
    "    'clf__C' :(0.01,0.05,0.1,0.2,0.3,0.4,0.5,1,5,8,12) ,\n",
    "    #'clf__class_weight' : ({1:1},{1:2},{1:3},{1:4},{1:5}\n",
    "    #'clf__alpha':(0.2,0.5,1,2,3,10,15,25,50)\n",
    "    \n",
    "    #'clf__loss': 'hinge' #, 'log', 'modified_huber', 'squared_hinge', 'perceptron'\n",
    "}\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(penalty='l2', random_state=_RANDOM_STATE_V, max_iter=1000)),\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(text_clf, parameters, cv=5,n_jobs=1, verbose=1, scoring='accuracy')\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in text_clf.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "#validation_set = PredefinedSplit(test_fold=validation_text)\n",
    "grid_search.fit(Xtr, Ytr)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "#print('Training {0}'.format(SGDClassifier.__name__))\n",
    "#text_clf.fit(X_train_oversamples, Y_train_resampled)\n",
    "#pred = text_clf.predict(validation_text)\n",
    "#print(classification_report(pred, validation_target))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(penalty='l2', random_state=_RANDOM_STATE_V, max_iter=1000, C=0.5)),\n",
    "])\n",
    "\n",
    "text_clf.fit(Xtr,Ytr)\n",
    "pred = text_clf.predict(Xde)\n",
    "print('Accuracy: ', accuracy_score(pred, Yde))\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adj Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import spacy\n",
    "import en_core_web_sm\n",
    "nlp=en_core_web_sm.load()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def get_noun_adj_pairs(corpus):\n",
    "    result = list()\n",
    "    for c in corpus:\n",
    "        parsed=nlp(c)\n",
    "        noun_adj_pairs_res=[]\n",
    "        for i, tok in enumerate(parsed):\n",
    "            if tok.pos_ not in ('NOUN','PRON'):\n",
    "                continue\n",
    "            for j in range(i+1, len(parsed)):\n",
    "                if parsed[j].pos_=='ADJ':\n",
    "                    noun_adj_pairs_res.append((tok, parsed[j]))\n",
    "                    break\n",
    "        result.append(noun_adj_pairs_res)\n",
    "    return result\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Xtr[5]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"get_noun_adj_pairs(tr_file['preprocessed'])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    embeddings_index = {}\n",
    "    f = open(gloveFile, encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-300])\n",
    "        coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glove_model = loadGloveModel('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build glove-vec embedding layer\n",
    "def build_glove_embedding_layers():\n",
    "    embed_matrix=np.zeros((max_features, embedding_dims))\n",
    "    for word, indx in t.word_index.items():\n",
    "        if indx >= max_features:\n",
    "            continue\n",
    "        if word in glove_model:\n",
    "            embed_vec=glove_model[word]\n",
    "            if embed_vec is not None:\n",
    "                embed_matrix[indx]=embed_vec\n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "embedding_weights=build_glove_embedding_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(num_words=5000) # create tokenizer with a max number of words to take into account according to frequency\n",
    "t.fit_on_texts(tr_file['Statement']) # fit tokenizer with train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences # To make vectors the same size. \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPool1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 300\n",
    "x_train = t.texts_to_sequences(tr_file['Statement'])\n",
    "x_dev = t.texts_to_sequences(va_file['Statement'])\n",
    "x_train = pad_sequences(x_train,maxlen=maxlen,padding='post')\n",
    "x_dev = pad_sequences(x_dev, maxlen=maxlen, padding='post')\n",
    "x_test = t.texts_to_sequences(te_file['Statement'])\n",
    "x_test =  pad_sequences(x_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(tr_file['m_label'], num_classes=6)\n",
    "y_dev = to_categorical(va_file['m_label'], num_classes=6)\n",
    "y_test =  to_categorical(te_file['m_label'], num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = len(t.word_index)+1\n",
    "filters = 128\n",
    "kernel_size = 2\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "embedding_dims = 300\n",
    "model = Sequential()\n",
    "maxlen = 300\n",
    "hidden_dims = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#Simple one hidden layer NN with a Convolutional layer for filtering and GlobalMaxPooling 1D \n",
    "model.add(Embedding(input_dim=max_features, \n",
    "                           output_dim=embedding_dims, \n",
    "                        input_length=maxlen,\n",
    "                         weights=[embedding_weights],\n",
    "                         trainable=False,\n",
    "                   ))\n",
    "model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPool1D(5))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool1D(5))\n",
    "model.add(Conv1D(filters=128, kernel_size=4, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.8))\n",
    "#model.add(Dense(hidden_dims, activation='relu'))\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_dev,y_dev),\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_dev, y_dev, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation ==> Loss: {0}, Accuracy: {1}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test ==Loss: {0}, Accuracy: {1}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training.iloc[6117:6120,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altered behavior of NLTK so CoreNLP performs sentence splits\n",
    "def constituency_parse(sentences, return_parse_obj=False):\n",
    "    core_nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "    \"\"\"Creates parse strings for each sentence.  \n",
    "    Each parse string can be fed into Tree.fromstring() to create NLTK Tree objects.\n",
    "\n",
    "    parser (CoreNLPParser): parser to parse sentences\n",
    "    sentences (str): essay text\n",
    "    return_parse_obj (bool): return parse object or string of trees\n",
    "    RETURNS (list): a list of parses in string form\n",
    "    \"\"\"\n",
    "    result = list()\n",
    "    index= 0\n",
    "    for s in sentences:\n",
    "        print(index)\n",
    "        default_properties = {'outputFormat': 'json', 'annotators': 'tokenize,pos,parse'}\n",
    "        parsed_data = core_nlp.annotate(s, properties=default_properties)\n",
    "        if return_parse_obj:\n",
    "            return parsed_data\n",
    "        else:\n",
    "            parses = list()\n",
    "            dependencies = list()\n",
    "            for parsed_sent in parsed_data['sentences']:\n",
    "                parse = parsed_sent['parse']\n",
    "                # Compress whitespace\n",
    "                parse = re.sub('[\\s]+', ' ', parse)\n",
    "                parses.append(parse)\n",
    "                if 'enhancedDependencies' in parsed_sent:\n",
    "                    for dep in parsed_sent['enhancedDependencies']:\n",
    "                        dependencies.append(dep['dep'])\n",
    "            result.append((parses, dependencies))\n",
    "        index += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for e in constituency_parse(tr_file['Statement'].values, return_parse_obj=True)['sentences'][0]['enhancedDependencies']:\n",
    "#    print(e['dep'])\n",
    "parsed_sentences=constituency_parse(features_training['text'].values, return_parse_obj=False)\n",
    "parsed_sentences_validation=constituency_parse(features_validation['text'].values, return_parse_obj=False)\n",
    "parsed_sentences_test =constituency_parse(features_testing['text'].values, return_parse_obj=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training['parenthesis'] = tr_file['parenthesis']\n",
    "features_validation['parenthesis']= va_file['parenthesis'] \n",
    "features_testing['parenthesis'] =te_file['parenthesis'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = ['ROOT','advcl','det','predet','preconj','vmod','mwe','mark','advmod','neg','rcmod','quantmod','nn','npadvmod','tmod','num','number','prep','poss','possessive','prt','parataxis','goeswith','punct','ref','sdep','xsubj','root','dep','aux','auxpass','cop','arg','agent','comp','acomp','ccomp','xcomp','obj','dobj','iobj','pobj','subj','nsubj','nsubjpass','csubj','csubjpass','cc','conj','expl','mod','amod','appos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_parsed = list()\n",
    "for each in parsed_sentences:\n",
    "     for each_p in each[1]:\n",
    "        if each_p.split(':')[0] not in unique_parsed:\n",
    "            unique_parsed.append(each_p.split(':')[0])\n",
    "    #print(each[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_parse = np.zeros((len(parsed_sentences), len(unique_parsed))) \n",
    "for x,each in enumerate(parsed_sentences):\n",
    "     for y,up in enumerate(unique_parsed):\n",
    "            if up in each[1]:\n",
    "                vector_parse[x,y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_parse_validation = np.zeros((len(parsed_sentences_validation), len(unique_parsed))) \n",
    "for x,each in enumerate(parsed_sentences_validation):\n",
    "     for y,up in enumerate(unique_parsed):\n",
    "            if up in each[1]:\n",
    "                vector_parse_validation[x,y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mi_parsed = mutual_info_classif(vector_parse,tr_file['b_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#filtered_unique_parser = list()\n",
    "#for index, each in enumerate(mi_parsed):\n",
    "#    if each>=0.005:\n",
    "#        filtered_unique_parser.append(unique_parsed[index])\n",
    "#        print(index, unique_parsed[index], each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstring = [\" \".join(x[1]) for x in parsed_sentences]\n",
    "xstringva = [\" \".join(x[1]) for x in parsed_sentences_validation]\n",
    "\n",
    "xstring = np.array([x.replace(':','_').replace('ROOT','') for x in xstring])\n",
    "xstringva = np.array([x.replace(':','_').replace('ROOT','') for x in xstringva])\n",
    "xtrain = xstring[(tr_file['b_label']!=0)]\n",
    "ytrain = tr_file['b_label'][(tr_file['b_label']!=0)]\n",
    "xtest = xstringva[(va_file['b_label']!=0)]\n",
    "ytest = va_file['b_label'][(va_file['b_label']!=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(penalty='l2', random_state=_RANDOM_STATE_V, max_iter=1000, C=0.5)),\n",
    "])\n",
    "\n",
    "text_clf.fit(xtrain,ytrain)\n",
    "pred = text_clf.predict(xtest)\n",
    "print('Accuracy: ', accuracy_score(pred, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DummyClassifier(strategy='most_frequent').fit(xtrain,ytrain)\n",
    "accuracy_score(dc.predict(xtest),ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for depth in range(1,20):\n",
    "dt_clf = DecisionTreeClassifier(max_depth = 3).fit(xtrain,ytrain)\n",
    "print(depth, accuracy_score(dt_clf.predict(xtest),ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, values in enumerate(dt_clf.feature_importances_):\n",
    "    if values>=0.05:\n",
    "        print(unique_parsed[index], values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({'id':tr_file['Id'],'label':tr_file['Label'],'parsed_sentences': parsed_sentences,'b_labels':tr_file['b_label'] ,'m_labels':tr_file['m_label']}, index= None).to_csv('parsed_sentences.csv',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"parameters = {'batch_size': [25, 32],\n",
    "          'epochs': [100, 500],\n",
    "          'optimizer': ['adam', 'rmsprop']}\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                       param_grid = parameters,\n",
    "                       scoring = 'accuracy',\n",
    "                       cv = 10)\n",
    "grid_search = grid_search.fit(x_train, y_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "_model = KerasClassifier(build_fn=model, verbose=0)\n",
    "\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=_model, param_grid=param_grid, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid.fit(x_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we can go ahead and set the parameter space\n",
    "p = {'lr': (0.5, 5, 10),\n",
    "     'first_neuron':[4, 8, 16, 32, 64],\n",
    "     'hidden_layers':[0, 1, 2],\n",
    "     'batch_size': (2, 30, 10, 64),\n",
    "     'epochs': [150],\n",
    "     'dropout': (0, 0.5,0.8, 5),\n",
    "     'weight_regulizer':[None],\n",
    "     'emb_output_dims': [None],\n",
    "     'shape':['brick','long_funnel'],\n",
    "     #'optimizer': [Adam, Nadam, RMSprop],\n",
    "     #'losses': [logcosh, binary_crossentropy],\n",
    "     #'activation':[relu, elu],\n",
    "     #'last_activation': 'sigmoid'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_model(xtrain, xdev, ytrain, ydev, params):\n",
    "    model = Sequential()\n",
    "    #Simple one hidden layer NN with a Convolutional layer for filtering and GlobalMaxPooling 1D \n",
    "    model.add(Embedding(input_dim=max_features, \n",
    "                               output_dim=embedding_dims, \n",
    "                               input_length=maxlen,\n",
    "                        weights=[embedding_weights],\n",
    "                         trainable=False,\n",
    "                       ))\n",
    "    model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPool1D(5))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPool1D(5))\n",
    "    model.add(Conv1D(filters=128, kernel_size=4, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    #model.add(Dense(hidden_dims, activation='relu'))\n",
    "    model.add(Dense(6, activation='sigmoid'))\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    history = model.fit(xtrain, ytrain,\n",
    "                        epochs=params['epochs'],\n",
    "                        validation_data=[xdev,ydev],\n",
    "                        batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = talos.Scan(x_train, y_train,\n",
    "          params=p,\n",
    "          dataset_name='first_test',\n",
    "          experiment_no='2',\n",
    "          model=nlp_model,\n",
    "          grid_downsample=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords # nltk stop words\n",
    "from nltk.tokenize import word_tokenize # nltk word tokenizer\n",
    "import string\n",
    "import re\n",
    "## Function to clean text data\n",
    "def lemmatize_remove_stop_words(corpus):\n",
    "    print('Processing data...')\n",
    "    result = list()\n",
    "    # define a lemmatizer\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # tokenize and remove stopwords from every sentence\n",
    "    stop_words = stopwords.words('english')\n",
    "    # iterate over every sentence and apply the three filters to remove stopwords, extract lemmas for verbs and nouns\n",
    "    for index, sentence in enumerate(corpus):\n",
    "        #print('Cleaning sentence number:', index, end=\"##\")\n",
    "        #print('Making text all lower case...',end=\"--\")\n",
    "        clean_text = sentence.lower() #  make all words lower case\n",
    "        #print('Removing punctuation...', end=\"--\")\n",
    "        clean_text = re.sub(r'[{0}]'.format(string.punctuation),'',clean_text)# remove punctuation\n",
    "        #print('Removing numberic data and symbols...', end=\"--\")\n",
    "        clean_text = re.sub(r'\\w*\\d\\w*','',clean_text) # remove alpha numerics\n",
    "        #print('Splitting into tokens...', end=\"--\")\n",
    "        tokenized_sentence = word_tokenize(clean_text) # split sentence into tokens\n",
    "        #print('Lemmatizing verbs...',end=\"--\")\n",
    "        filter_one = [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in tokenized_sentence] # lemmatize verbs\n",
    "        #print('Lemmatizing nouns...',end=\"--\")\n",
    "        filter_two = [wordnet_lemmatizer.lemmatize(word, pos=\"n\") for word in filter_one] # lemmatize nouns\n",
    "        #print('Removing stop words...',end=\"--\")\n",
    "        filter_three = [w for w in filter_two if w not in stop_words] # remove stop words\n",
    "        #print('Removing extra white spaces',end=\"--\")\n",
    "        all_clean = (' '.join(filter_three)).strip() # remove extra white spaces\n",
    "        #print('All clean for sentence: ', index,end=\"\\r\")\n",
    "        result.append(all_clean) # append data to result\n",
    "    print('Finished!')\n",
    "    return np.array(result) # convert to numpy array and return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING ML MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training\n",
    "features_validation\n",
    "features_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = lemmatize_remove_stop_words(features_training['text'])\n",
    "Xdev = lemmatize_remove_stop_words(features_validation['text'])\n",
    "Xtest = lemmatize_remove_stop_words(features_testing['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import different models that we want to train\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RANDOM_STATE_V = 45\n",
    "models = [DummyClassifier,\n",
    "          LogisticRegression, \n",
    "          SVC,\n",
    "          ExtraTreesClassifier,\n",
    "          RandomForestClassifier,\n",
    "          AdaBoostClassifier,\n",
    "          GradientBoostingClassifier\n",
    "         ]#, \n",
    "          #SVC]\n",
    "defaults = [{'strategy':'most_frequent', 'random_state': _RANDOM_STATE_V}, #Baseline\n",
    "            {'penalty':'l2','tol':0.0001, 'C':1.0,'max_iter':100,'random_state': _RANDOM_STATE_V}, #Linear Regression\n",
    "            {'C':1.0, 'kernel':'linear', 'degree':3, 'gamma':1,'random_state': _RANDOM_STATE_V},\n",
    "            {'n_estimators':10, 'criterion': 'entropy', 'max_depth':2, 'min_samples_split':2,'random_state': _RANDOM_STATE_V},\n",
    "            {'n_estimators':10, 'criterion': 'entropy', 'max_depth':2, 'min_samples_split':2,'random_state': _RANDOM_STATE_V},\n",
    "            {'n_estimators':50, 'learning_rate':1.0, 'algorithm':'SAMME.R', 'random_state': _RANDOM_STATE_V},\n",
    "            {'loss':'deviance', 'learning_rate':0.1, 'n_estimators':100}\n",
    "            #{'n_neighbors':5},\n",
    "           #{}\n",
    "           ]#, #KNeighborsClassifier\n",
    "            #{'C':1.0, 'kernel':'rbf', 'degree':3, 'gamma':1,'random_state': _RANDOM_STATE_V}, #SVC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = Xtrain[(features_training['b_label']!=0)]\n",
    "Ytr = features_training['b_label'][features_training['b_label']!=0]\n",
    "Xde = Xdev[(features_validation['b_label']!=0)]\n",
    "Yde = features_validation['b_label'][features_validation['b_label']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModels(Xtr, Ytr, Xde, Yde, vocab= None, verbose= True):\n",
    "    report_list = list()\n",
    "    for index, model in enumerate(models):\n",
    "        text_clf = Pipeline([\n",
    "            ('vect', CountVectorizer(vocabulary=vocab)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', model(**defaults[index])),\n",
    "        ])\n",
    "        #text_clf = model(**defaults[index])\n",
    "        if verbose:\n",
    "            print('Training {0}'.format(model.__name__))\n",
    "        text_clf.fit(Xtr, Ytr)\n",
    "        pred = text_clf.predict(Xde)\n",
    "        #print('Accuracy: ', np.mean(pred==Yde))\n",
    "        if verbose:\n",
    "            print('Accuracy: ', accuracy_score(pred,Yde))\n",
    "            print(classification_report(pred, Yde))\n",
    "        #report_list.append((model.__name__, np.mean(pred==Yde), f1_score(pred, Yde, average='binary', pos_label=1),f1_score(pred, Yde, average='binary', pos_label=-1)))\n",
    "        report_list.append((model.__name__, accuracy_score(pred,Yde)))\n",
    "    if verbose:\n",
    "        for e in report_list:\n",
    "            #print('{0},{1},{2}'.format(e[1], e[2], e[3]))\n",
    "            print('{0}: {1}'.format(e[0], e[1]))          \n",
    "    return report_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModels(Xtr, Ytr, Xde, Yde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_training = features_training['pos']\n",
    "pos_validation = features_validation['pos']\n",
    "pos_testing = features_testing['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list()\n",
    "for pos in pos_training:\n",
    "    for ep in pos.split(' '):\n",
    "        if ep not in vocab:\n",
    "            vocab.append(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_str =  [x.lower().replace('$','dollar') for x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Report for every individual tag\n",
    "report_pos = list()\n",
    "for i in range(0,len(vocab)-1):\n",
    "    vocab_filtered = vocab_str[i:i+1]\n",
    "    print(vocab_filtered)\n",
    "    report = trainModels(pos_training[(tr_file['b_label']!=0)], Ytr, pos_validation[(va_file['b_label']!=0)], Yde, vocab=vocab_filtered, \n",
    "                verbose=False)\n",
    "    report_pos.append((vocab_filtered, report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['vbz'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['dt'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['nnps'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['vbp'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['jj'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['nn'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['nns'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['in'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['wrb'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['vbd'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['prp'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['rp'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['wdt'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['to'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['vb'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['nnp'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5868725868725869),\n",
       "   ('SVC', 0.5868725868725869),\n",
       "   ('ExtraTreesClassifier', 0.5868725868725869),\n",
       "   ('RandomForestClassifier', 0.5868725868725869),\n",
       "   ('AdaBoostClassifier', 0.5868725868725869),\n",
       "   ('GradientBoostingClassifier', 0.5868725868725869)]),\n",
       " (['vbg'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['prpdollar'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['vbn'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['rbr'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5965250965250966),\n",
       "   ('SVC', 0.5965250965250966),\n",
       "   ('ExtraTreesClassifier', 0.5965250965250966),\n",
       "   ('RandomForestClassifier', 0.5965250965250966),\n",
       "   ('AdaBoostClassifier', 0.5965250965250966),\n",
       "   ('GradientBoostingClassifier', 0.5965250965250966)]),\n",
       " (['cd'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5868725868725869),\n",
       "   ('SVC', 0.5868725868725869),\n",
       "   ('ExtraTreesClassifier', 0.5868725868725869),\n",
       "   ('RandomForestClassifier', 0.5868725868725869),\n",
       "   ('AdaBoostClassifier', 0.5868725868725869),\n",
       "   ('GradientBoostingClassifier', 0.5868725868725869)]),\n",
       " (['rb'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['wp'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['jjs'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5994208494208494),\n",
       "   ('SVC', 0.5994208494208494),\n",
       "   ('ExtraTreesClassifier', 0.5994208494208494),\n",
       "   ('RandomForestClassifier', 0.5994208494208494),\n",
       "   ('AdaBoostClassifier', 0.5994208494208494),\n",
       "   ('GradientBoostingClassifier', 0.5994208494208494)]),\n",
       " (['cc'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['jjr'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.6206563706563707),\n",
       "   ('SVC', 0.6206563706563707),\n",
       "   ('ExtraTreesClassifier', 0.6206563706563707),\n",
       "   ('RandomForestClassifier', 0.6206563706563707),\n",
       "   ('AdaBoostClassifier', 0.6206563706563707),\n",
       "   ('GradientBoostingClassifier', 0.6206563706563707)]),\n",
       " (['md'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['pdt'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5926640926640927),\n",
       "   ('SVC', 0.5926640926640927),\n",
       "   ('ExtraTreesClassifier', 0.5926640926640927),\n",
       "   ('RandomForestClassifier', 0.5926640926640927),\n",
       "   ('AdaBoostClassifier', 0.5926640926640927),\n",
       "   ('GradientBoostingClassifier', 0.5926640926640927)]),\n",
       " (['ex'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5955598455598455),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['rbs'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5916988416988417),\n",
       "   ('SVC', 0.5916988416988417),\n",
       "   ('ExtraTreesClassifier', 0.5916988416988417),\n",
       "   ('RandomForestClassifier', 0.5916988416988417),\n",
       "   ('AdaBoostClassifier', 0.5916988416988417),\n",
       "   ('GradientBoostingClassifier', 0.5916988416988417)]),\n",
       " (['uh'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['wpdollar'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5955598455598455),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5955598455598455),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)]),\n",
       " (['fw'],\n",
       "  [('DummyClassifier', 0.5945945945945946),\n",
       "   ('LogisticRegression', 0.5945945945945946),\n",
       "   ('SVC', 0.5945945945945946),\n",
       "   ('ExtraTreesClassifier', 0.5945945945945946),\n",
       "   ('RandomForestClassifier', 0.5945945945945946),\n",
       "   ('AdaBoostClassifier', 0.5945945945945946),\n",
       "   ('GradientBoostingClassifier', 0.5945945945945946)])]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vbz', 'dt']\n",
      "['vbz', 'nnps']\n",
      "['vbz', 'vbp']\n",
      "['vbz', 'jj']\n",
      "['vbz', 'nn']\n",
      "['vbz', 'nns']\n",
      "['vbz', 'in']\n",
      "['vbz', 'wrb']\n",
      "['vbz', 'vbd']\n",
      "['vbz', 'prp']\n",
      "['vbz', 'rp']\n",
      "['vbz', 'wdt']\n",
      "['vbz', 'to']\n",
      "['vbz', 'vb']\n",
      "['vbz', 'nnp']\n",
      "['vbz', 'vbg']\n",
      "['vbz', 'prpdollar']\n",
      "['vbz', 'vbn']\n",
      "['vbz', 'rbr']\n",
      "['vbz', 'cd']\n",
      "['vbz', 'rb']\n",
      "['vbz', 'wp']\n",
      "['vbz', 'jjs']\n",
      "['vbz', 'cc']\n",
      "['vbz', 'jjr']\n",
      "['vbz', 'md']\n",
      "['vbz', 'pdt']\n",
      "['vbz', 'ex']\n",
      "['vbz', 'rbs']\n",
      "['vbz', 'uh']\n",
      "['vbz', 'wpdollar']\n",
      "['vbz', 'fw']\n",
      "['vbz', 'ls']\n",
      "['dt', 'nnps']\n",
      "['dt', 'vbp']\n",
      "['dt', 'jj']\n",
      "['dt', 'nn']\n",
      "['dt', 'nns']\n",
      "['dt', 'in']\n",
      "['dt', 'wrb']\n",
      "['dt', 'vbd']\n",
      "['dt', 'prp']\n",
      "['dt', 'rp']\n",
      "['dt', 'wdt']\n",
      "['dt', 'to']\n",
      "['dt', 'vb']\n",
      "['dt', 'nnp']\n",
      "['dt', 'vbg']\n",
      "['dt', 'prpdollar']\n",
      "['dt', 'vbn']\n",
      "['dt', 'rbr']\n",
      "['dt', 'cd']\n",
      "['dt', 'rb']\n",
      "['dt', 'wp']\n",
      "['dt', 'jjs']\n",
      "['dt', 'cc']\n",
      "['dt', 'jjr']\n",
      "['dt', 'md']\n",
      "['dt', 'pdt']\n",
      "['dt', 'ex']\n",
      "['dt', 'rbs']\n",
      "['dt', 'uh']\n",
      "['dt', 'wpdollar']\n",
      "['dt', 'fw']\n",
      "['dt', 'ls']\n",
      "['nnps', 'vbp']\n",
      "['nnps', 'jj']\n",
      "['nnps', 'nn']\n",
      "['nnps', 'nns']\n",
      "['nnps', 'in']\n",
      "['nnps', 'wrb']\n",
      "['nnps', 'vbd']\n",
      "['nnps', 'prp']\n",
      "['nnps', 'rp']\n",
      "['nnps', 'wdt']\n",
      "['nnps', 'to']\n",
      "['nnps', 'vb']\n",
      "['nnps', 'nnp']\n",
      "['nnps', 'vbg']\n",
      "['nnps', 'prpdollar']\n",
      "['nnps', 'vbn']\n",
      "['nnps', 'rbr']\n",
      "['nnps', 'cd']\n",
      "['nnps', 'rb']\n",
      "['nnps', 'wp']\n",
      "['nnps', 'jjs']\n",
      "['nnps', 'cc']\n",
      "['nnps', 'jjr']\n",
      "['nnps', 'md']\n",
      "['nnps', 'pdt']\n",
      "['nnps', 'ex']\n",
      "['nnps', 'rbs']\n",
      "['nnps', 'uh']\n",
      "['nnps', 'wpdollar']\n",
      "['nnps', 'fw']\n",
      "['nnps', 'ls']\n",
      "['vbp', 'jj']\n",
      "['vbp', 'nn']\n",
      "['vbp', 'nns']\n",
      "['vbp', 'in']\n",
      "['vbp', 'wrb']\n",
      "['vbp', 'vbd']\n",
      "['vbp', 'prp']\n",
      "['vbp', 'rp']\n",
      "['vbp', 'wdt']\n",
      "['vbp', 'to']\n",
      "['vbp', 'vb']\n",
      "['vbp', 'nnp']\n",
      "['vbp', 'vbg']\n",
      "['vbp', 'prpdollar']\n",
      "['vbp', 'vbn']\n",
      "['vbp', 'rbr']\n",
      "['vbp', 'cd']\n",
      "['vbp', 'rb']\n",
      "['vbp', 'wp']\n",
      "['vbp', 'jjs']\n",
      "['vbp', 'cc']\n",
      "['vbp', 'jjr']\n",
      "['vbp', 'md']\n",
      "['vbp', 'pdt']\n",
      "['vbp', 'ex']\n",
      "['vbp', 'rbs']\n",
      "['vbp', 'uh']\n",
      "['vbp', 'wpdollar']\n",
      "['vbp', 'fw']\n",
      "['vbp', 'ls']\n",
      "['jj', 'nn']\n",
      "['jj', 'nns']\n",
      "['jj', 'in']\n",
      "['jj', 'wrb']\n",
      "['jj', 'vbd']\n",
      "['jj', 'prp']\n",
      "['jj', 'rp']\n",
      "['jj', 'wdt']\n",
      "['jj', 'to']\n",
      "['jj', 'vb']\n",
      "['jj', 'nnp']\n",
      "['jj', 'vbg']\n",
      "['jj', 'prpdollar']\n",
      "['jj', 'vbn']\n",
      "['jj', 'rbr']\n",
      "['jj', 'cd']\n",
      "['jj', 'rb']\n",
      "['jj', 'wp']\n",
      "['jj', 'jjs']\n",
      "['jj', 'cc']\n",
      "['jj', 'jjr']\n",
      "['jj', 'md']\n",
      "['jj', 'pdt']\n",
      "['jj', 'ex']\n",
      "['jj', 'rbs']\n",
      "['jj', 'uh']\n",
      "['jj', 'wpdollar']\n",
      "['jj', 'fw']\n",
      "['jj', 'ls']\n",
      "['nn', 'nns']\n",
      "['nn', 'in']\n",
      "['nn', 'wrb']\n",
      "['nn', 'vbd']\n",
      "['nn', 'prp']\n",
      "['nn', 'rp']\n",
      "['nn', 'wdt']\n",
      "['nn', 'to']\n",
      "['nn', 'vb']\n",
      "['nn', 'nnp']\n",
      "['nn', 'vbg']\n",
      "['nn', 'prpdollar']\n",
      "['nn', 'vbn']\n",
      "['nn', 'rbr']\n",
      "['nn', 'cd']\n",
      "['nn', 'rb']\n",
      "['nn', 'wp']\n",
      "['nn', 'jjs']\n",
      "['nn', 'cc']\n",
      "['nn', 'jjr']\n",
      "['nn', 'md']\n",
      "['nn', 'pdt']\n",
      "['nn', 'ex']\n",
      "['nn', 'rbs']\n",
      "['nn', 'uh']\n",
      "['nn', 'wpdollar']\n",
      "['nn', 'fw']\n",
      "['nn', 'ls']\n",
      "['nns', 'in']\n",
      "['nns', 'wrb']\n",
      "['nns', 'vbd']\n",
      "['nns', 'prp']\n",
      "['nns', 'rp']\n",
      "['nns', 'wdt']\n",
      "['nns', 'to']\n",
      "['nns', 'vb']\n",
      "['nns', 'nnp']\n",
      "['nns', 'vbg']\n",
      "['nns', 'prpdollar']\n",
      "['nns', 'vbn']\n",
      "['nns', 'rbr']\n",
      "['nns', 'cd']\n",
      "['nns', 'rb']\n",
      "['nns', 'wp']\n",
      "['nns', 'jjs']\n",
      "['nns', 'cc']\n",
      "['nns', 'jjr']\n",
      "['nns', 'md']\n",
      "['nns', 'pdt']\n",
      "['nns', 'ex']\n",
      "['nns', 'rbs']\n",
      "['nns', 'uh']\n",
      "['nns', 'wpdollar']\n",
      "['nns', 'fw']\n",
      "['nns', 'ls']\n",
      "['in', 'wrb']\n",
      "['in', 'vbd']\n",
      "['in', 'prp']\n",
      "['in', 'rp']\n",
      "['in', 'wdt']\n",
      "['in', 'to']\n",
      "['in', 'vb']\n",
      "['in', 'nnp']\n",
      "['in', 'vbg']\n",
      "['in', 'prpdollar']\n",
      "['in', 'vbn']\n",
      "['in', 'rbr']\n",
      "['in', 'cd']\n",
      "['in', 'rb']\n",
      "['in', 'wp']\n",
      "['in', 'jjs']\n",
      "['in', 'cc']\n",
      "['in', 'jjr']\n",
      "['in', 'md']\n",
      "['in', 'pdt']\n",
      "['in', 'ex']\n",
      "['in', 'rbs']\n",
      "['in', 'uh']\n",
      "['in', 'wpdollar']\n",
      "['in', 'fw']\n",
      "['in', 'ls']\n",
      "['wrb', 'vbd']\n",
      "['wrb', 'prp']\n",
      "['wrb', 'rp']\n",
      "['wrb', 'wdt']\n",
      "['wrb', 'to']\n",
      "['wrb', 'vb']\n",
      "['wrb', 'nnp']\n",
      "['wrb', 'vbg']\n",
      "['wrb', 'prpdollar']\n",
      "['wrb', 'vbn']\n",
      "['wrb', 'rbr']\n",
      "['wrb', 'cd']\n",
      "['wrb', 'rb']\n",
      "['wrb', 'wp']\n",
      "['wrb', 'jjs']\n",
      "['wrb', 'cc']\n",
      "['wrb', 'jjr']\n",
      "['wrb', 'md']\n",
      "['wrb', 'pdt']\n",
      "['wrb', 'ex']\n",
      "['wrb', 'rbs']\n",
      "['wrb', 'uh']\n",
      "['wrb', 'wpdollar']\n",
      "['wrb', 'fw']\n",
      "['wrb', 'ls']\n",
      "['vbd', 'prp']\n",
      "['vbd', 'rp']\n",
      "['vbd', 'wdt']\n",
      "['vbd', 'to']\n",
      "['vbd', 'vb']\n",
      "['vbd', 'nnp']\n",
      "['vbd', 'vbg']\n",
      "['vbd', 'prpdollar']\n",
      "['vbd', 'vbn']\n",
      "['vbd', 'rbr']\n",
      "['vbd', 'cd']\n",
      "['vbd', 'rb']\n",
      "['vbd', 'wp']\n",
      "['vbd', 'jjs']\n",
      "['vbd', 'cc']\n",
      "['vbd', 'jjr']\n",
      "['vbd', 'md']\n",
      "['vbd', 'pdt']\n",
      "['vbd', 'ex']\n",
      "['vbd', 'rbs']\n",
      "['vbd', 'uh']\n",
      "['vbd', 'wpdollar']\n",
      "['vbd', 'fw']\n",
      "['vbd', 'ls']\n",
      "['prp', 'rp']\n",
      "['prp', 'wdt']\n",
      "['prp', 'to']\n",
      "['prp', 'vb']\n",
      "['prp', 'nnp']\n",
      "['prp', 'vbg']\n",
      "['prp', 'prpdollar']\n",
      "['prp', 'vbn']\n",
      "['prp', 'rbr']\n",
      "['prp', 'cd']\n",
      "['prp', 'rb']\n",
      "['prp', 'wp']\n",
      "['prp', 'jjs']\n",
      "['prp', 'cc']\n",
      "['prp', 'jjr']\n",
      "['prp', 'md']\n",
      "['prp', 'pdt']\n",
      "['prp', 'ex']\n",
      "['prp', 'rbs']\n",
      "['prp', 'uh']\n",
      "['prp', 'wpdollar']\n",
      "['prp', 'fw']\n",
      "['prp', 'ls']\n",
      "['rp', 'wdt']\n",
      "['rp', 'to']\n",
      "['rp', 'vb']\n",
      "['rp', 'nnp']\n",
      "['rp', 'vbg']\n",
      "['rp', 'prpdollar']\n",
      "['rp', 'vbn']\n",
      "['rp', 'rbr']\n",
      "['rp', 'cd']\n",
      "['rp', 'rb']\n",
      "['rp', 'wp']\n",
      "['rp', 'jjs']\n",
      "['rp', 'cc']\n",
      "['rp', 'jjr']\n",
      "['rp', 'md']\n",
      "['rp', 'pdt']\n",
      "['rp', 'ex']\n",
      "['rp', 'rbs']\n",
      "['rp', 'uh']\n",
      "['rp', 'wpdollar']\n",
      "['rp', 'fw']\n",
      "['rp', 'ls']\n",
      "['wdt', 'to']\n",
      "['wdt', 'vb']\n",
      "['wdt', 'nnp']\n",
      "['wdt', 'vbg']\n",
      "['wdt', 'prpdollar']\n",
      "['wdt', 'vbn']\n",
      "['wdt', 'rbr']\n",
      "['wdt', 'cd']\n",
      "['wdt', 'rb']\n",
      "['wdt', 'wp']\n",
      "['wdt', 'jjs']\n",
      "['wdt', 'cc']\n",
      "['wdt', 'jjr']\n",
      "['wdt', 'md']\n",
      "['wdt', 'pdt']\n",
      "['wdt', 'ex']\n",
      "['wdt', 'rbs']\n",
      "['wdt', 'uh']\n",
      "['wdt', 'wpdollar']\n",
      "['wdt', 'fw']\n",
      "['wdt', 'ls']\n",
      "['to', 'vb']\n",
      "['to', 'nnp']\n",
      "['to', 'vbg']\n",
      "['to', 'prpdollar']\n",
      "['to', 'vbn']\n",
      "['to', 'rbr']\n",
      "['to', 'cd']\n",
      "['to', 'rb']\n",
      "['to', 'wp']\n",
      "['to', 'jjs']\n",
      "['to', 'cc']\n",
      "['to', 'jjr']\n",
      "['to', 'md']\n",
      "['to', 'pdt']\n",
      "['to', 'ex']\n",
      "['to', 'rbs']\n",
      "['to', 'uh']\n",
      "['to', 'wpdollar']\n",
      "['to', 'fw']\n",
      "['to', 'ls']\n",
      "['vb', 'nnp']\n",
      "['vb', 'vbg']\n",
      "['vb', 'prpdollar']\n",
      "['vb', 'vbn']\n",
      "['vb', 'rbr']\n",
      "['vb', 'cd']\n",
      "['vb', 'rb']\n",
      "['vb', 'wp']\n",
      "['vb', 'jjs']\n",
      "['vb', 'cc']\n",
      "['vb', 'jjr']\n",
      "['vb', 'md']\n",
      "['vb', 'pdt']\n",
      "['vb', 'ex']\n",
      "['vb', 'rbs']\n",
      "['vb', 'uh']\n",
      "['vb', 'wpdollar']\n",
      "['vb', 'fw']\n",
      "['vb', 'ls']\n",
      "['nnp', 'vbg']\n",
      "['nnp', 'prpdollar']\n",
      "['nnp', 'vbn']\n",
      "['nnp', 'rbr']\n",
      "['nnp', 'cd']\n",
      "['nnp', 'rb']\n",
      "['nnp', 'wp']\n",
      "['nnp', 'jjs']\n",
      "['nnp', 'cc']\n",
      "['nnp', 'jjr']\n",
      "['nnp', 'md']\n",
      "['nnp', 'pdt']\n",
      "['nnp', 'ex']\n",
      "['nnp', 'rbs']\n",
      "['nnp', 'uh']\n",
      "['nnp', 'wpdollar']\n",
      "['nnp', 'fw']\n",
      "['nnp', 'ls']\n",
      "['vbg', 'prpdollar']\n",
      "['vbg', 'vbn']\n",
      "['vbg', 'rbr']\n",
      "['vbg', 'cd']\n",
      "['vbg', 'rb']\n",
      "['vbg', 'wp']\n",
      "['vbg', 'jjs']\n",
      "['vbg', 'cc']\n",
      "['vbg', 'jjr']\n",
      "['vbg', 'md']\n",
      "['vbg', 'pdt']\n",
      "['vbg', 'ex']\n",
      "['vbg', 'rbs']\n",
      "['vbg', 'uh']\n",
      "['vbg', 'wpdollar']\n",
      "['vbg', 'fw']\n",
      "['vbg', 'ls']\n",
      "['prpdollar', 'vbn']\n",
      "['prpdollar', 'rbr']\n",
      "['prpdollar', 'cd']\n",
      "['prpdollar', 'rb']\n",
      "['prpdollar', 'wp']\n",
      "['prpdollar', 'jjs']\n",
      "['prpdollar', 'cc']\n",
      "['prpdollar', 'jjr']\n",
      "['prpdollar', 'md']\n",
      "['prpdollar', 'pdt']\n",
      "['prpdollar', 'ex']\n",
      "['prpdollar', 'rbs']\n",
      "['prpdollar', 'uh']\n",
      "['prpdollar', 'wpdollar']\n",
      "['prpdollar', 'fw']\n",
      "['prpdollar', 'ls']\n",
      "['vbn', 'rbr']\n",
      "['vbn', 'cd']\n",
      "['vbn', 'rb']\n",
      "['vbn', 'wp']\n",
      "['vbn', 'jjs']\n",
      "['vbn', 'cc']\n",
      "['vbn', 'jjr']\n",
      "['vbn', 'md']\n",
      "['vbn', 'pdt']\n",
      "['vbn', 'ex']\n",
      "['vbn', 'rbs']\n",
      "['vbn', 'uh']\n",
      "['vbn', 'wpdollar']\n",
      "['vbn', 'fw']\n",
      "['vbn', 'ls']\n",
      "['rbr', 'cd']\n",
      "['rbr', 'rb']\n",
      "['rbr', 'wp']\n",
      "['rbr', 'jjs']\n",
      "['rbr', 'cc']\n",
      "['rbr', 'jjr']\n",
      "['rbr', 'md']\n",
      "['rbr', 'pdt']\n",
      "['rbr', 'ex']\n",
      "['rbr', 'rbs']\n",
      "['rbr', 'uh']\n",
      "['rbr', 'wpdollar']\n",
      "['rbr', 'fw']\n",
      "['rbr', 'ls']\n",
      "['cd', 'rb']\n",
      "['cd', 'wp']\n",
      "['cd', 'jjs']\n",
      "['cd', 'cc']\n",
      "['cd', 'jjr']\n",
      "['cd', 'md']\n",
      "['cd', 'pdt']\n",
      "['cd', 'ex']\n",
      "['cd', 'rbs']\n",
      "['cd', 'uh']\n",
      "['cd', 'wpdollar']\n",
      "['cd', 'fw']\n",
      "['cd', 'ls']\n",
      "['rb', 'wp']\n",
      "['rb', 'jjs']\n",
      "['rb', 'cc']\n",
      "['rb', 'jjr']\n",
      "['rb', 'md']\n",
      "['rb', 'pdt']\n",
      "['rb', 'ex']\n",
      "['rb', 'rbs']\n",
      "['rb', 'uh']\n",
      "['rb', 'wpdollar']\n",
      "['rb', 'fw']\n",
      "['rb', 'ls']\n",
      "['wp', 'jjs']\n",
      "['wp', 'cc']\n",
      "['wp', 'jjr']\n",
      "['wp', 'md']\n",
      "['wp', 'pdt']\n",
      "['wp', 'ex']\n",
      "['wp', 'rbs']\n",
      "['wp', 'uh']\n",
      "['wp', 'wpdollar']\n",
      "['wp', 'fw']\n",
      "['wp', 'ls']\n",
      "['jjs', 'cc']\n",
      "['jjs', 'jjr']\n",
      "['jjs', 'md']\n",
      "['jjs', 'pdt']\n",
      "['jjs', 'ex']\n",
      "['jjs', 'rbs']\n",
      "['jjs', 'uh']\n",
      "['jjs', 'wpdollar']\n",
      "['jjs', 'fw']\n",
      "['jjs', 'ls']\n",
      "['cc', 'jjr']\n",
      "['cc', 'md']\n",
      "['cc', 'pdt']\n",
      "['cc', 'ex']\n",
      "['cc', 'rbs']\n",
      "['cc', 'uh']\n",
      "['cc', 'wpdollar']\n",
      "['cc', 'fw']\n",
      "['cc', 'ls']\n",
      "['jjr', 'md']\n",
      "['jjr', 'pdt']\n",
      "['jjr', 'ex']\n",
      "['jjr', 'rbs']\n",
      "['jjr', 'uh']\n",
      "['jjr', 'wpdollar']\n",
      "['jjr', 'fw']\n",
      "['jjr', 'ls']\n",
      "['md', 'pdt']\n",
      "['md', 'ex']\n",
      "['md', 'rbs']\n",
      "['md', 'uh']\n",
      "['md', 'wpdollar']\n",
      "['md', 'fw']\n",
      "['md', 'ls']\n",
      "['pdt', 'ex']\n",
      "['pdt', 'rbs']\n",
      "['pdt', 'uh']\n",
      "['pdt', 'wpdollar']\n",
      "['pdt', 'fw']\n",
      "['pdt', 'ls']\n",
      "['ex', 'rbs']\n",
      "['ex', 'uh']\n",
      "['ex', 'wpdollar']\n",
      "['ex', 'fw']\n",
      "['ex', 'ls']\n",
      "['rbs', 'uh']\n",
      "['rbs', 'wpdollar']\n",
      "['rbs', 'fw']\n",
      "['rbs', 'ls']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uh', 'wpdollar']\n",
      "['uh', 'fw']\n",
      "['uh', 'ls']\n",
      "['wpdollar', 'fw']\n",
      "['wpdollar', 'ls']\n",
      "['fw', 'ls']\n"
     ]
    }
   ],
   "source": [
    "# Sequentially Adding pos.\n",
    "report_pos_sequential = list()\n",
    "for x in range(0,len(vocab)):\n",
    "    for y in range(x+1, len(vocab)):\n",
    "        vocab_filtered = [vocab_str[x],vocab_str[y]]\n",
    "        print(vocab_filtered)\n",
    "        report = trainModels(pos_training[(tr_file['b_label']!=0)], Ytr, pos_validation[(va_file['b_label']!=0)], Yde, vocab=vocab_filtered, \n",
    "                    verbose=False)\n",
    "        report_pos_sequential.append((vocab_filtered, report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_jjr_list = list()\n",
    "for x in report_pos_sequential:\n",
    "    for y in x[1]:\n",
    "        if y[1]>0.6207:\n",
    "            #print(x[0])\n",
    "            #print(y)\n",
    "            filtered_jjr_list.append(x[0][0])\n",
    "            filtered_jjr_list.append(x[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_jjr_list = np.unique(filtered_jjr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp',\n",
       "       'nns', 'prp', 'vb', 'vbd', 'vbg', 'vbp'], dtype='<U3')"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_jjr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'vbz']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'nnps']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'nn']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'wrb']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'rp']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'wdt']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'to']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'prpdollar']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'vbn']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'rbr']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'rb']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'wp']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'md']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'pdt']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'rbs']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'uh']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'wpdollar']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'prp' 'vb'\n",
      " 'vbd' 'vbg' 'vbp' 'fw']\n"
     ]
    }
   ],
   "source": [
    "# Sequentially Adding pos.\n",
    "report_pos_sequential_jjr = list()\n",
    "for x in range(0,len(vocab_str)):\n",
    "    #for y in range(x+1, len(vocab)):\n",
    "    if vocab_str[x] not in filtered_jjr_list:\n",
    "        vocab_filtered = np.append(filtered_jjr_list, vocab_str[x])\n",
    "        print(vocab_filtered)\n",
    "        report = trainModels(pos_training[(tr_file['b_label']!=0)], Ytr, pos_validation[(va_file['b_label']!=0)], Yde, vocab=vocab_filtered, \n",
    "                    verbose=False)\n",
    "        report_pos_sequential_jjr.append((vocab_filtered, report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered_jjr_list_2 = list()\n",
    "for x in report_pos_sequential_jjr:\n",
    "    for y in x[1]:\n",
    "        if y[1]>0.635:\n",
    "            for vv in x[0]:\n",
    "                filtered_jjr_list_2.append(vv)\n",
    "            #print(x[0])\n",
    "            #print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_jjr_list_2 = np.unique(filtered_jjr_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'vbz']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'nnps']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'nn']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wrb']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wdt']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'to']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'vbn']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'rbr']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wp']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'md']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'rbs']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'uh']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'fw']\n"
     ]
    }
   ],
   "source": [
    "# Sequentially Adding pos.\n",
    "report_pos_sequential_jjr_2 = list()\n",
    "for x in range(0,len(vocab_str)):\n",
    "    #for y in range(x+1, len(vocab)):\n",
    "    if vocab_str[x] not in filtered_jjr_list_2:\n",
    "        vocab_filtered = np.append(filtered_jjr_list_2, vocab_str[x])\n",
    "        print(vocab_filtered)\n",
    "        report = trainModels(pos_training[(tr_file['b_label']!=0)], Ytr, pos_validation[(va_file['b_label']!=0)], Yde, vocab=vocab_filtered, \n",
    "                    verbose=False)\n",
    "        report_pos_sequential_jjr_2.append((vocab_filtered, report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6438\n",
    "filtered_jjr_list_3 = list()\n",
    "for x in report_pos_sequential_jjr_2:\n",
    "    for y in x[1]:\n",
    "        if y[1]>0.6438:\n",
    "             for vv in x[0]:\n",
    "                filtered_jjr_list_3.append(vv)\n",
    "            #print(x[0])\n",
    "            #print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_jjr_list_3 = np.unique(filtered_jjr_list_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'vbz']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'nnps']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'nn']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wrb']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wdt']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'to']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'vbn']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'rbr']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wp']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'rbs']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'uh']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'fw']\n"
     ]
    }
   ],
   "source": [
    "# Sequentially Adding pos.\n",
    "report_pos_sequential_jjr_3 = list()\n",
    "for x in range(0,len(vocab_str)):\n",
    "    #for y in range(x+1, len(vocab)):\n",
    "    if vocab_str[x] not in filtered_jjr_list_3:\n",
    "        vocab_filtered = np.append(filtered_jjr_list_2, vocab_str[x])\n",
    "        print(vocab_filtered)\n",
    "        report = trainModels(pos_training[(tr_file['b_label']!=0)], Ytr, pos_validation[(va_file['b_label']!=0)], Yde, vocab=vocab_filtered, \n",
    "                    verbose=False)\n",
    "        report_pos_sequential_jjr_3.append((vocab_filtered, report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'vbn']\n",
      "('GradientBoostingClassifier', 0.6428571428571429)\n"
     ]
    }
   ],
   "source": [
    "# 0.6438\n",
    "filtered_jjr_list_4 = list()\n",
    "for x in report_pos_sequential_jjr_3:\n",
    "    for y in x[1]:\n",
    "        if y[1]>0.64:\n",
    "            for vv in x[0]:\n",
    "                filtered_jjr_list_4.append(vv)\n",
    "            print(x[0])\n",
    "            print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_jjr_list_4 = np.unique(filtered_jjr_list_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cc', 'cd', 'dt', 'ex', 'in', 'jj', 'jjr', 'jjs', 'ls', 'nnp',\n",
       "       'nns', 'pdt', 'prp', 'prpdollar', 'rb', 'rp', 'vb', 'vbd', 'vbg',\n",
       "       'vbn', 'vbp', 'wpdollar'], dtype='<U9')"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_jjr_list_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'vbz']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'nnps']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'nn']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wrb']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wdt']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'to']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'rbr']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'wp']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'md']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'rbs']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'uh']\n",
      "['cc' 'cd' 'dt' 'ex' 'in' 'jj' 'jjr' 'jjs' 'ls' 'nnp' 'nns' 'pdt' 'prp'\n",
      " 'prpdollar' 'rb' 'rp' 'vb' 'vbd' 'vbg' 'vbp' 'wpdollar' 'fw']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-243-fcc93d9f550e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_filtered\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         report = trainModels(pos_training[(tr_file['b_label']!=0)], Ytr, pos_validation[(va_file['b_label']!=0)], Yde, vocab=vocab_filtered, \n\u001b[1;32m----> 9\u001b[1;33m                     verbose=False)\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mreport_pos_sequential_jjr_4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_filtered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-170-6675eb3f80d6>\u001b[0m in \u001b[0;36mtrainModels\u001b[1;34m(Xtr, Ytr, Xde, Yde, vocab, verbose)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mtext_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXde\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m#print('Accuracy: ', np.mean(pred==Yde))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_sparse_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    291\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshrinking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m                 random_seed)\n\u001b[0m\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\svm\\libsvm_sparse.pyx\u001b[0m in \u001b[0;36msklearn.svm.libsvm_sparse.libsvm_sparse_train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;34m\"\"\"base matrix class for compressed row and column oriented matrices\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0m_data_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Sequentially Adding pos.\n",
    "report_pos_sequential_jjr_4 = list()\n",
    "for x in range(0,len(vocab_str)):\n",
    "    #for y in range(x+1, len(vocab)):\n",
    "    if vocab_str[x] not in filtered_jjr_list_4:\n",
    "        vocab_filtered = np.append(filtered_jjr_list_2, vocab_str[x])\n",
    "        print(vocab_filtered)\n",
    "        report = trainModels(pos_training[(tr_file['b_label']!=0)], Ytr, pos_validation[(va_file['b_label']!=0)], Yde, vocab=vocab_filtered, \n",
    "                    verbose=False)\n",
    "        report_pos_sequential_jjr_4.append((vocab_filtered, report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20ea8334400>]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD0CAYAAAB+WlaPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl0U2X+P/B3tqZtku77Bm2h0IUCLZtgKwIti6AMezuUARxGHTeUQRx10IN8Wfwe/TrqgAqI/KpAQXQEZUdokZ1A6ZZCKaUb3VtKkzbNdn9/VIqVsqVJbnLzeZ3DOfTe3Hs/Dwmf3jz3eT4Pj2EYBoQQQjiJz3YAhBBCzIeSPCGEcBgleUII4TBK8oQQwmGU5AkhhMMoyRNCCIcJ2Q7gj+RyOdshEEKITYqPj79rm9UleaD7QB+GQqFAZGSkiaOxbtRm+0Bttg89afO9bpCpu4YQQjiMkjwhhHAYJXlCCOEwSvKEEMJhlOQJIYTDepTkL126hLS0tLu2//LLL5g+fTpmz56NHTt2AADUajVefvllpKamYtGiRWhsbOzJpQkhhDwEo5P8hg0b8M4776C9vb3Ldq1Wi9WrV+Orr75Ceno6MjIyUFdXh23btiEiIgJbt27F1KlTsW7duh4HTwgh5P6MHicfEhKCTz/9FG+88UaX7cXFxQgJCYGrqyuAjjHv58+fh1wux1//+lcAQGJiot0n+Ra1Fvtyq3FYUYO8ymY0qDQwMAx8XRwRG+SK0f18MDnWH84OVjmVgRCjafUGZF2pw97calyquImKplZodAZ4ScWI8JXhiQhvPD0ogO0wOcPoDDJ+/HhUVFTctV2pVEImk3X+LJFIoFQqu2yXSCRoaWm557kVCoVRManVaqOPtZQ2rQHf5zdjV8FNtGkZ+EqFiPR2hFewGDwAtUodzhR3/AdYsTsP06NdMS3aDQ4CXrfns4U2mxq12TYZGAZHipX49lITapQ6yMR8RHk7IravDCIBD41tOhTVN+N/rtZj7X4FRvdyxsI2Hdyd7OdGxxzvs8n/9aRSKVQqVefPKpUKMpmsy3aVSgUXF5d7nsPYGV/WPkPu3PVGvL4nG+WNbZgY44dFiWEYHOwGHq9rAmcYBueuN+HLrGvYcrEGWeUafDJnMAYEud51TmtvszlQm21PRVMrlu7MwalrDRgQ6IoVf+qDJ/v5wEF4d4/x9XoVvjpRgq1nSnG26gbenxqDZwYFshC15dnEjNfw8HCUlpbi5s2b0Gg0OH/+PAYPHoy4uDhkZmYCALKysowuXWCrvj1TijlfngYA7HjuMayfG4+4EPe7EjwA8Hg8DAv1wMa/DMH/WzgM7Vo9pq8/iR3nyi0dNiE9dv56I57+7ARyKm5izbQB2P3SKIyP9us2wQNAby8JVjwTg3VPB6Gfnwyvbs/Gv/6bB53eYOHIucFkd/J79uxBa2srZs+ejTfffBPPPvssGIbB9OnT4evri5SUFCxbtgwpKSkQiUT48MMPTXVpq8YwDD44cBnrjxVjTH8ffJIyGFLxw/+zJ0Z446dXEvDq9ot4Y1cOGlQavDA63IwRE2I6+/Oq8Mr2bAS6OWHT848hzFv60McGuzpg26IB+ODAZXyZdQ31ynZ8PGcQxEKBGSPmnh4l+aCgoM4hklOmTOncPmbMGIwZM6bLa52cnPDJJ5/05HI26cODV7D+WDFSh4dgxdPREAoe/cuTh8QBm/4yFEt2XsLa/YXQ6g14ZWxfM0RLiOkcLqjBS1svIjbIFZv+MhTuEodHPodQwMdbkyLh5+KIFT8VoC1djg3zhkBkxP8je0X/Umb0RWYxPjt6FSnDgvE/U2OMSvC3OQj5+Hj2IEyLC8RHh65g29kyE0ZKiGmdvtaAv397AdEBLtiycJhRCf73Fj4eilV/GoBjl+vw5q5cMAxjoki5z34eW1vYEUUN1uwvxFOx/lg5dUC3fe+PSsDnYe30WDQoNXj7h1wEuDnBxwSxEmJK5Y2teOEbOUI8nbFl4TDIHEUmOW/q8BDUtqjx8eEiBHs4YfG4CJOcl+voTt4MrtYq8er2bEQHuODDmQMh4Pc8wd8mEvCx7s9x6Osjw6vbL6JWqTPZuQnpqVaNDov+33kYGGDjvCFwc+7ZHfwfvTq2L6bHBeHjw0U4ernWpOfmKkryJtau0+OlrRcgFvLxZdoQOIpM/5BIIhZi/dw46PQMVmXWQKOjUQfEOrz/UwEu17Tgs9TB6O0lMfn5eTweVk6NQX8/GV7LyEblzTaTX4NrKMmb2IcHr6CwugX/OzMWAW5OZrtOmLcUH8yIxeX6dnx29KrZrkPIwzqQX41tZ8vxXGI4Evp6m+06Tg4CrJ8bD52ewT92XILBQP3z90NJ3oROFTdgw/Fr+PPwEIzp72v2600a4I+xYVL85+hV5FU2m/16hNxLbYsab+7KQXSAC15PMn9feaiXBO88FYlT1xrw7ZlSs1/PllGSNxG1Vo9lu3LQ21OCt5+y3MzE54Z5wlPigCU7LqFdp7fYdQn5vRV7CqDS6PHvOYPuOcnJ1GYPDUZCXy+s2luIsoZWi1zTFlGSN5H/HL2KssZW/M/UGIsWFZOJBVgzfQAu17Rg4/ESi12XkNsyr9Thp5wqvDi6D/r4yB58gInweDysmR4LAZ+Ht/9LwyrvhZK8CVytVeLzzGL8aXAgRvbxsvj1x/T3xcQYP3z6SxE9iCIWpdbqsfzHPIR5SfD86DCLXz/QzQmvJ0XgeFE9DhbUWPz6toCSfA8xDIN3d+fBSSTAW5PYKyB1u4to1c+2XamQ2JYvMq+htKEVK/8Uw1q5gbTHeiHCV4r3fyqAWktdln9ESb6Hjl2pw4mrDVg8LgLeMjFrcQS5O+Pvo/vg59wqnLxaz1ocxH7UtqjxRVYxJg3ww8hwy3+DvU0k4OO9p6NR0dSGLzKvsRaHtaIk3wN6A4M1ewsR4uGMuSN6sR0O/pYYhkA3J6zZX0j9k8Ts/n24CBqdAW+M7892KBgZ7oWJMX74IqsY9cr2Bx9gRyjJ98D3FypwuaYFb0zoZ7ERBffjKBLg1XF9kVPRjP151WyHQzjsaq0S28+VY+6IXmaZ9GSMf4zvh3adAf+heSNdsJ+ZbJRaq8dHh65gYJArnhrgz3Y4naYNDkQfHyn+9+Blqr9NzGbt/kI4iQR4eUwftkPpFO4txYy4IHx7ugwVTTSk8jZK8kb69kwZqprVeHNipEmKj5mKUMDHP5IjcK1Ohe8vVLIdDuGg3IpmHCqowd8Sw+ApZe85VHdeHddRgvvfh4tYjsR6UJI3glqrxxeZxRgR5oHHwj3ZDucu46P9MDDIFf8+UkR1bYjJffJLEVwchZg/qjfbodwlwM0Jc0f0wq4LFSiuU7IdjlUwOskbDAYsX74cs2fPRlpaGkpL70wtVigUSEtL6/wzYMAAZGVl4ebNmxg+fHjn9i1btpikEZa283w5alva8coY61y4g8fjYfG4CFTebMOP2XQ3T0yn4MYtHCqowbOPh8HFRCWETe3vT4ZDJODj82PFbIdiFYyemnn48GFoNBpkZGQgOzsba9aswfr16wF0LMSdnp4OANi3bx98fHyQmJiIkydPYvLkyfjXv/5lmuhZoNEZsP5YMeJ7uVvlXfxto/t5I9LfBeszizEtLsik5Y6J/frsaBFkYuu8i7/NSyrGnKHB+PZMGRYnRSDQjIUCbYHRd/JyuRwJCQkAgEGDBiEvL++u17S2tuLTTz/F22+/DQDIy8tDfn4+5s6di1deeQW1tbZXD3rXhQrcaFbjlbF9raov/o94PB5efDIc1+pUNNKGmMSVmhbsza3GglG94epknXfxty1K7Jh9uyGLxs0bfSevVCohld5ZlFcgEECn00EovHPK7777DhMmTICHhwcAICwsDDExMRg5ciR2796NlStXdrvuq0Jh3KxNtVpt9LEPw8Aw+PRQOfp6iuGtq4NCwf6ko/u1ubeAQaCLCB/uz0OosMmqfyk9CnO/z9bIGtr80YlaiIU8jPLWWiSWnrb5yVAJtp0pxfggBm5OtrH4tzneZ6OTvFQqhUql6vzZYDB0SfAAsGfPni5JfMSIEXBy6vjqlJSUdM+FvSMjjSsPoFAojD72YRwqqMGNlhJ8mjIAUVEBZrvOo3hQm19tk+GN73JQJ/TGExHmq/FtSeZ+n60R222ubVEjs+Q6UoaFYPjgGItcs6dtXuYZjMP/l4kTdSL8Y3w/E0ZmPj1ps1wu73a70d01cXFxyMrKAgBkZ2cjIqJrDemWlhZoNBr4+98ZQ/7OO+/gwIEDAIBTp04hOjra2MuzYuPxawh0c8LEGD+2Q3loUwcFwlsmxle/UoVKYrz0U6XQGgxYMCqU7VAeWh8fKZIiffHtmVK7rmljdJJPSkqCg4MD5syZg9WrV+Of//wnNm/ejCNHjgAASkpKEBgY2OWYJUuWYNu2bUhLS8P27ds7++ptQW5FM86UNGL+yN4QCmxn5KmDkI+0Eb2QeaUOV2tpSBl5dG0aPb45XYqkSF+rmd36sBaMCkVTqxb/vWi/o8yM7q7h8/lYsWJFl23h4eGdf4+NjcW6deu67A8ODu4cdWNrNv16DRIHAWYPC2Y7lEeWOjwEn/1yFV+fLMHKqQPYDofYmF0XKtDUqsVfEyxfSrinRoR5oL+fDJtPXMfsocGceS71KGznlpRF1c1q/JRThdlDQ6x2bPD9eEnFeGZQAHbJK9HcqmU7HGJDDAYGX/1agtggVwzt7c52OI+Mx+Nh4ahQXK5pwaniBrbDYQUl+YfwzelSGBgGC6x4bPCDLBgVijatHhnny9gOhdiQX6/W41q9Cs8+Hmqzd8FPDwqAh8QBX524znYorKAk/wBavQHbz5VjTH8fBHs4sx2O0aICXDAizANbTpZCT6vbk4f0zelSeEocMMGGBhv8kaNIgNRhIThSWIPSBtWDD+AYSvIPcKigBvXKdqQOD2E7lB6bP7I3Km+24dhl25uERiyvqrkNRwprMXNIMGurPpnK3BG9wOfxsPWs/X2TpST/AN+eKUWgmxOeiPBhO5QeGxvpC2+ZGNvs8INOHl3GuXIYGAapw2z/BsfP1RFj+/vgu/MVdle0j5L8fVyrU+LE1QakDAvmRO0XkYCPWUOC8EthLaqaacFvcm86vQHbz5Yjsa83Qjxtt5vy91KGh6BBpcEhO1vwm5L8fWw7WwYhn4dZQ2xv2OS9zBkaAgMD7DhXwXYoxIodKaxF9S01/syBbsrbEvt6I9DNye6+yVKSvwe1Vo+d8gokR/vCx8WR7XBMJtjDGQl9vZBxrowewJJ7+vZMGfxdHTGmv+13U94m4PMwe2gwfr1ab1cPYCnJ38O+vCrcbNXiz8PZX6Db1FKHheBGsxqZV+gBLLlbeWMrsq7UYc7QEJua3f0wZg0JBp8HbD9XznYoFsOtd9CEdp6vQIiHMx4Ls96a8cYaF+ULL6kYW8/YzwedPLzv5BXg8YCZQ4LYDsXk/FwdMaa/L3aeL7ebB7CU5LtR0dSKk8UNmBEfBD4HHrj+0Z0HsDWouaVmOxxiRQwGBrsuVODxPl4I4OhiG6nDg1Gv1OCIwj4ewFKS78btBbCnxQU+4JW2a0Z8EAwM7LpwE7nbmZJGVDS1YUY89+7ib3siwge+LmLsumAfgw8oyf8BwzD4Tl6BkeGeCHLnxtCx7oR5SxEX4oZdFyrAMPQAlnTYKS+HTCzE+GjbneH6IAI+D1MHB+LY5TrUK9vZDsfsKMn/wbnrTShrbOX0ncxt0+KCcKVGifwbt9gOhVgBZbsO+3KrMXlgABxFtj3D9UGmDQ6CzsBgd/YNtkMxO0ryf/CdvBwSB4FN1+p4WFNiA+Ag4OM7uX18bSX3tze3Cm1avV3c4PTzkyEm0AXfX+T+Z5+S/O+0anT4OacKT8X6w9nB6FL7NsPVWYRxUT7YfekGtHr7GGlA7u07eQXCvCSIC3FjOxSLmB4XhLzKW7hc3cJ2KGZFSf539udVQ6XRY0Y8d2a4Psj0uCA0qjQ4drmO7VAIi0obVDhb0ojp8UE2W1L4UT09MABCPg/fc/wBrNG3qwaDAe+99x4uX74MBwcHrFy5Er163Zk4tHLlSly4cAESScdyYevWrYNWq8U//vEPqNVq+Pj4YPXq1Z0Le1uD7+QdY+NtcXEEYyVGeMNT4oDvL1QgKcqX7XAIS76/UAkej9sjyv7IUyrG6H4++OFiJZaO78e5iV+3Gd2qw4cPQ6PRICMjA0uWLMGaNWu67M/Pz8fGjRuRnp6O9PR0yGQyrFu3DpMnT8bWrVsRFRWFjIyMHjfAVKqb1Th1rQF/GhxoN3cyQMeY+WcGBeKIohY3WzVsh0NYwDAMdl+6gcfCPOHvaj03XZYwPS4QtS3tOMHhVaOMTvJyuRwJCQkAgEGDBiEvL69zn8FgQGlpKZYvX445c+bgu+++u+uYxMREnDx5siexm9RPOTfAMB2ryNibaXGB0OgN2JNTxXYohAW5lc0oqVfh6YH299kfE+kDVycRdnF48IHR3TVKpRJSqbTzZ4FAAJ1OB6FQiNbWVsydOxcLFiyAXq/HvHnzEBMTA6VSCZlMBgCQSCRoaen+gYdCoTAqJrVabfSxGacr0cfDAZr6cijqjToFK3rS5tv4DIMQVxEyTl7FENdWE0VmPqZos60xZ5u/PtcAIR8Ic7hlVf+ulnqfRwU74kB+FS7mOMBRxG6XjTnabHSSl0qlUKnuVHIzGAwQCjtO5+TkhHnz5nX2t48YMQKFhYWdxzg6OkKlUsHFxaXbc0dGRhoVk0KhMOrYknoVihqu4a1J/REZGW7UtdlibJv/aEaVCB8dugK3gN5W/5XdVG22JeZqs97A4MQPRzC6ny+GDYox+fl7wlLv8zyxD/ZeOY0Kxh1TItn9NtOTNsvl8m63G/1rKy4uDllZWQCA7OxsREREdO67fv06UlNTodfrodVqceHCBURHRyMuLg6ZmZkAgKysLMTHxxt7eZPac+kGeDxgih1+Xb3tdtt/pi4bu3K2pBE1t9rxjB12U942tLcHfF3E2H2JmxOjjL6TT0pKwokTJzBnzhwwDINVq1Zh8+bNCAkJwdixYzFlyhTMmjULIpEIzzzzDPr27YsXXngBy5Ytw44dO+Du7o4PP/zQlG0xCsMw+DG7EkN7e1j9Haw5hXpJMCDQFbsv3cBfE8LYDodYyO5LlXB2EGBcpP2OrBLweXhqQAC+OV2K5jYtXJ1EbIdkUkYneT6fjxUrVnTZFh5+p6tj0aJFWLRoUZf9Xl5e2LRpk7GXNIuCqlsorlNhwahQtkNh3ZSB/li1txDX61Xo7SVhOxxiZhqdAXtzq5Ec5QsnB26XMXiQKQP98dWJEhzMr8ZMDq0EB9BkKOzOvgEhn4dJA/zZDoV1k2M7vrLv4ejXVtJV1pU6NLdp8cwg+xkbfy+Dgt0Q7OHEyRFmdp3kDQYGey7dQEJfL3hIHNgOh3UBbk4Y2tsde3IoyduDHy/dgLuzCI/39WI7FNbxeDxMiQ3Aiav1aOBYZUq7TvLysibcaFbb5dj4e3l6YACu1ChRWE2VKbmsVaPD4YIaTBrgDxFHZ3o+qikDA6A3MNibV812KCZl1+/uT5duQCzkIymK+xUnH9bEAf4Q8HnUZcNxvxTWok2r7+yiI0B/Pxn6+kg599m32yRvMDDYl1eNJ/v5QCrmfsXJh+UlFWNkuCf2XKqixUQ4bF9uNbykYgwL9WA7FKvB4/EwZWAAzl1vRFVzG9vhmIzdJnl5WRNqW9oxcQDdxf/RlNgAlDW2Ireyme1QiBm0afT4pbAWE2J8IeDgGsY9MWVgABiGW/NF7DbJ782tgoOQjzH9fdgOxeokR/tCyOdhH8f6JkmHY5c7umomxdCIsj8K9ZIg0t+FU599u0zyBgOD/XnVSOzrDZkjtyY+mIKbswMeC/fEvlzqsuGivXnV8JA4UFfNPUyK8YO8tAnVzWq2QzEJu0zy2RU3UdWsxiTqqrmniTH+uN7QCkUVt1fNsTdqrR6/KGowPtqXs/XTe+p2F+6BfG7czdvlu7wvtwoiAQ/jaJGMe0qO9gWfB+zP407fJAEyr9RBpdHT5L/76OPTMcpmH0c++3aX5BmGwd7caiT09YYLddXck5dUjOGhnpwbM2zv9uVWwc1ZhBFhnmyHYtUmDvDH2ZJG1HNgYpTdJfmcimZU3mzDxBjqqnmQiQP8cLVWiaIa6rLhgnadHocVtUiO8qUJUA8wMcYPBgY4mF/Ddig9Znfv9N68Kgj5PFrP9CGMj/YDjwdOjTSwZ78W1UPZrsNE6qp5oP5+MoR6STjRZWNXSZ5hGOzLrcbIPl5wc6ZaNQ/i6+KIIb3csTfX9j/oBPg5twoujkKMCqdaNQ/C4/EwIcYPJ4sb0KSy7bWP7SrJ59+4hbLGVkyirpqHNiHGH4XVLSipVz34xcRqaXQGHCqoQVKUHxyEdvXf3miTYvyhNzA4pLDtLhu7erf35VVBwOchOZqS/MOa8NsvRC58bbVnJ4rr0aLW0bDhRxAT6IIgdyfst/HuSrtJ8rdH1TwW5kllhR9BoJsTBga7YV+ubX/Q7d2+3CrIxEIqK/wIeDweJsb44XhRHW6ptWyHYzSjkrzBYMDy5csxe/ZspKWlobS0tMv+r7/+GjNnzsTMmTPx2WefAehIsgkJCUhLS0NaWprFl/67UqNESb2q886UPLxJMX7IrWxGeWMr26EQI+j0HV01T/b3gVho3ytAPaoJMf7Q6hn8oqhlOxSjGVV+8fDhw9BoNMjIyEB2djbWrFmD9evXAwDKy8uxe/du7Ny5EzweD6mpqRg3bhycnJwQHR2Nzz//3KQNeFiHCjruRJNpVM0jmxjjj9X7CrE/rxqLEmn9V1sjL21CU6sW46mb8pENDnaDn4sj9uZWYepg21xBy6g7eblcjoSEBADAoEGDkJeX17nPz88PGzduhEAgAJ/Ph06ng1gsRn5+PmpqapCWloZFixbh2rVrpmnBQzpYUIPBIW7wcXG06HW5IMTTGf39ZDhUYNsPoOzVwYIaOAj4eKKfN9uh2Bw+n4fkaF9kFdWhTaNnOxyjGHUnr1QqIZVKO38WCATQ6XQQCoUQiUTw8PAAwzD44IMPEBUVhdDQUNTX1+Nvf/sbJk6ciPPnz2Pp0qXYtWtXt+dXKBRGNUatVnd7bJ1Kh5yKZiyI8zD63NbqXm02tThfAbbnNOLUhTy4ObH7ld9SbbYmxraZYRj8nF2OgX5ilF8rMkNk5mMt73N/aTvUWgO2H72IESHmXeDeHG02KslLpVKoVHeG1BkMBgiFd07V3t6Ot956CxKJBO+++y4AICYmBgJBR3IYMmQIampqwDAMeLy761lHRkYaExYUCkW3x547dR0AMPfJAQj3lt6135bdq82mlurSjK2XfkW53hWPRbK7mr2l2mxNjG1zYfUtVCtL8EpSJCIjQ8wQmflYy/vcJ8KANccPoeCWCAvMHE9P2iyXy7vdblR3TVxcHLKysgAA2dnZiIiI6NzHMAz+/ve/o1+/flixYkVnYv/ss8+wZcsWAEBhYSECAgK6TfDmcDC/BmHeEs4leEuKDnBBoJsTDhbQKBtbcjC/BjweMC6K1k0wlkjAx9hIXxxW1ECnN7AdziMz6k4+KSkJJ06cwJw5c8AwDFatWoXNmzcjJCQEBoMBZ8+ehUajwfHjxwEAr7/+Ov72t79h6dKlyMzMhEAgwOrVq03akHtpbtXi9LUG/DWBHhj2BI/X0Tf57ZkyqNp1kNCSiTbhYEE1Bge7wUdGz6J6IjnKFz9crMT50iabK+5m1P9UPp+PFStWdNkWHh7e+ffc3Nxuj/vyyy+NuVyPHL1cC52BQXI0jarpqeQoP2w+cR1ZV+qo/okNqLzZhrzKW3hzYn+2Q7F5iRHecBDycTC/xuaSPOcnQx0sqIa3TIxBQW5sh2LzhvZ2h5uzCAdplI1NOPzb+0TDhntOIhYioY8XDhZU29xqaZxO8mqtHpmX65AU5Qs+LVjcY0IBH2P7++KIogZaG+ybtDcHC6oR7i1BGD2LMonkaF9UNLXZ3GppnE7yp4oboNLo6U7GhJKjfXFLrcPZkka2QyH30fEsqpHqNJnQ2Ehf8HiwucEHnE7yBwuqIRUL8Vi4bfWhWbPEvt5wFPFxkCPrX3LVL5droDcwdINjQl5SMYb0cre5hUQ4m+T1BgaHCmowup831eswIScHARL6euNgQY3N9U3ak0MFNfCRiTGQnkWZVHKUHwqqbtlUHSfOJvns8ibUKzX0ddUMxkf7oapZjdzKZrZDId1Qa/U4Rs+izOL2inK2VOKDs0n+YH4NRAIeRlO9DpMb298HfB431r/kopPF9WjV6OkGxwx6e0nQz1dmU/3ynEzyDMPgQH41RoR5wsVRxHY4nOMuccCwUA+b+qDbk4P5NZCKhRgR5sF2KJyUHO2LsyWNNrMsICeT/NVaJa43tNKdjBklR/l11ugn1kNvYHBYQc+izCk5yg8GBjhsI8sCcjLJ356skxRJIwvM5U7fJN3NWxN6FmV+MYEu8Hd1tJlJgZxN8gOD3eDnSvU6zCXYwxlR/i7UL29l6FmU+fF4PCRH+eK4jdSY51ySr25W41L5TRofbAHJ0b6QlzWhrqWd7VAIOp5FHSyowWPhXvQsysySo/2g1hqQVVTHdigPxLkkf+i3frLxVJDM7JKj/MAwwBEb6ZvkuuK6jmckSXSDY3bDQj3g4ijEARuYFMi5JH8wvxphXlQ73hIi/WUIcneymb5JrjuQT8+iLOV2jflfCmutvsY8p5K8UqPHqeIGJEX7WmxBEnvW0Tfph1+v1kPZrmM7HLtHz6IsKznKFzdbtTh3vYntUO6LU0n+fEVbR+34KBpZYCnJ0b7Q6AzIumL9fZNcRs+iLO92jXlrn/1qdJI3GAxYvnw5Zs+ejbS0NJSWlnbZv2PHDkybNg2zZs3C0aNHAQCNjY1YuHAhUlNTsXjxYrS1tfUs+j84Va6Cl1SMwcFUr8NShvRyh7uzyOo/6FxHz6IsTyIW4nEbqDFvdJI/fPgwNBoNMjIysGTJEqxZs6ZzX11dHdLT07F9+3Zs2rQJH330ETQaDdatW4fJkydj69atiIqKQkZGhknDCgk/AAAc/ElEQVQaAQDtOj3OVbYiKcqH6nVYkFDAxxiqMc+6QwU1CKVnURaXHGX9NeaNTvJyuRwJCQkAgEGDBiEvL69zX05ODgYPHgwHBwfIZDKEhISgsLCwyzGJiYk4efJkD8O/41RxA9q01FXDBqoxz65bai1OFdcjOYqeRVmaLdSYNzrJK5VKSKV37hoEAgF0Ol3nPplM1rlPIpFAqVR22S6RSNDSYrrffrkVzZCI+FQ7ngVUY55dxy7XQaundYzZ4C0TIz7EumvMG7WQNwBIpVKoVHfqlhgMBgiFwm73qVQqyGSyzu2Ojo5QqVRwcXHp9twKheKR43nMy4DwJC+UXL3yyMfaMrVabdS/l6kN9nPE3pwKzO7LN/vdpLW02ZLu1+bvTtXA3VEAsbIaCg7NWbCV93mgFw+b5Ldw7FwOfKU9m4RmjjYbneTj4uJw9OhRTJo0CdnZ2YiIiOjcFxsbi48//hjt7e3QaDQoLi5GREQE4uLikJmZiWnTpiErKwvx8fHdnjsyMtKomBQKhdHH2iprafM0lRRLv8uBwTUQMYGuZr2WtbTZku7V5nadHhe2l2HKwADEREexEJn52Mr7PNdbhU3yYyjRyDA6MrRH5+pJm+VyebfbjU7ySUlJOHHiBObMmQOGYbBq1Sps3rwZISEhGDt2LNLS0pCamgqGYfDaa69BLBbjhRdewLJly7Bjxw64u7vjww8/NPbyxMqMjfT9rcZ8tdmTPLnj9LVGKNt1NMuVRaFeEvT1keJgfg0WjOpZkjcHo5M8n8/HihUrumwLDw/v/PusWbMwa9asLvu9vLywadMmYy9JrJiHxAFDenvgYEENXk/ux3Y4duNgfjWcHQQYGe7Fdih2LTnaF59nXsPNVg3cnB3YDqcLTk2GIuxKjvJFYXULShuoxrwlGH63jrGjiGrHsykpyg96A4NfCmvZDuUulOSJydwevkoToywjp7IZtS3tNGzYCsQGusLXRWyVo2woyROTCfF0Rn8/GRUss5CD+dUQ8Hl4sp8P26HYPT6fh6QoX2ReqYNaa1015inJE5NKjvbD+euNaFBSjXlzO1hQgxFhHnB1ptrx1iA5yg9tWj1+LapnO5QuKMkTk0qO8oWBAY5YYd8klxTXKXG1VkldNVZkRJgnZGKh1c1+pSRPTCo6wAWBbk5W2TfJJbefe9DQSevhIORjdH8fHFHUQm+wnoJllOSJSfF4HX2Tx4vq0KqhGvPm0jEfwQUBbk5sh0J+JznKFw0qDS6UWU+NeUryxOSSo3zRrjPguJX1TXJFbYsaF8tvUleNFRrdzxsiAc+q6jhRkicmNzTUA65OIuqyMZMjilowDKggmRWSOYowMtwLBwtqrKbGPCV5YnIiAR9j+/vgSGGN1a9/aYsO5lcjxMMZ/XxlD34xsbjkaF+UNrSiqFbJdigAKMkTM0mOto31L22Nsl2HE1cbqHa8FRv320Lq1tJlQ0memEVC3471L61tOJmty7xcB43eQKNqrJiviyMGBbtZzaRASvLELCRiIRL6eOGQFfVNcsGhgmp4SBwQ38ud7VDIfSRH+yKnohlVzaZdx9oYlOSJ2SRHW//6l7ZEqzfgSGEtxvb3gVBA/3Wt2e2RT4et4G6ePinEbGxh/UtbcuZaI1rUOiRH09BJa9fHR4owL4lVdNlQkidm4yUVY0gv617/0pYcLKiGo4iPx/tQ7XhbkBTti1PFDWhu07IaByV5YlbJUX4oqLqF8sZWtkOxaQzTUTs+sa83nByodrwtSI7yg87A4Nhldus4GZXk1Wo1Xn75ZaSmpmLRokVobGy86zVr167F7NmzMX36dOzYsQMAcPPmTQwfPhxpaWlIS0vDli1behY9sXq3R4Ec5tAC02y42qhBVbOaumpsyOBgN3hJ2a8xb1SS37ZtGyIiIrB161ZMnToV69at67L/9OnTKCsrQ0ZGBrZt24YNGzagubkZBQUFmDx5MtLT05Geno6//OUvJmkEsV69vSSI8JWy/kG3dafKVODzgLH9qXa8rbhdY/7Y5Vq069irMW9UkpfL5UhISAAAJCYm4tSpU132Dx48GKtWrer8Wa/XQygUIi8vD/n5+Zg7dy5eeeUV1NZSOVp7kBzlh7PXG9Gk0rAdis06VabC0N4ecJdY1/qh5P6So3yh0uhxsriBtRgeuJD3zp077+pW8fT0hEzWMaVaIpGgpaXrEDmxWAyxWAytVos333wTs2fPhkQiQVhYGGJiYjBy5Ejs3r0bK1euxCeffHLXNRUKhVGNUavVRh9rq2yhzX2d26E3MPjmaDbGhfd8Kr4ttNmUKpo1uH5TiwkRPLtqNxfeZw+9AU5CHnacKISf4cGJ3hxtfmCSnzlzJmbOnNll20svvQSVqmOxZpVKBRcXl7uOa25uxiuvvIJhw4bhueeeAwCMGDECTk4dpVGTkpK6TfAAEBkZ+Wit+I1CoTD6WFtlC23uzzBYfbwBeU18vGyCWG2hzab0y9GrAIC/jB0If1f7KS3Mlfd5TGQ7zl5vRL9+/cHn378URU/aLJfLu91uVHdNXFwcMjMzAQBZWVmIj4/vsl+tVmP+/PmYPn06Xnzxxc7t77zzDg4cOAAAOHXqFKKjo425PLExPB4PydG+yLpSb3XrX9qCfXlV6OcltqsEzyXJ0b6oa2lHdsVNVq5vVJJPSUlBUVERUlJSkJGRgZdeegkA8MEHHyAnJwfbt29HeXk5du7c2TmSpry8HEuWLMG2bduQlpaG7du34+233zZpY4j1Sorytcr1L61deWMr8ipv4fFeErZDIUYa3c8HQj6PtcEHD+yu6Y6Tk1O3XS1vvPEGACA2Nhbz58/v9tj09HRjLkls3PBQT8gchTiQX41xVFzroe3P65gtPIqSvM1ydRLhsXBP7M+rwrIJ/SxePZQmQxGLcBB21Jg/pKiBlmrMP7R9eVWIDnCBv0zEdiikBybG+ON6QysrdZwoyROLmTTAHzdbtTh9jb3hZLakqrkNF8puYmIMTYCydcnRvuDzgL25VRa/NiV5YjGJEd6QOAhY+aDbogO/ddVMiPFnORLSU15SMUaEeWJvbpXFS29TkicW4ygSYGykLw7k07KAD2NfXjX6+kjRx0fKdijEBCYO8Me1ehUu11i2y4aSPLGoSQP80ajS4PS1u+sdkTvqle04d72Rumo4ZEK0X0eXTY5lv8lSkicWNbqfN5wdBNibR10293MwvwYGpuPuj3CDt0yMYaEe2Jtn2fUVKMkTi3IUCTCmvw8O5FVTl8197MurQm9PZ/T363kZCGI9Jg3wx9VaJa5YsMuGkjyxuKcG+KNBpcHZEuqy6c7NVg1OFTdgQoy/xcdUE/OaEOMHnoVH2VCSJxY3up8PnETUZXMvhwpqoDMw1B/PQT4yRwzt7UFJnnCbk0NHl83+vBroDZYdTmYLfsqpQpC7E2KDXNkOhZjBpBg/XKlR4mqtZbpsKMkTVkwa4I96ZTt12fxBo0qDX6/WY8rAAOqq4ajbD9P35lrmASwlecKKJ/t7w1HEp4lRf7Avrwp6A4MpsQFsh0LMxNfFEUN6uVvss09JnrDC2UGIMf19sC+vmrpsfmfPpRsI95Yg0p9G1XDZpAH+KKxuQXGd0uzXoiRPWDM5NgD1ynacoVo2AICaW2qcKWmkrho7MGmAP3i8jl/q5kZJnrBmTH8fSMVC/Jht/g+6Lfg5pwoM0/HLj3Cbn6sjhod6YHf2DbPXsqEkT1jjKBJgfLQf9uZVsbqavbXYk3MDUf4uVKvGTjwzKBDX6lXIq7xl1utQkiesenpQAFrUOhy7XMd2KKwqb2zFxbKbmDyQyhjYi4kxfhAJePgxu9Ks1zFqZSi1Wo2lS5eioaEBEokEa9euhYeHR5fXPP/887h58yZEIhHEYjE2btyI0tJSvPnmm+DxeOjbty/effdd8Pn0e8aejQr3hKfEAbuzb2B8tP1O/vn5t5EWNKrGfrg5O+CJCB/sybmBf06KhOABi3wby6gMu23bNkRERGDr1q2YOnUq1q1bd9drysrKsG3bNqSnp2Pjxo0AgNWrV2Px4sXYunUrGIbBkSNHehY9sXlCAR+TY/1xWFGDFrWW7XBYs+fSDQwKdkOwhzPboRALemZQAGputeNMifkGHxiV5OVyORISEgAAiYmJOHXqVJf99fX1uHXrFp5//nmkpKTg6NGjAID8/HwMGzas87iTJ0/2JHbCEU8PCkS7zsDaQsdsu1rbgvwbtzBlIN3F25txkb6QOAiw24yDDx7YXbNz505s2bKlyzZPT0/IZB3jeCUSCVpauk7P1Wq1WLhwIebNm4fm5makpKQgNjYWDMN0Dg3r7rjbFAqFUY1Rq9VGH2uruNBmJ4aBr1SIrSeuIMr5wVO9udDm3/v6QiP4PKC/s/Ke7eJamx+GvbR5RJATfrpUiZR+Qhi07SZv8wOT/MyZMzFz5swu21566SWoVCoAgEqlgouLS5f9Xl5emDNnDoRCITw9PREZGYmSkpIu/e/dHXdbZGTkIzcE6PjlYOyxtoorbZ5RzsfnmdfgFRQGb5n4vq/lSpsBwGBgcPy/vyAxwhuj4gbc83VcavPDspc2p/E9cWTzOVTzPBDi2GR0m+VyebfbjequiYuLQ2ZmJgAgKysL8fHxXfafPHkSixcvBtCRzIuKihAWFoaoqCicOXOm87ghQ4YYc3nCQc8MCoTewODnHPsaM3+6pAE3mtWYFhfEdiiEJaP6eHUOPjAHo5J8SkoKioqKkJKSgoyMDLz00ksAgA8++AA5OTl44okn0KtXL8yaNQvPPvssXn/9dXh4eGDZsmX49NNPMXv2bGi1WowfP96kjSG2K8JXhkh/F/xw0bzDyazN9xcqIRMLkRzly3YohCUiAR9P/Tb4QKUx/UI6Rg2hdHJywieffHLX9jfeeKPz72+//fZd+0NDQ/HNN98Yc0liB6bHBWLlzwpcqWlBhC/3a7e0anTYl1uFybEBcBQJ2A6HsGj20GD8cKESTW06k5+bBqkTq/GnwYEQ8nnYeb6c7VAs4mB+DVQaPabFBbIdCmFZdIArLixPQpCrg8nPTUmeWA1PqRhj+vvgh4s3oLWD9V93XahAoJsThvb2ePCLCeeJBOZJx5TkiVWZOSQY9cp2ZHK8zEHNLTVOXK3HtLhA8M0005EQgJI8sTKj+3nDS+qAnXJud9n8cLESBqaji4oQc6IkT6yKSMDH1EGBOKKoRYOyne1wzIJhGGScK8fQ3u4I86aKk8S8KMkTqzNzSDB0BoazdeZPX2tESb0Kc4aGsB0KsQOU5InV6ecnQ2yQK3bKK8y+oAIbtp8rg4ujEE/FUllhYn6U5IlVmjkkGIqqW7hU0cx2KCbVpNJgX241/jQ4kMbGE4ugJE+s0tRBAXB2EOCb06Vsh2JS31+shEZvwJxh1FVDLIOSPLFKMkcRpg4OxJ5LN3CzVcN2OCbBMAy2ny3DwGA3RPp3X5yPEFOjJE+s1tzhvdCuM+A7eQXboZjEhbImFNUqkTosmO1QiB2hJE+sVlSAC+J7uePbM2UwGGz/Aew3p8sgFQsxmZb4IxZESZ5YtbkjQlBSr8LJYvMtj2YJtS1q/JRzAzPigyARG1UXkBCjUJInVm1ijD/cnUU2/wD229Nl0BkYzB/Zm+1QiJ2hJE+smqNIgFlDgnFIUYPKm21sh2OUdp0e354pw5P9fNDbS8J2OMTOUJInVm/eb3e/X58oYTcQI+3NrUK9sp3u4gkrKMkTqxfo5oRJA/yx/Ww5WtRatsN5JAzDYPOJ6wj3liChrxfb4RA7ZNQTILVajaVLl6KhoQESiQRr166Fh8edmthZWVnYsGEDgI4PuVwux08//QS1Wo3nn38evXv3BtCxjOCkSZN63grCeYsSQrHn0g1knCvHKBvKlfLSJuRUNOP9Z6LB41FJYWJ5RiX5bdu2ISIiAi+//DJ+/vlnrFu3Du+8807n/sTERCQmJgIANm7ciLi4OISHh2Pnzp1YsGABFi5caJroid2IDXLD8FAPfPVrCYY/bTs1X9YfK4a7swjT42mhbsIOo7pr5HI5EhISAHQk9FOnTnX7uurqavz444+dC33n5eXh2LFj+POf/4y33noLSqXSyLCJPVqUEIYbzWr8el3FdigPpbD6Fo4U1mL+yFA4O9CwScKOB37ydu7ciS1btnTZ5unpCZmsY6FliUSClpaWbo/dvHkz5s+fDweHjnULY2NjMXPmTMTExGD9+vX4z3/+g2XLlt11nEKheOSGAB3dSMYea6vsqc3+YBDkIsLO3CY8EVpg9d0fHxyvhaOQh8e82nv8HtnT+3wbtdlEGCO8+OKLzKVLlxiGYZhbt24xTz311F2v0ev1THJyMtPW1ta5rbm5ufPvRUVFzLx58+467vz588aExDAMwxQUFBh9rK2ytzbvOFfG9Fr2E3Mov5rtUO6rrEHFhP3zZ+b9PfkmOZ+9vc8MQ21+VPfKnUZ118TFxSEzMxNAx0PW+Pj4u15z5coVhIaGwtHRsXPbs88+i5ycHADAqVOnEB0dbczliR370+BA+EmF+PjIFauuNf9FVjH4POCvCWFsh0LsnFFJPiUlBUVFRUhJSUFGRkZnn/sHH3zQmcRLSkoQHNy1ENN7772HVatWIS0tDRcuXMDf//73HoZP7I1QwMecWDfkVd7CL4W1bIfTrfLGVmScK8fMIcHwc3V88AGEmJFRT4OcnJzwySef3LX9jTfe6Pz7xIkTMXHixC77o6OjsX37dmMuSUinseEy7CpU4d9HijCmv4/V9c3/+0gReDweXh7Th+1QCKHJUMT2CPk8vPRkH+RUNONQQQ3b4XRxtbYF31+oQNqIXvB3dWI7HEIoyRPbNC0uCOHeEqzZXwit3sB2OJ3+71ARHEUCvDA6nO1QCAFASZ7YKJGAj39OjMS1OhW2ny1jOxwAQHb5TfycW4WFo0LhJRWzHQ4hACjJExs2NtIHj4V54v8OF+EWyzVtDAYG7+3Oh5dUjOeeoBE1xHpQkic2i8fj4e2nItGo0uA/R6+yGssPFyuRXX4Tyyb0g8xRxGoshPweJXli02ICXTEzPgibjpfgcnX3M6/NTdmuw9r9hRgY7IbpcVSjhlgXSvLE5r01KRIuTiK89UMuK2vBrt1XiDplO96bEgU+37qGcxJCSZ7YPHeJA96eFAl5aRO2Wvgh7OlrDUg/XYoFI0MxOMTdotcm5GFQkiecMC0uEKP6eGL1XgWu11umSmWbRo9lu3IQ4uGMf4yPsMg1CXlUlOQJJ/B4PPzvjIEQ8Hl4dftFi4ydf//nApQ2tGLN9AFUSphYLUryhDMC3JywZnosLlU046NDV8x6rR+zK7H1TBmefyIcI8NtaKkqYncoyRNOmTTAH3OGBmP9sWLsy60yyzWu1LTgre9zMaSXO5YkUzcNsW6U5AnnvPd0NAaHuOG1HdnIq2w26blrb6mxYPM5SMRCfJo6GCIB/Rci1o0+oYRzHEUCfJk2BJ4SMeZvPoertaZZZrK5VYsFX59DU6sGX80fSgXIiE2gJE84yVsmxpaFwwAAKRtO9zjRN6o0SN14GkU1SvwnNQ4xga6mCJMQs6MkTzirj48U2xYNB8MwmL7+JH4tqjfqPNfqlJj1xSlcrVXiy3nxeLK/j4kjJcR8epTkDx06hCVLlnS7b8eOHZg2bRpmzZqFo0ePAgAaGxuxcOFCpKamYvHixWhra+vJ5Ql5oL6+Mnz/wij4uojxl81n8cmRooceXskwDH64WIFnPjuBRpUGWxYOw+h+lOCJbTE6ya9cuRIffvghDIa7/8PU1dUhPT0d27dvx6ZNm/DRRx9Bo9Fg3bp1mDx5MrZu3YqoqChkZGT0KHhCHkaIpzN2vTASTw3wx0eHruCpT47j55wq6O9RAoFhGJy51oA/bzyD1zIuoY+vFLtfGoURYZ4WjpyQnjN6BkdcXBzGjRvXbaLOycnB4MGD4eDgAAcHB4SEhKCwsBByuRzPPfccACAxMREfffQR5s+fb3TwhDwsmaMIn6QMxpSBAVi9T4EXt16Al9QBT/bzQaS/C1ydRGjV6lFcq8Sxy7W43tAKL6kDVjwTjT8P7wUB1aQhNuqBSX7nzp3YsmVLl22rVq3CpEmTcObMmW6PUSqVkMlknT9LJBIolcou2yUSCVpauq8aqFAoHroBv6dWq40+1lZRmx9NEA/4ZIIPTpe3IrNEif15N7BTXtG5XyzkIdrHEU8/5oUxYVKIhW24crnQVKEbjd5n+2CONj8wyc+cORMzZ858pJNKpVKoVHfqh6hUKshkss7tjo6OUKlUcHFx6fb4yMjIR7rebQqFwuhjbRW12Tgx0cBf0dE1U6/UoE2jh0jIg6/M0SorSdL7bB960ma5XN7tdrOMromNjYVcLkd7eztaWlpQXFyMiIgIxMXFITMzEwCQlZWF+Ph4c1yekIfG4/HgLRMjxNMZ/q5OVpngCekJk1ZV2rx5M0JCQjB27FikpaUhNTUVDMPgtddeg1gsxgsvvIBly5Zhx44dcHd3x4cffmjKyxNCCPmDHiX54cOHY/jw4Z0/L1iwoPPvs2bNwqxZs7q83svLC5s2berJJQkhhDwCmgxFCCEcRkmeEEI4jJI8IYRwGCV5QgjhMEryhBDCYTyGYbov4MGSew3oJ4QQcn/dzT2yuiRPCCHEdKi7hhBCOIySPCGEcBgnkrzBYMDy5csxe/ZspKWlobS0lO2QzE6r1WLp0qVITU3FjBkzcOTIEbZDsoiGhgY88cQTKC4uZjsUi/jiiy8we/ZsTJs2DTt37mQ7HLPTarVYsmQJ5syZg9TUVM6/z5cuXUJaWhoAoLS0FCkpKUhNTcW7777b7VodxuBEkj98+DA0Gg0yMjKwZMkSrFmzhu2QzG737t1wc3PD1q1bsWHDBrz//vtsh2R2Wq0Wy5cvh6OjI9uhWMSZM2dw8eJFbNu2Denp6aiurmY7JLPLzMyETqfD9u3b8eKLL+Ljjz9mOySz2bBhA9555x20t7cDAFavXo3Fixdj69atYBjGZDdunEjycrkcCQkJAIBBgwYhLy+P5YjMb8KECXj11Vc7fxYIBCxGYxlr167FnDlz4ONjH0vw/frrr4iIiMCLL76I559/HqNHj2Y7JLMLDQ2FXq+HwWCAUqmEUGjSGopWJSQkBJ9++mnnz/n5+Rg2rGPx+cTERJw8edIk1+HEv6BSqYRUKu38WSAQQKfTcfoDIpFIAHS0/ZVXXsHixYtZjsi8vv/+e3h4eCAhIQFffvkl2+FYRFNTE27cuIHPP/8cFRUVeOGFF7B//37weNwth+zs7IzKykpMnDgRTU1N+Pzzz9kOyWzGjx+Pioo7C9YwDNP53t5vUaVHxYk7+T8uUmIwGDid4G+rqqrCvHnz8Mwzz2DKlClsh2NWu3btwsmTJ5GWlgaFQoFly5ahrq6O7bDMys3NDY8//jgcHBwQFhYGsViMxsZGtsMyq6+//hqPP/44Dhw4gB9//BFvvvlmZ3cG1/H5d9Lx/RZVeuTzmuQsLIuLi0NWVhYAIDs7GxERESxHZH719fVYuHAhli5dihkzZrAdjtl9++23+Oabb5Ceno7IyEisXbsW3t7ebIdlVvHx8Th+/DgYhkFNTQ3a2trg5ubGdlhm5eLi0rlEqKurK3Q6HfR6PctRWUZUVFTnkqpZWVkYMmSISc7LidvdpKQknDhxAnPmzAHDMFi1ahXbIZnd559/jlu3bmHdunVYt24dgI4HOfbyUNIePPnkkzh37hxmzJgBhmGwfPlyzj97mT9/Pt566y2kpqZCq9Xitddeg7OzM9thWcSyZcvwr3/9Cx999BHCwsIwfvx4k5yXZrwSQgiHcaK7hhBCSPcoyRNCCIdRkieEEA6jJE8IIRxGSZ4QQjiMkjwhhHAYJXlCCOEwSvKEEMJh/x8kitAopYmbTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "x = np.linspace(0, 10, 1000)\n",
    "ax.plot(x, np.sin(x))\n",
    "ax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
