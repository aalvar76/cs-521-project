{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "CNN model for text classification\n",
    "This implementation is based on the original paper of Yoon Kim [1].\n",
    "# References\n",
    "- [1] [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)\n",
    "@author: Christopher Masch\n",
    "\"\"\"\n",
    "\n",
    "from keras.layers import Activation, Input, Dense, Dropout, Embedding\n",
    "from keras.layers.convolutional import SeparableConv1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "\n",
    "class CNN:\n",
    "    \n",
    "    __version__ = '0.0.2'\n",
    "    \n",
    "    def __init__(self, embedding_layer=None, num_words=None, embedding_dim=None,\n",
    "                 max_seq_length=100, filter_sizes=[3,4,5], feature_maps=[100,100,100],\n",
    "                 hidden_units=100, dropout_rate=None, nb_classes=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            embedding_layer : If not defined with pre-trained embeddings it will be created from scratch (default: None)\n",
    "            num_words       : Maximal amount of words in the vocabulary (default: None)\n",
    "            embedding_dim   : Dimension of word representation (default: None)\n",
    "            max_seq_length  : Max length of sequence (default: 100)\n",
    "            filter_sizes    : An array of filter sizes per channel (default: [3,4,5])\n",
    "            feature_maps    : Defines the feature maps per channel (default: [100,100,100])\n",
    "            hidden_units    : Hidden units per convolution channel (default: 100)\n",
    "            dropout_rate    : If defined, dropout will be added after embedding layer & concatenation (default: None)\n",
    "            nb_classes      : Number of classes which can be predicted\n",
    "        \"\"\"\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.num_words       = num_words\n",
    "        self.max_seq_length  = max_seq_length\n",
    "        self.embedding_dim   = embedding_dim\n",
    "        self.filter_sizes    = filter_sizes\n",
    "        self.feature_maps    = feature_maps\n",
    "        self.hidden_units    = hidden_units\n",
    "        self.dropout_rate    = dropout_rate\n",
    "        self.nb_classes      = nb_classes\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build the model\n",
    "        \n",
    "        Returns:\n",
    "            Model           : Keras model instance\n",
    "        \"\"\"\n",
    "\n",
    "        # Checks\n",
    "        if len(self.filter_sizes)!=len(self.feature_maps):\n",
    "            raise Exception('Please define `filter_sizes` and `feature_maps` with the same length.')\n",
    "        if not self.embedding_layer and (not self.num_words or not self.embedding_dim):\n",
    "            raise Exception('Please define `num_words` and `embedding_dim` if you not use a pre-trained embeddings')\n",
    "        \n",
    "        \n",
    "        # Building embeddings from scratch\n",
    "        if self.embedding_layer is None:\n",
    "            self.embedding_layer = Embedding(\n",
    "                input_dim=self.num_words, \n",
    "                output_dim=self.embedding_dim,       \n",
    "                input_length=self.max_seq_length,\n",
    "                weights=None, trainable=True,\n",
    "                name=\"word_embedding\"\n",
    "            )\n",
    "        \n",
    "        word_input = Input(shape=(self.max_seq_length,), dtype='int32', name='word_input')\n",
    "        x = self.embedding_layer(word_input)\n",
    "        x = Dropout(self.dropout_rate)(x)\n",
    "        x = self.building_block(x, self.filter_sizes, self.feature_maps)\n",
    "        x = Activation('relu')(x)\n",
    "        prediction = Dense(self.nb_classes, activation='softmax')(x)\n",
    "        return Model(inputs=word_input, outputs=prediction)\n",
    "    \n",
    "    \n",
    "    def building_block(self, input_layer, filter_sizes, feature_maps):\n",
    "        \"\"\" \n",
    "        Creates several CNN channels in parallel and concatenate them \n",
    "        \n",
    "        Arguments:\n",
    "            input_layer : Layer which will be the input for all convolutional blocks\n",
    "            filter_sizes: Array of filter sizes\n",
    "            feature_maps: Array of feature maps\n",
    "            \n",
    "        Returns:\n",
    "            x           : Building block with one or several channels\n",
    "        \"\"\"\n",
    "        channels = []\n",
    "        for ix in range(len(self.filter_sizes)):\n",
    "            x = self.create_channel(input_layer, filter_sizes[ix], feature_maps[ix])\n",
    "            channels.append(x)\n",
    "            \n",
    "        # Checks how many channels, one channel doesn't need a concatenation\n",
    "        if (len(channels)>1):\n",
    "            x = concatenate(channels)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def create_channel(self, x, filter_size, feature_map):\n",
    "        \"\"\"\n",
    "        Creates a layer, working channel wise\n",
    "        \n",
    "        Arguments:\n",
    "            x           : Input for convoltuional channel\n",
    "            filter_size : Filter size for creating Conv1D\n",
    "            feature_map : Feature map \n",
    "            \n",
    "        Returns:\n",
    "            x           : Channel including (Conv1D + GlobalMaxPooling + Dense + Dropout)\n",
    "        \"\"\"\n",
    "        x = SeparableConv1D(feature_map, kernel_size=filter_size, activation='relu', strides=1, padding='same',\n",
    "                            depth_multiplier=4)(x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "        x = Dense(self.hidden_units)(x)\n",
    "        x = Dropout(self.dropout_rate)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
