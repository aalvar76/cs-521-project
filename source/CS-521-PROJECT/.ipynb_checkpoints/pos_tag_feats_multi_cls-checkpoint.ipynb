{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'train.tsv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-aed7db27c6bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Importing the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.tsv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m dataset.columns=['statement_ID','label','statement','subject','speaker','job_title',\n\u001b[0;32m     32\u001b[0m            \u001b[1;34m'state_info'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'pantry_affiliation'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'barely_true_cnt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'false_cnt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'train.tsv' does not exist"
     ]
    }
   ],
   "source": [
    "# pos tagging for fake news statement\n",
    "## use pos-tagging to build features\n",
    "\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "## set up stanfordcorenlp server\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "# Importing the dataset\n",
    "dataset=pd.read_csv('train.tsv',delimiter='\\t',encoding='utf-8')\n",
    "dataset.columns=['statement_ID','label','statement','subject','speaker','job_title',\n",
    "           'state_info','pantry_affiliation','barely_true_cnt','false_cnt',\n",
    "           'half_true_cnt','mostly_true_cnt','pants_on_fire_cnt','context']\n",
    "\n",
    "\n",
    "dataset.head()\n",
    "\n",
    "total_rows = len(dataset.index)\n",
    "cols = list(dataset.columns.values)\n",
    "\n",
    "# Cleaning the texts\n",
    "corpus = []\n",
    "for i in range(0, total_rows):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['statement'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "train_data=corpus\n",
    "## define key-value\n",
    "data_dict=dict()\n",
    "\n",
    "data_dict['ids'] = train_data[['ID']].values[:,0]\n",
    "data_dict['labels'] = train_data[['Label']].values[:,0]\n",
    "data_dict['statements'] = train_data[['Statement']].values[:,0]\n",
    "data_dict['subjects'] = train_data[['Subject']].values[:,0]\n",
    "data_dict['contexts'] = train_data[['Context']].values[:,0]\n",
    "# Process each sentence\n",
    "# Obtain the Part Of Speech for every word in the statement\n",
    "# Get POS bigram way\n",
    "# Get POS trigram way\n",
    "\n",
    "word_pos_list = list()\n",
    "pos_list = list()\n",
    "unigram_pos_list=list()\n",
    "bigram_pos_list = list()\n",
    "trigram_pos_list = list()\n",
    "\n",
    "for sent_id, txt in enumerate(data_dict['text']):\n",
    "    print('Procesing sentence number {0}: {1}'.format(sent_id,txt))\n",
    "    output = nlp.annotate(txt, properties={\n",
    "      'annotators': 'tokenize,pos,parse',\n",
    "      'outputFormat': 'json'\n",
    "      })\n",
    "    result_word_pos = str()\n",
    "    result_pos = list()\n",
    "    unigram_pos=list()\n",
    "    bigram_pos = list()\n",
    "    trigram_pos = list()\n",
    "    for o in output['sentences']:\n",
    "        result_pos.append('<s>')\n",
    "        for t in o['tokens']:\n",
    "            result_word_pos += '{0}/{1} '.format(t['word'],t['pos'])\n",
    "            result_pos.append('{0}'.format(t['pos']))\n",
    "        for rpIndex, rp in enumerate(result_pos):\n",
    "            if rpIndex==len(result_pos):\n",
    "                unigram_pos.append('{0}').format(rp, result_pos[rpIndex+1])\n",
    "            if rpIndex < len(result_pos)-1 and len(result_pos)>=2:\n",
    "                bigram_pos.append('{0} {1}'.format(rp, result_pos[rpIndex+1]))\n",
    "            if rpIndex < len(result_pos)-2 and len(result_pos)>=3:\n",
    "                trigram_pos.append('{0} {1} {2}'.format(rp, result_pos[rpIndex+1], result_pos[rpIndex+2]))\n",
    "\n",
    "    word_pos_list.append(result_word_pos)\n",
    "    pos_list.append(result_pos)\n",
    "    unigram_pos_list.append(unigram_pos)\n",
    "    bigram_pos_list.append(bigram_pos)\n",
    "    trigram_pos_list.append(trigram_pos)\n",
    "\n",
    "##\n",
    "data_dict['word_pos'] = word_pos_list\n",
    "data_dict['pos'] = pos_list\n",
    "data_dict['unigram'] = unigram_pos_list\n",
    "data_dict['bigrams'] = bigram_pos_list\n",
    "data_dict['trigrams'] = trigram_pos_list\n",
    "\n",
    "##\n",
    "dataset['Word POS'] = data_dict['word_pos']\n",
    "dataset['POS'] = data_dict['pos']\n",
    "dataset['unigram_pos'] = data_dict['unigram']\n",
    "dataset['bigrams_pos'] = data_dict['bigrams']\n",
    "dataset['trigram_pos'] = data_dict['trigrams']\n",
    "\n",
    "## use countVectorizor to build postag output as features for training ML model\n",
    "cv = CountVectorizer()\n",
    "\n",
    "pos_uni_feats = cv.fit_transform(dataset['unigram_pos']).toarray()\n",
    "pos_big_feats = cv.fit_transform(dataset['bigram_pos']).toarray()\n",
    "pos_trig_feats = cv.fit_transform(dataset['trigram_pos']).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(pos_uni_feats,\n",
    "        dataset['class'],train_size=0.8, random_state=123)\n",
    "\n",
    "## train bigram_pos features on logistic regression\n",
    "LR = LogisticRegression()\n",
    "LR = LR.fit(X=X_train, y=y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "print(\"accuracy metrics for logistic regression classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "### train features on multinomial naive bayes classifier\n",
    "nb_cly=OneVsRestClassifier(MultinomialNB()).fit(X=X_train, y=y_train)\n",
    "y_pred=nb_cly.predict(X_test)\n",
    "print(\"accuracy metrics for Multinomial naive bayes classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "### train features on SVM\n",
    "\n",
    "svm_clf=SVC(C=1, kernel='rbf', degree=3, gamma='auto', random_state=None)\n",
    "svm_clf=svm_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=svm_clf.predict(X_test)\n",
    "print(\"accuracy metrics for support vector machine classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "### train features on SGD\n",
    "sgd_clf = linear_model.SGDClassifier(max_iter=1000)\n",
    "sgd_clf=sgd_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=sgd_clf.predict(X_test)\n",
    "print(\"accuracy metrics for stochastic gardient desent classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "### train features on random forest\n",
    "randFor_clf=RandomForestClassifier(n_estimators=2, criterion='gini', max_features='auto', class_weight={1:.9, 2:.5, 3:.01})\n",
    "randFor_clf=randFor_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=randFor_clf.predict(X_test)\n",
    "print(\"accuracy metrics for random forest classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "##----------- More compact solution for training several emprical ML classfier model -----------##\n",
    "### use kfold cross validation\n",
    "## Kforld cross validation on training set\n",
    "kf = KFold(len(X_train), numFolds=10, shuffle=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(pos_uni_feats,\n",
    "        dataset['class'],train_size=0.8, random_state=123)\n",
    "Y=dataset['class']\n",
    "\n",
    "params = [{}, {'loss': 'log', 'penalty': 'l2', 'n_iter':1000},class_weight={1:.9, 2:.5, 3:.01}]\n",
    "Models = [LogisticRegression, SGDClassifier, MultinomialNB, RandomForestClassifier, SVC]\n",
    "\n",
    "for param, Model in zip(params, Models):\n",
    "    total = 0\n",
    "    for train_indices, test_indices in kf:\n",
    "        train_X = X_train[train_indices, :]; train_Y = y_train[train_indices]\n",
    "        test_X = X_test[test_indices, :]; test_Y = y_test[test_indices]\n",
    "        reg = Model(**param)\n",
    "        reg.fit(train_X, train_Y)\n",
    "        predictions = reg.predict(test_X)\n",
    "        total += accuracy_score(test_Y, predictions)\n",
    "\n",
    "    accuracy = total / numFolds\n",
    "    print(\"Accuracy score of {0}: {1}: {2}: {3}: {4}\".format(Model.__name__, accuracy))\n",
    "    print(\"accuracy metrics for {0}: {1}: {2}: {3}: {4}\".format(Model.__name__, classification_report(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
