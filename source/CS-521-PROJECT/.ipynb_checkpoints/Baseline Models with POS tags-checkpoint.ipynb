{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos tagging for fake news statement\n",
    "## use pos-tagging to build features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# import utility libraries\n",
    "import util\n",
    "import preprocessing\n",
    "import importlib\n",
    "import string\n",
    "\n",
    "# libraries for model testing and selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'D:\\\\UIC\\\\Fall 2018\\\\Statistical NLP\\\\Project\\\\jurat-aldo-project\\\\cs-521-project.git\\\\source\\\\CS-521-PROJECT\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "tr_file, va_file, te_file = util.load_files()\n",
    "tr_dict = util.tsv_to_dict(tsv_file=tr_file)\n",
    "va_dict = util.tsv_to_dict(tsv_file=va_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data without punctuation\n",
    "tr_data_no_punctuation = preprocessing.clean_text(sentences=tr_dict['statement'], remove_punctuation=True)\n",
    "va_data_no_punctuation = preprocessing.clean_text(sentences=va_dict['statement'], remove_punctuation=True)\n",
    "\n",
    "## Data without punctuation and uppercases\n",
    "tr_data_no_punct_upper = preprocessing.clean_text(sentences=tr_dict['statement'], remove_punctuation=True, lower_case= True)\n",
    "va_data_no_punct_upper = preprocessing.clean_text(sentences=va_dict['statement'], remove_punctuation=True, lower_case= True)\n",
    "\n",
    "## Data without punctuation, uppercases and stopwords\n",
    "tr_data_no_punct_upper_stopw = preprocessing.clean_text(sentences=tr_dict['statement'], remove_punctuation=True, lower_case=True, stop_words=True)\n",
    "va_data_no_punct_upper_stopw = preprocessing.clean_text(sentences=va_dict['statement'], remove_punctuation=True, lower_case=True, stop_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting POS tags grouped by unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting POS Tags\n",
      "Finished\n",
      "Extracting POS Tags\n",
      "Finished\n",
      "Extracting POS Tags\n",
      "Finished\n",
      "Extracting POS Tags\n",
      "Finished\n",
      "Extracting POS Tags\n",
      "Finished\n",
      "Extracting POS Tags\n",
      "Finished\n",
      "Extracting POS Tags\n",
      "Finished\n",
      "Extracting POS Tags\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# POS extracted for no preprocessed data\n",
    "unigram_pos, bigrams_pos, trigram_pos = preprocessing.extract_POS(tr_dict['statement'])\n",
    "unigram_pos_va, bigrams_pos_va, trigram_pos_va = preprocessing.extract_POS(statements=va_dict['statement'])\n",
    "\n",
    "# POS without punctuation\n",
    "unigram_pos_no_p, bigrams_pos_no_p, trigram_pos_no_p = preprocessing.extract_POS(tr_data_no_punctuation)\n",
    "unigram_pos_no_p_va, bigrams_pos_no_p_va, trigram_pos_no_p_va = preprocessing.extract_POS(va_data_no_punctuation)\n",
    "\n",
    "# POS withoutn punctuation and with lower case\n",
    "unigram_pos_no_p_u, bigrams_pos_no_p_u, trigram_pos_no_p_u = preprocessing.extract_POS(tr_data_no_punct_upper)\n",
    "unigram_pos_no_p_u_va, bigrams_pos_no_p_u_va, trigram_pos_no_p_u_va = preprocessing.extract_POS(va_data_no_punct_upper)\n",
    "\n",
    "# POW without punctuation, lower case and with no stop words\n",
    "unigram_pos_no_p_u_sw, bigrams_pos_no_p_u_sw, trigram_pos_no_p_u_sw = preprocessing.extract_POS(tr_data_no_punct_upper_stopw)\n",
    "unigram_pos_no_p_u_sw_va, bigrams_pos_no_p_u_sw_va, trigram_pos_no_p_u_sw_va = preprocessing.extract_POS(va_data_no_punct_upper_stopw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(nlp_util)\n",
    "nlp_task = nlp_util.NLP_Task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique POS by different preprocessing type of text\n",
    "# Unigram of unclean data\n",
    "unigram_list_tr = nlp_task.UniquePosTags(unigram_pos)\n",
    "unigram_list_va = nlp_task.UniquePosTags(unigram_pos_va)\n",
    "\n",
    "# Unigram of no punctuation data\n",
    "unigram_list_tr_no_p = nlp_task.UniquePosTags(unigram_pos_no_p)\n",
    "unigram_list_va_no_p = nlp_task.UniquePosTags(unigram_pos_no_p_va)\n",
    "\n",
    "# Unigram of no punctuation and with lower case\n",
    "unigram_list_tr_no_p_u = nlp_task.UniquePosTags(unigram_pos_no_p_u) \n",
    "unigram_list_va_no_p_u = nlp_task.UniquePosTags(unigram_pos_no_p_u_va)\n",
    "\n",
    "# Unigram of no punctuation, without lower case and without stop words\n",
    "unigram_list_tr_no_p_u_sw = nlp_task.UniquePosTags(unigram_pos_no_p_u_sw)\n",
    "unigram_list_va_no_p_u_sw = nlp_task.UniquePosTags(unigram_pos_no_p_u_sw_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new labels grouping different classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labels_tr = np.array(preprocessing.create_labels(labels=tr_dict['label'],label_values={'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}))\n",
    "binary_labels_va = np.array(preprocessing.create_labels(labels=va_dict['label'],label_values={'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}))\n",
    "\n",
    "tr_indexes = [i for i,x in enumerate(binary_labels_tr) if x!=0]\n",
    "va_indexes = [i for i,x in enumerate(binary_labels_va) if x!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use countVectorizor to build postag output as features for training ML model\n",
    "## uni_vocabulary: vocabulary=['cd','jjr','vb','jjs','nnp','in','vbp','to','rb','vbg','md','jj','dt']\n",
    "## bi_vocabulary: vocabulary=['NNP NNP', 'IN DT', 'JJR IN', 'CD NN', 'IN CD', 'CD NNS', 'DT JJS']\n",
    "## tri_vocabulary: ['NNP NNP NNP','CD NN IN','VBZ NNP NNP','IN DT NN','IN DT JJ','NN IN DT','<s> VBZ NNP','JJR IN CD','NNP NNP VBD','DT JJ CD','NNS IN DT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Train machine learning models with different preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_features_1 = []\n",
    "pruned_features_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING POSTags: NO PUNCTUATION\n",
      "Testing with user defined vectors\n",
      "Accuracy:  0.6438223938223938\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.59      0.42      0.49       420\n",
      "          1       0.67      0.80      0.73       616\n",
      "\n",
      "avg / total       0.63      0.64      0.63      1036\n",
      "\n",
      "VBZ 0.03675686719754296\n",
      "DT 0.029988763672937353\n",
      "NNPS 0.029744948243489285\n",
      "VBP 0.035678250862883995\n",
      "JJ 0.01673452984379914\n",
      "IN 0.034637454287928984\n",
      "WRB 0.008787453029583624\n",
      "VBD 0.019837055476423143\n",
      "PRP 0.01720870742459847\n",
      "RP 0.00309903014464449\n",
      "WDT 0.015923495335051833\n",
      "VB 0.09304912862253181\n",
      "NNP 0.06951052888157801\n",
      "VBG 0.04064254276118422\n",
      "PRP$ 0.005857735168362604\n",
      "VBN 0.02718188325028323\n",
      "CD 0.22206490656861025\n",
      "RB 0.07659126256463986\n",
      "WP 0.004375101380674572\n",
      "JJS 0.07049077766738678\n",
      "JJR 0.1096812091659285\n",
      "EX 0.0175591632631623\n",
      "RBS 0.00695613784919607\n",
      "FW 0.003139243335096094\n",
      "LS 0.004503824002482545\n",
      "['VBZ', 'DT', 'NNPS', 'VBP', 'JJ', 'IN', 'WRB', 'VBD', 'PRP', 'RP', 'WDT', 'VB', 'NNP', 'VBG', 'PRP$', 'VBN', 'CD', 'RB', 'WP', 'JJS', 'JJR', 'EX', 'RBS', 'FW', 'LS']\n",
      "Testing with sklearn built in vectors\n",
      "Accuracy:  0.6418918918918919\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.58      0.41      0.48       420\n",
      "          1       0.67      0.80      0.73       616\n",
      "\n",
      "avg / total       0.63      0.64      0.63      1036\n",
      "\n",
      "VBZ 0.03650912834948451\n",
      "DT 0.02978664139393292\n",
      "NNPS 0.029544469264315216\n",
      "VBP 0.03543778181741261\n",
      "JJ 0.0271164799373348\n",
      "IN 0.03440400014237159\n",
      "WRB 0.008728226178741931\n",
      "VBD 0.019703355037640122\n",
      "PRP 0.020522966016759545\n",
      "RP 0.0026603949236990746\n",
      "WDT 0.015816172032166172\n",
      "VB 0.0924219836644509\n",
      "NNP 0.06904203252522348\n",
      "VBG 0.0381628026764789\n",
      "VBN 0.026998679159241035\n",
      "CD 0.220568203820472\n",
      "RB 0.07607504253268962\n",
      "JJS 0.070015674499203\n",
      "CC 0.002524234875250604\n",
      "JJR 0.10894196508763442\n",
      "PDT 0.0030781428869245515\n",
      "EX 0.017440815666880296\n",
      "RBS 0.006909253941260334\n",
      "FW 0.0031180850431385063\n",
      "LS 0.004473468527293745\n",
      "['VBZ', 'DT', 'NNPS', 'VBP', 'JJ', 'IN', 'WRB', 'VBD', 'PRP', 'RP', 'WDT', 'VB', 'NNP', 'VBG', 'VBN', 'CD', 'RB', 'JJS', 'CC', 'JJR', 'PDT', 'EX', 'RBS', 'FW', 'LS']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "training_data = [unigram_pos, unigram_pos_no_p, unigram_pos_no_p_u, unigram_pos_no_p_u_sw] \n",
    "testing_data = [unigram_pos_va, unigram_pos_no_p_va, unigram_pos_no_p_u_va, unigram_pos_no_p_u_sw_va]\n",
    "#training_data = [bigrams_pos, bigrams_pos_no_p, bigrams_pos_no_p_u, bigrams_pos_no_p_u_sw] \n",
    "#testing_data = [bigrams_pos_va, bigrams_pos_no_p_va, bigrams_pos_no_p_u_va, bigrams_pos_no_p_u_sw_va]\n",
    "testing_title = ['RAW DATA', 'NO PUNCTUATION', 'NO PUNCTUATION LOWER CASE', 'NO PUNCTUATION LC STOP WORDS']\n",
    "\n",
    "trda = training_data[1:2]\n",
    "teda = testing_data[1:2]\n",
    "tetil = testing_title[1:2]\n",
    "\n",
    "for i, each_data in enumerate(trda):\n",
    "    print(\"TRAINING POSTags:\", tetil[i])\n",
    "    print(\"Testing with user defined vectors\")\n",
    "    Xtr, Xte = PreparePOSDataForTraining(each_data, teda[i], use_built_in_vectors=False,user_defined_vocabulary=unigram_list_tr_no_p)\n",
    "    Xtr = Xtr[tr_indexes]\n",
    "    Xte = Xte[va_indexes]\n",
    "    Ytr = binary_labels_tr[tr_indexes]\n",
    "    Yte = binary_labels_va[va_indexes]\n",
    "    #pruned_features_1 = FeatureSelector(Xtr, Xte, Ytr, Yte, unigram_list_tr_no_p,max_depth=6, threshold=0.00001)\n",
    "    #print(pruned_features_1)\n",
    "    DecisionTreeFeaturesSelector(Xtr, Xte, Ytr, Yte)\n",
    "    #TrainModels(Xtr, Xte, Ytr, Yte)\n",
    "    print(\"Testing with sklearn built in vectors\")\n",
    "    Xtr, Xte = PreparePOSDataForTraining(each_data, teda[i], use_built_in_vectors=True,user_defined_vocabulary=unigram_list_tr_no_p)\n",
    "    Xtr = Xtr[tr_indexes]\n",
    "    Xte = Xte[va_indexes]\n",
    "    Ytr = binary_labels_tr[tr_indexes]\n",
    "    Yte = binary_labels_va[va_indexes]\n",
    "    #FeatureSelector(Xtr, Xte, Ytr, Yte, unigram_list_tr_no_p,max_depth=6,threshold=0.000001)\n",
    "    DecisionTreeFeaturesSelector(Xtr, Xte, Ytr, Yte)\n",
    "    #TrainModels(Xtr, Xte, Ytr, Yte)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTreeFeaturesSelector(Xtr, Xte, Ytr, Yte):\n",
    "    for i in range(1,10):\n",
    "        print('max_depth',i)\n",
    "        clf = DecisionTreeClassifier(max_depth=i)\n",
    "        clf.fit(Xtr, Ytr)\n",
    "        pred = clf.predict(Xte)\n",
    "        print('Accuracy: ', np.mean(Yte == pred))\n",
    "        print(classification_report(Yte, pred))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureSelector(Xtr, Xte, Ytr, Yte,feature_list,max_depth = 1,threshold=0.1):\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    clf.fit(Xtr, Ytr)\n",
    "    pred = clf.predict(Xte)\n",
    "    print('Accuracy: ', np.mean(Yte == pred))\n",
    "    print(classification_report(Yte, pred)) \n",
    "    \n",
    "    pos_Tags = list()\n",
    "    for i,each_f in enumerate(clf.feature_importances_):\n",
    "        if each_f >=threshold:\n",
    "            pos_Tags.append(feature_list[i])\n",
    "            print(feature_list[i], each_f)\n",
    "    print(pos_Tags)\n",
    "    return (clf.feature_importances_<=threshold) # return filetered features\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use_built_in_vectors => if True use CountVectorizer, otherwise it uses the version of pos_vectors in preprocessing\n",
    "def PreparePOSDataForTraining(training_data, testing_data, use_built_in_vectors = False, user_defined_vocabulary=None):\n",
    "    tr_feats = []\n",
    "    te_feats = []\n",
    "    if(use_built_in_vectors == False):\n",
    "        tr_feats, returned_dict = preprocessing.pos_vectors(training_data, vector_dictionary= user_defined_vocabulary, return_dictionary=True)\n",
    "        te_feats = preprocessing.pos_vectors(testing_data, vector_dictionary= returned_dict)\n",
    "    else:\n",
    "        if user_defined_vocabulary != None:\n",
    "            user_defined_vocabulary = [x.lower() for x in user_defined_vocabulary]\n",
    "        training_str = [\" \".join(x) for x in training_data]\n",
    "        testing_str = [\" \".join(x) for x in testing_data]\n",
    "        tr_vectorizer = CountVectorizer(vocabulary=user_defined_vocabulary,binary=True)\n",
    "        tr_feats = tr_vectorizer.fit_transform(training_str).toarray()\n",
    "        te_vectorizer = CountVectorizer(vocabulary=tr_vectorizer.get_feature_names(),binary=True)\n",
    "        te_feats = te_vectorizer.fit_transform(testing_str).toarray()\n",
    "    return tr_feats, te_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Different Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModels(Xtrain, Xtest, Ytrain, Ytest):\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    # ,'class_weight':{1:.9, 2:.5, 3:.01}\n",
    "    params = [{'max_depth':1,'criterion':'entropy'},{}, {'loss': 'log', 'penalty': 'l2', 'max_iter':1000},{},{},{}]\n",
    "    Models = [DecisionTreeClassifier,LogisticRegression, linear_model.SGDClassifier, MultinomialNB, RandomForestClassifier, SVC]\n",
    "    accuracy_list = list()\n",
    "    accuracy_metrics = list()\n",
    "    for param, Model in zip(params, Models):\n",
    "        total = 0\n",
    "        for train_indices, test_indices in kf.split(Xtrain):\n",
    "            train_X = Xtrain[train_indices, :]; train_Y = Ytrain[train_indices]\n",
    "            test_X = Xtrain[test_indices, :]; test_Y = Ytrain[test_indices]\n",
    "            reg = Model(**param)\n",
    "            reg.fit(train_X, train_Y)\n",
    "            predictions = reg.predict(test_X)\n",
    "            total += accuracy_score(test_Y, predictions)\n",
    "        accuracy = total / kf.n_splits\n",
    "        reg = Model(**param)\n",
    "        reg.fit(Xtrain, Ytrain)\n",
    "        predictions = reg.predict(Xtest)\n",
    "        accuracy_list.append((Model.__name__, accuracy))\n",
    "        accuracy_metrics.append((Model.__name__, classification_report(Ytest, predictions)))\n",
    "\n",
    "    for i, value in enumerate(accuracy_list):\n",
    "        print(\"Accuracy score of {0}: {1}\".format(value[0],value[1]))\n",
    "        print(\"accuracy metrics for {0}:\\n{1}\".format(accuracy_metrics[i][0], accuracy_metrics[i][1]))\n",
    "\n",
    "#print(\"Accuracy score of {0}: {1}: {2}: {3}: {4}\".format(accuracy_list[0], accuracy_list[]))\n",
    "#print(\"accuracy metrics for {0}: {1}: {2}: {3}: {4}\".format(Model.__name__, classification_report(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
