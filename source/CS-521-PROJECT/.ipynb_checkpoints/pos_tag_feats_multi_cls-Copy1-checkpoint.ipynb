{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# pos tagging for fake news statement\n",
    "## use pos-tagging to build features\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import libraries\n",
    "\n",
    "import util\n",
    "from nlp_util import NLP_Task\n",
    "import preprocessing\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "tr_file, va_file, te_file = util.load_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n"
     ]
    }
   ],
   "source": [
    "nlpTask = NLP_Task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dict = util.tsv_to_dict(tsv_file=tr_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_dict = util.tsv_to_dict(tsv_file=va_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for statement in tr_dict['statement']:\n",
    "    review = re.sub('[^a-zA-Z]',' ',statement)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = ''.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n",
      "Extracting POS Tags\n",
      "Stringify\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(preprocessing)\n",
    "unigram_pos, bigrams_pos, trigram_pos = preprocessing.extract_POS(statements=tr_dict['statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n",
      "Extracting POS Tags\n",
      "Stringify\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "unigram_pos_va, bigrams_pos_va, trigram_pos_va = preprocessing.extract_POS(statements=va_dict['statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use countVectorizor to build postag output as features for training ML model\n",
    "## uni_vocabulary: vocabulary=['cd','jjr','vb','jjs','nnp','in','vbp','to','rb','vbg','md','jj','dt']\n",
    "## bi_vocabulary: vocabulary=['NNP NNP', 'IN DT', 'JJR IN', 'CD NN', 'IN CD', 'CD NNS', 'DT JJS']\n",
    "## tri_vocabulary: ['NNP NNP NNP','CD NN IN','VBZ NNP NNP','IN DT NN','IN DT JJ','NN IN DT','<s> VBZ NNP','JJR IN CD','NNP NNP VBD','DT JJ CD','NNS IN DT']\n",
    "\n",
    "cv_uni = CountVectorizer()\n",
    "cv_bi = CountVectorizer()\n",
    "cv_tri = CountVectorizer()\n",
    "\n",
    "pos_uni_feats = cv_uni.fit_transform(unigram_pos).toarray()\n",
    "pos_bi_feats = cv_bi.fit_transform(bigrams_pos).toarray()\n",
    "pos_tri_feats = cv_tri.fit_transform(trigram_pos).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_uni_va = CountVectorizer(vocabulary=cv_uni.get_feature_names())\n",
    "cv_bi_va = CountVectorizer(vocabulary=cv_bi.get_feature_names())\n",
    "cv_tri_va = CountVectorizer(vocabulary=cv_tri.get_feature_names())\n",
    "\n",
    "pos_uni_feats_va = cv_uni_va.fit_transform(unigram_pos).toarray()\n",
    "pos_bi_feats_va = cv_bi_va.fit_transform(bigrams_pos).toarray()\n",
    "pos_tri_feats_va = cv_tri_va.fit_transform(trigram_pos).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labels_tr = np.array(preprocessing.create_labels(labels=tr_dict['label'],label_values={'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}))\n",
    "binary_labels_va = np.array(preprocessing.create_labels(labels=va_dict['label'],label_values={'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "tr_indexes = [i for i,x in enumerate(binary_labels_tr) if x!=0]\n",
    "va_indexes = [i for i,x in enumerate(binary_labels_va) if x!=0]\n",
    "transformer = Binarizer().fit(pos_uni_feats[tr_indexes]) # fit does nothing.\n",
    "X_train = transformer.transform(pos_uni_feats[tr_indexes])\n",
    "transformer = Binarizer().fit(pos_uni_feats[va_indexes])\n",
    "X_test =  transformer.transform(pos_uni_feats[va_indexes])\n",
    "y_train = binary_labels_tr[tr_indexes]\n",
    "y_test = binary_labels_va[va_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy metrics for logistic regression classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.39      0.31      0.35       420\n",
      "          1       0.59      0.66      0.62       616\n",
      "\n",
      "avg / total       0.51      0.52      0.51      1036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## train bigram_pos features on logistic regression\n",
    "LR = LogisticRegression()\n",
    "LR = LR.fit(X=X_train, y=y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "print(\"accuracy metrics for logistic regression classifier:\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy metrics for Multinomial naive bayes classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.38      0.28      0.32       420\n",
      "          1       0.58      0.69      0.63       616\n",
      "\n",
      "avg / total       0.50      0.52      0.51      1036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### train features on multinomial naive bayes classifier\n",
    "nb_cly=MultinomialNB()\n",
    "nb_cly.fit(X=X_train, y=y_train)\n",
    "y_pred=nb_cly.predict(X_test)\n",
    "print(\"accuracy metrics for Multinomial naive bayes classifier:\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy metrics for support vector machine classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.38      0.21      0.27       420\n",
      "          1       0.59      0.77      0.67       616\n",
      "\n",
      "avg / total       0.50      0.54      0.50      1036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### train features on SVM\n",
    "\n",
    "svm_clf=SVC(C=1, kernel='rbf', degree=3, gamma='auto', random_state=None)\n",
    "svm_clf=svm_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=svm_clf.predict(X_test)\n",
    "print(\"accuracy metrics for support vector machine classifier:\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy metrics for stochastic gardient desent classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.37      0.14      0.20       420\n",
      "          1       0.59      0.84      0.69       616\n",
      "\n",
      "avg / total       0.50      0.56      0.49      1036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### train features on SGD\n",
    "sgd_clf = linear_model.SGDClassifier(max_iter=1000)\n",
    "sgd_clf=sgd_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=sgd_clf.predict(X_test)\n",
    "print(\"accuracy metrics for stochastic gardient desent classifier:\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy metrics for random forest classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.41      0.41      0.41       420\n",
      "          1       0.60      0.60      0.60       616\n",
      "\n",
      "avg / total       0.52      0.52      0.52      1036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### train features on random forest\n",
    "randFor_clf=RandomForestClassifier(n_estimators=2, criterion='gini', max_features='auto')\n",
    "randFor_clf=randFor_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=randFor_clf.predict(X_test)\n",
    "print(\"accuracy metrics for random forest classifier:\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## set up stanfordcorenlp server\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "# Importing the dataset\n",
    "dataset=pd.read_csv('train.tsv',delimiter='\\t',encoding='utf-8')\n",
    "dataset.columns=['statement_ID','label','statement','subject','speaker','job_title',\n",
    "           'state_info','pantry_affiliation','barely_true_cnt','false_cnt',\n",
    "           'half_true_cnt','mostly_true_cnt','pants_on_fire_cnt','context']\n",
    "\n",
    "\n",
    "dataset.head()\n",
    "\n",
    "total_rows = len(dataset.index)\n",
    "cols = list(dataset.columns.values)\n",
    "\n",
    "# Cleaning the texts\n",
    "corpus = []\n",
    "for i in range(0, total_rows):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['statement'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "train_data=corpus\n",
    "## define key-value\n",
    "data_dict=dict()\n",
    "\n",
    "data_dict['ids'] = train_data[['ID']].values[:,0]\n",
    "data_dict['labels'] = train_data[['Label']].values[:,0]\n",
    "data_dict['statements'] = train_data[['Statement']].values[:,0]\n",
    "data_dict['subjects'] = train_data[['Subject']].values[:,0]\n",
    "data_dict['contexts'] = train_data[['Context']].values[:,0]\n",
    "# Process each sentence\n",
    "# Obtain the Part Of Speech for every word in the statement\n",
    "# Get POS bigram way\n",
    "# Get POS trigram way\n",
    "\n",
    "word_pos_list = list()\n",
    "pos_list = list()\n",
    "unigram_pos_list=list()\n",
    "bigram_pos_list = list()\n",
    "trigram_pos_list = list()\n",
    "\n",
    "for sent_id, txt in enumerate(data_dict['text']):\n",
    "    print('Procesing sentence number {0}: {1}'.format(sent_id,txt))\n",
    "    output = nlp.annotate(txt, properties={\n",
    "      'annotators': 'tokenize,pos,parse',\n",
    "      'outputFormat': 'json'\n",
    "      })\n",
    "    result_word_pos = str()\n",
    "    result_pos = list()\n",
    "    unigram_pos=list()\n",
    "    bigram_pos = list()\n",
    "    trigram_pos = list()\n",
    "    for o in output['sentences']:\n",
    "        result_pos.append('<s>')\n",
    "        for t in o['tokens']:\n",
    "            result_word_pos += '{0}/{1} '.format(t['word'],t['pos'])\n",
    "            result_pos.append('{0}'.format(t['pos']))\n",
    "        for rpIndex, rp in enumerate(result_pos):\n",
    "            if rpIndex==len(result_pos):\n",
    "                unigram_pos.append('{0}').format(rp, result_pos[rpIndex+1])\n",
    "            if rpIndex < len(result_pos)-1 and len(result_pos)>=2:\n",
    "                bigram_pos.append('{0} {1}'.format(rp, result_pos[rpIndex+1]))\n",
    "            if rpIndex < len(result_pos)-2 and len(result_pos)>=3:\n",
    "                trigram_pos.append('{0} {1} {2}'.format(rp, result_pos[rpIndex+1], result_pos[rpIndex+2]))\n",
    "\n",
    "    word_pos_list.append(result_word_pos)\n",
    "    pos_list.append(result_pos)\n",
    "    unigram_pos_list.append(unigram_pos)\n",
    "    bigram_pos_list.append(bigram_pos)\n",
    "    trigram_pos_list.append(trigram_pos)\n",
    "\n",
    "##\n",
    "data_dict['word_pos'] = word_pos_list\n",
    "data_dict['pos'] = pos_list\n",
    "data_dict['unigram'] = unigram_pos_list\n",
    "data_dict['bigrams'] = bigram_pos_list\n",
    "data_dict['trigrams'] = trigram_pos_list\n",
    "\n",
    "##\n",
    "dataset['Word POS'] = data_dict['word_pos']\n",
    "dataset['POS'] = data_dict['pos']\n",
    "dataset['unigram_pos'] = data_dict['unigram']\n",
    "dataset['bigrams_pos'] = data_dict['bigrams']\n",
    "dataset['trigram_pos'] = data_dict['trigrams']\n",
    "\n",
    "## use countVectorizor to build postag output as features for training ML model\n",
    "cv = CountVectorizer()\n",
    "\n",
    "pos_uni_feats = cv.fit_transform(dataset['unigram_pos']).toarray()\n",
    "pos_big_feats = cv.fit_transform(dataset['bigram_pos']).toarray()\n",
    "pos_trig_feats = cv.fit_transform(dataset['trigram_pos']).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(pos_uni_feats,\n",
    "        dataset['class'],train_size=0.8, random_state=123)\n",
    "\n",
    "## train bigram_pos features on logistic regression\n",
    "LR = LogisticRegression()\n",
    "LR = LR.fit(X=X_train, y=y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "print(\"accuracy metrics for logistic regression classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "### train features on multinomial naive bayes classifier\n",
    "nb_cly=OneVsRestClassifier(MultinomialNB()).fit(X=X_train, y=y_train)\n",
    "y_pred=nb_cly.predict(X_test)\n",
    "print(\"accuracy metrics for Multinomial naive bayes classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "### train features on SVM\n",
    "\n",
    "svm_clf=SVC(C=1, kernel='rbf', degree=3, gamma='auto', random_state=None)\n",
    "svm_clf=svm_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=svm_clf.predict(X_test)\n",
    "print(\"accuracy metrics for support vector machine classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "### train features on SGD\n",
    "sgd_clf = linear_model.SGDClassifier(max_iter=1000)\n",
    "sgd_clf=sgd_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=sgd_clf.predict(X_test)\n",
    "print(\"accuracy metrics for stochastic gardient desent classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "### train features on random forest\n",
    "randFor_clf=RandomForestClassifier(n_estimators=2, criterion='gini', max_features='auto', class_weight={1:.9, 2:.5, 3:.01})\n",
    "randFor_clf=randFor_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=randFor_clf.predict(X_test)\n",
    "print(\"accuracy metrics for random forest classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "##----------- More compact solution for training several emprical ML classfier model -----------##\n",
    "### use kfold cross validation\n",
    "## Kforld cross validation on training set\n",
    "kf = KFold(len(X_train), numFolds=10, shuffle=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(pos_uni_feats,\n",
    "        dataset['class'],train_size=0.8, random_state=123)\n",
    "Y=dataset['class']\n",
    "\n",
    "params = [{}, {'loss': 'log', 'penalty': 'l2', 'n_iter':1000,class_weight:{1:.9, 2:.5, 3:.01}}]\n",
    "Models = [LogisticRegression, SGDClassifier, MultinomialNB, RandomForestClassifier, SVC]\n",
    "\n",
    "for param, Model in zip(params, Models):\n",
    "    total = 0\n",
    "    for train_indices, test_indices in kf:\n",
    "        train_X = X_train[train_indices, :]; train_Y = y_train[train_indices]\n",
    "        test_X = X_test[test_indices, :]; test_Y = y_test[test_indices]\n",
    "        reg = Model(**param)\n",
    "        reg.fit(train_X, train_Y)\n",
    "        predictions = reg.predict(test_X)\n",
    "        total += accuracy_score(test_Y, predictions)\n",
    "\n",
    "    accuracy = total / numFolds\n",
    "    print(\"Accuracy score of {0}: {1}: {2}: {3}: {4}\".format(Model.__name__, accuracy))\n",
    "    print(\"accuracy metrics for {0}: {1}: {2}: {3}: {4}\".format(Model.__name__, classification_report(y_test, predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
