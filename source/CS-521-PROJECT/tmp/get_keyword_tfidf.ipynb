{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## find key phrase with TF-IDF weight\n",
    "# import re\n",
    "# import string\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from nltk import pos_tag\n",
    "# from nltk.corpus import stopwords\n",
    "# from itertools import chain, groupby\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# from nltk.chunk import tree2conlltags\n",
    "# from nltk import word_tokenize, sent_tokenize\n",
    "# from nltk.chunk.regexp import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm_punct=re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "# stop_words=set(stopwords.words('english'))\n",
    "\n",
    "# def get_cand_words(sent, cand_type='word', remove_punct=False):\n",
    "#     candidates=list()\n",
    "#     sent=rm_punct.sub(' ', sent)\n",
    "#     tokenized=word_tokenize(sent)\n",
    "#     tagged_words=pos_tag(tokenized)\n",
    "#     if cand_type=='word':\n",
    "#         pos_tag_patt=tags = set(['JJ', 'JJR', 'JJS', 'NN', 'NNP', 'NNS', 'NNPS'])\n",
    "#         tagged_words=chain.from_iterable(tagged_words)\n",
    "#         for word, tag in enumerate(tagged_words):\n",
    "#             if tag in pos_tag_patt and word not in stop_words:\n",
    "#                 candidates.append(word)\n",
    "                \n",
    "# #     elif cand_type == 'phrase':\n",
    "# #         grammar = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "# #         chunker = RegexpParser(grammar)\n",
    "# #         all_tag = chain.from_iterable([chunker.parse(tag) for tag in tagged_words])\n",
    "# #         for key, group in groupby(all_tag, lambda tag: tag[2] != 'O'):\n",
    "# #             candidate = ' '.join([word for (word, pos, chunk) in group])\n",
    "# #             if key is True and candidate not in stop_words:\n",
    "# #                 candidates.append(candidate)\n",
    "# #     else:\n",
    "# #         print(\"return word or phrase as target phrase\")\n",
    "#     return candidates\n",
    "\n",
    "\n",
    "# doc=\"Hillary Clinton agrees with John McCain by voting to give George Bush the benefit of the doubt on Iran.\"\n",
    "# get_cand_words(sent=doc, cand_type='word', remove_punct=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read dataset\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "dataset=pd.read_csv('train.tsv',delimiter='\\t',encoding='utf-8')\n",
    "dataset.columns=['ID','Label','Statement','Subject','speaker','job_title',\n",
    "           'state_info','pantry_affiliation','barely_true_cnt','false_cnt',\n",
    "           'half_true_cnt','mostly_true_cnt','pants_on_fire_cnt','Context']\n",
    "#---------------------------------------------------#\n",
    "# ## data cleaning phase\n",
    "# stop=set(stopwords.words('english'))\n",
    "# text_no_stops=[]\n",
    "# for elm in range(0, len(dataset.index)):\n",
    "#     res=' '.join([i for i in dataset['Statement'][elm].lower().split() if i not in stop])\n",
    "#     text_no_stops.append(res)\n",
    "\n",
    "# ##\n",
    "# dataset['Statement']=text_no_stops\n",
    "# ## remove the punctuation\n",
    "# rm_punct=re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "# text_no_punc=[]\n",
    "# for i in range(0, len(dataset.index)):\n",
    "#     removed=rm_punct.sub(' ', dataset['Statement'][i])\n",
    "#     text_no_punc.append(removed)\n",
    "\n",
    "# dataset['Statement']=text_no_punc\n",
    "# ## here is how to clean up non-letter symbols in statement columns\n",
    "# all_text = []\n",
    "# for i in range(0, len(dataset.index)):\n",
    "#     patt = re.sub('[^a-zA-Z]', ' ', dataset['Statement'][i])\n",
    "#     res = ' '.join(str(patt).lower().split())\n",
    "#     all_text.append(res)\n",
    "\n",
    "# dataset['Statement']=all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## full implementation of TF-IDF\n",
    "from nltk.corpus import stopwords\n",
    "import string, re\n",
    "\n",
    "def preprocessing_txt(dataset):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    rm_punct = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    corpus=[]\n",
    "    for elm in range(0, len(dataset.index)):\n",
    "        res=' '.join([i for i in dataset['Statement'][elm].lower().split() if i not in stop_words])\n",
    "        res=rm_punct.sub('',res)\n",
    "        corpus.append(res)\n",
    "    return corpus\n",
    "\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "mydat=dataset['Statement'].tolist()\n",
    "cv=CountVectorizer(max_df=0.85, stop_words=None)\n",
    "word_cnt_vector=cv.fit_transform(mydat)\n",
    "word_cnt_vector.shape\n",
    "\n",
    "list(cv.vocabulary_.keys())[:10]\n",
    "list(cv.get_feature_names())[2000:3000]\n",
    "\n",
    "\n",
    "## TF-IDF transformation to compute IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_cnt_vector)\n",
    "\n",
    "## print idf\n",
    "tfidf_transformer.idf_\n",
    "\n",
    "##\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples=zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "##\n",
    "\n",
    "feature_name=cv.get_feature_names()\n",
    "doc=mydat[0]\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "key_words=extract_topn_from_vector(feature_name, sorted_items,10)\n",
    "key_words\n",
    "\n",
    "##\n",
    "def get_keywords():\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    keywords=extract_topn_from_vector(feature_name,sorted_items,1)\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict_keys(['started']),\n",
       " dict_keys(['doubt']),\n",
       " dict_keys(['surgeries']),\n",
       " dict_keys(['turnaround']),\n",
       " dict_keys(['tenured']),\n",
       " dict_keys(['dunnam']),\n",
       " dict_keys(['watergate']),\n",
       " dict_keys(['noaa']),\n",
       " dict_keys(['leibham']),\n",
       " dict_keys(['margin']),\n",
       " dict_keys(['slipped']),\n",
       " dict_keys(['slow']),\n",
       " dict_keys(['bled']),\n",
       " dict_keys(['waived']),\n",
       " dict_keys(['chose']),\n",
       " dict_keys(['buy']),\n",
       " dict_keys(['wis']),\n",
       " dict_keys(['subsidiary']),\n",
       " dict_keys(['rico']),\n",
       " dict_keys(['adjust']),\n",
       " dict_keys(['corporate']),\n",
       " dict_keys(['auto']),\n",
       " dict_keys(['favors']),\n",
       " dict_keys(['rid']),\n",
       " dict_keys(['jonathan']),\n",
       " dict_keys(['mosques']),\n",
       " dict_keys(['landslide']),\n",
       " dict_keys(['tenn']),\n",
       " dict_keys(['youth']),\n",
       " dict_keys(['endorsing']),\n",
       " dict_keys(['climates']),\n",
       " dict_keys(['going']),\n",
       " dict_keys(['business']),\n",
       " dict_keys(['pryor']),\n",
       " dict_keys(['toilet']),\n",
       " dict_keys(['founded']),\n",
       " dict_keys(['nuclear']),\n",
       " dict_keys(['destruct']),\n",
       " dict_keys(['added']),\n",
       " dict_keys(['suggested']),\n",
       " dict_keys(['kurds']),\n",
       " dict_keys(['solyndra']),\n",
       " dict_keys(['vaginal']),\n",
       " dict_keys(['use']),\n",
       " dict_keys(['youth']),\n",
       " dict_keys(['illegally']),\n",
       " dict_keys(['commissions']),\n",
       " dict_keys(['khans']),\n",
       " dict_keys(['burners']),\n",
       " dict_keys(['paperback']),\n",
       " dict_keys(['hassan']),\n",
       " dict_keys(['bedroom']),\n",
       " dict_keys(['jobs']),\n",
       " dict_keys(['water']),\n",
       " dict_keys(['earn']),\n",
       " dict_keys(['verifies']),\n",
       " dict_keys(['ago']),\n",
       " dict_keys(['choosing']),\n",
       " dict_keys(['latinos']),\n",
       " dict_keys(['pe']),\n",
       " dict_keys(['who']),\n",
       " dict_keys(['unanimously']),\n",
       " dict_keys(['teachers']),\n",
       " dict_keys(['sanctuary']),\n",
       " dict_keys(['transgender']),\n",
       " dict_keys(['rightly']),\n",
       " dict_keys(['isis']),\n",
       " dict_keys(['electronically']),\n",
       " dict_keys(['bid']),\n",
       " dict_keys(['mcdonnell']),\n",
       " dict_keys(['collapsed']),\n",
       " dict_keys(['reasonably']),\n",
       " dict_keys(['ii']),\n",
       " dict_keys(['factor']),\n",
       " dict_keys(['perversity']),\n",
       " dict_keys(['forthcoming']),\n",
       " dict_keys(['veteran']),\n",
       " dict_keys(['advantage']),\n",
       " dict_keys(['unacceptable']),\n",
       " dict_keys(['deny']),\n",
       " dict_keys(['lives']),\n",
       " dict_keys(['strides']),\n",
       " dict_keys(['significance']),\n",
       " dict_keys(['southerland']),\n",
       " dict_keys(['nurses']),\n",
       " dict_keys(['dentures']),\n",
       " dict_keys(['explosion']),\n",
       " dict_keys(['provides']),\n",
       " dict_keys(['burdened']),\n",
       " dict_keys(['kidnapping']),\n",
       " dict_keys(['belong']),\n",
       " dict_keys(['squat']),\n",
       " dict_keys(['beat']),\n",
       " dict_keys(['grenada']),\n",
       " dict_keys(['jolly']),\n",
       " dict_keys(['depression']),\n",
       " dict_keys(['cellphones']),\n",
       " dict_keys(['died']),\n",
       " dict_keys(['foreclosure']),\n",
       " dict_keys(['freshfromflorida']),\n",
       " dict_keys(['streetcar']),\n",
       " dict_keys(['lift']),\n",
       " dict_keys(['ortega']),\n",
       " dict_keys(['nato']),\n",
       " dict_keys(['prochoice']),\n",
       " dict_keys(['portfolio']),\n",
       " dict_keys(['growing']),\n",
       " dict_keys(['lay']),\n",
       " dict_keys(['teachers']),\n",
       " dict_keys(['search']),\n",
       " dict_keys(['per']),\n",
       " dict_keys(['groups']),\n",
       " dict_keys(['promised']),\n",
       " dict_keys(['pace']),\n",
       " dict_keys(['netanyahu']),\n",
       " dict_keys(['freely']),\n",
       " dict_keys(['earl']),\n",
       " dict_keys(['underqualified']),\n",
       " dict_keys(['half']),\n",
       " dict_keys(['britains']),\n",
       " dict_keys(['decline']),\n",
       " dict_keys(['constables']),\n",
       " dict_keys(['prepaid']),\n",
       " dict_keys(['columbus']),\n",
       " dict_keys(['refunded']),\n",
       " dict_keys(['sciences']),\n",
       " dict_keys(['fantasy']),\n",
       " dict_keys(['disputes']),\n",
       " dict_keys(['guns']),\n",
       " dict_keys(['neighborhood']),\n",
       " dict_keys(['organizers']),\n",
       " dict_keys(['offered']),\n",
       " dict_keys(['halfway']),\n",
       " dict_keys(['guaranteed']),\n",
       " dict_keys(['saysmichael']),\n",
       " dict_keys(['amazing']),\n",
       " dict_keys(['iraqi']),\n",
       " dict_keys(['arriving']),\n",
       " dict_keys(['doorway']),\n",
       " dict_keys(['caprio']),\n",
       " dict_keys(['temporary']),\n",
       " dict_keys(['exceed']),\n",
       " dict_keys(['had']),\n",
       " dict_keys(['proliferation']),\n",
       " dict_keys(['lobbyists']),\n",
       " dict_keys(['worked']),\n",
       " dict_keys(['profiling']),\n",
       " dict_keys(['begging']),\n",
       " dict_keys(['waiting']),\n",
       " dict_keys(['economy']),\n",
       " dict_keys(['cornilles']),\n",
       " dict_keys(['trimester']),\n",
       " dict_keys(['steady']),\n",
       " dict_keys(['redemptions']),\n",
       " dict_keys(['admit']),\n",
       " dict_keys(['everglades']),\n",
       " dict_keys(['ponzi']),\n",
       " dict_keys(['carolinas']),\n",
       " dict_keys(['directly']),\n",
       " dict_keys(['soup']),\n",
       " dict_keys(['windmills']),\n",
       " dict_keys(['librarian']),\n",
       " dict_keys(['forfeiture']),\n",
       " dict_keys(['ago']),\n",
       " dict_keys(['hillarycare']),\n",
       " dict_keys(['complaints']),\n",
       " dict_keys(['proposals']),\n",
       " dict_keys(['statistic']),\n",
       " dict_keys(['package']),\n",
       " dict_keys(['mining']),\n",
       " dict_keys(['sheltered']),\n",
       " dict_keys(['worsened']),\n",
       " dict_keys(['sanctuary']),\n",
       " dict_keys(['jersey']),\n",
       " dict_keys(['elevated']),\n",
       " dict_keys(['conducting']),\n",
       " dict_keys(['prices']),\n",
       " dict_keys(['trap']),\n",
       " dict_keys(['judges']),\n",
       " dict_keys(['casinos']),\n",
       " dict_keys(['create']),\n",
       " dict_keys(['advantage']),\n",
       " dict_keys(['florida']),\n",
       " dict_keys(['rank']),\n",
       " dict_keys(['operation']),\n",
       " dict_keys(['charging']),\n",
       " dict_keys(['sound']),\n",
       " dict_keys(['stricklands']),\n",
       " dict_keys(['amber']),\n",
       " dict_keys(['muslim']),\n",
       " dict_keys(['exploding']),\n",
       " dict_keys(['stabbing']),\n",
       " dict_keys(['adjustment']),\n",
       " dict_keys(['jose']),\n",
       " dict_keys(['authorizing']),\n",
       " dict_keys(['got']),\n",
       " dict_keys(['guts']),\n",
       " dict_keys(['wages']),\n",
       " dict_keys(['promoting']),\n",
       " dict_keys(['pleasant']),\n",
       " dict_keys(['conclude']),\n",
       " dict_keys(['along']),\n",
       " dict_keys(['pack']),\n",
       " dict_keys(['denies']),\n",
       " dict_keys(['needle']),\n",
       " dict_keys(['consists']),\n",
       " dict_keys(['function']),\n",
       " dict_keys(['agencies']),\n",
       " dict_keys(['weidner']),\n",
       " dict_keys(['kean']),\n",
       " dict_keys(['biggest']),\n",
       " dict_keys(['commander']),\n",
       " dict_keys(['male']),\n",
       " dict_keys(['barrett']),\n",
       " dict_keys(['representatives']),\n",
       " dict_keys(['gridlock']),\n",
       " dict_keys(['lying']),\n",
       " dict_keys(['leased']),\n",
       " dict_keys(['lease']),\n",
       " dict_keys(['decade']),\n",
       " dict_keys(['nominations']),\n",
       " dict_keys(['raufs']),\n",
       " dict_keys(['lakefront']),\n",
       " dict_keys(['details']),\n",
       " dict_keys(['considerably']),\n",
       " dict_keys(['partition']),\n",
       " dict_keys(['limbaugh']),\n",
       " dict_keys(['wasnt']),\n",
       " dict_keys(['recruiting']),\n",
       " dict_keys(['perrys']),\n",
       " dict_keys(['abolish']),\n",
       " dict_keys(['privatize']),\n",
       " dict_keys(['graduate']),\n",
       " dict_keys(['imprisons']),\n",
       " dict_keys(['open']),\n",
       " dict_keys(['scores']),\n",
       " dict_keys(['paid']),\n",
       " dict_keys(['alameel']),\n",
       " dict_keys(['burns']),\n",
       " dict_keys(['port']),\n",
       " dict_keys(['sudafed']),\n",
       " dict_keys(['portlands']),\n",
       " dict_keys(['expire']),\n",
       " dict_keys(['pennsylvania']),\n",
       " dict_keys(['numerous']),\n",
       " dict_keys(['national']),\n",
       " dict_keys(['satisfied']),\n",
       " dict_keys(['driven']),\n",
       " dict_keys(['underwear']),\n",
       " dict_keys(['lower']),\n",
       " dict_keys(['rich']),\n",
       " dict_keys(['shootershould']),\n",
       " dict_keys(['okay']),\n",
       " dict_keys(['notion']),\n",
       " dict_keys(['christies']),\n",
       " dict_keys(['warren']),\n",
       " dict_keys(['lemieux']),\n",
       " dict_keys(['tally']),\n",
       " dict_keys(['unemployment']),\n",
       " dict_keys(['murnaghan']),\n",
       " dict_keys(['klein']),\n",
       " dict_keys(['trove']),\n",
       " dict_keys(['houstons']),\n",
       " dict_keys(['soetoro']),\n",
       " dict_keys(['vacations']),\n",
       " dict_keys(['infinite']),\n",
       " dict_keys(['staff']),\n",
       " dict_keys(['violence']),\n",
       " dict_keys(['questioning']),\n",
       " dict_keys(['legalizing']),\n",
       " dict_keys(['unintended']),\n",
       " dict_keys(['saudi']),\n",
       " dict_keys(['oswego']),\n",
       " dict_keys(['sector']),\n",
       " dict_keys(['revenue']),\n",
       " dict_keys(['four']),\n",
       " dict_keys(['anyone']),\n",
       " dict_keys(['dairies']),\n",
       " dict_keys(['same']),\n",
       " dict_keys(['nationwide']),\n",
       " dict_keys(['irresponsible']),\n",
       " dict_keys(['of']),\n",
       " dict_keys(['extremist']),\n",
       " dict_keys(['energy']),\n",
       " dict_keys(['amounted']),\n",
       " dict_keys(['skyrocketed']),\n",
       " dict_keys(['expand']),\n",
       " dict_keys(['belonged']),\n",
       " dict_keys(['infrastructure']),\n",
       " dict_keys(['noun']),\n",
       " dict_keys(['predatory']),\n",
       " dict_keys(['jobs']),\n",
       " dict_keys(['difficult']),\n",
       " dict_keys(['cbo']),\n",
       " dict_keys(['fairer']),\n",
       " dict_keys(['reserve']),\n",
       " dict_keys(['petroleum']),\n",
       " dict_keys(['fertile']),\n",
       " dict_keys(['will']),\n",
       " dict_keys(['mandates']),\n",
       " dict_keys(['homicides']),\n",
       " dict_keys(['abortion']),\n",
       " dict_keys(['pearl']),\n",
       " dict_keys(['rid']),\n",
       " dict_keys(['pill']),\n",
       " dict_keys(['soil']),\n",
       " dict_keys(['sorry']),\n",
       " dict_keys(['toldevery']),\n",
       " dict_keys(['inundated']),\n",
       " dict_keys(['locate']),\n",
       " dict_keys(['mcintyre']),\n",
       " dict_keys(['transfers']),\n",
       " dict_keys(['mechanisms']),\n",
       " dict_keys(['defunding']),\n",
       " dict_keys(['changes']),\n",
       " dict_keys(['bring']),\n",
       " dict_keys(['same']),\n",
       " dict_keys(['pockets']),\n",
       " dict_keys(['iv']),\n",
       " dict_keys(['drop']),\n",
       " dict_keys(['saluted']),\n",
       " dict_keys(['perquisites']),\n",
       " dict_keys(['airplane']),\n",
       " dict_keys(['affordable']),\n",
       " dict_keys(['afterschool']),\n",
       " dict_keys(['default']),\n",
       " dict_keys(['predatory']),\n",
       " dict_keys(['small']),\n",
       " dict_keys(['library']),\n",
       " dict_keys(['starbucks']),\n",
       " dict_keys(['footballs']),\n",
       " dict_keys(['birth']),\n",
       " dict_keys(['cleared']),\n",
       " dict_keys(['six']),\n",
       " dict_keys(['star']),\n",
       " dict_keys(['outsiders']),\n",
       " dict_keys(['created']),\n",
       " dict_keys(['lone']),\n",
       " dict_keys(['loan']),\n",
       " dict_keys(['disapproval']),\n",
       " dict_keys(['competitors']),\n",
       " dict_keys(['academic']),\n",
       " dict_keys(['million']),\n",
       " dict_keys(['mistakes']),\n",
       " dict_keys(['arkansas']),\n",
       " dict_keys(['train']),\n",
       " dict_keys(['jobs']),\n",
       " dict_keys(['times']),\n",
       " dict_keys(['income']),\n",
       " dict_keys(['covered']),\n",
       " dict_keys(['corporate']),\n",
       " dict_keys(['nationwide']),\n",
       " dict_keys(['scrutiny']),\n",
       " dict_keys(['class']),\n",
       " dict_keys(['jefferson']),\n",
       " dict_keys(['perks']),\n",
       " dict_keys(['logistically']),\n",
       " dict_keys(['americans']),\n",
       " dict_keys(['house']),\n",
       " dict_keys(['routinely']),\n",
       " dict_keys(['cell']),\n",
       " dict_keys(['savings']),\n",
       " dict_keys(['trimets']),\n",
       " dict_keys(['oyster']),\n",
       " dict_keys(['unwinding']),\n",
       " dict_keys(['imposed']),\n",
       " dict_keys(['camp']),\n",
       " dict_keys(['gop']),\n",
       " dict_keys(['country']),\n",
       " dict_keys(['syrias']),\n",
       " dict_keys(['postal']),\n",
       " dict_keys(['federal']),\n",
       " dict_keys(['hurting']),\n",
       " dict_keys(['representatives']),\n",
       " dict_keys(['wrinkly']),\n",
       " dict_keys(['searches']),\n",
       " dict_keys(['mine']),\n",
       " dict_keys(['taxed']),\n",
       " dict_keys(['itd']),\n",
       " dict_keys(['graduation']),\n",
       " dict_keys(['sold']),\n",
       " dict_keys(['deadliest']),\n",
       " dict_keys(['teachers']),\n",
       " dict_keys(['th']),\n",
       " dict_keys(['manages']),\n",
       " dict_keys(['trolley']),\n",
       " dict_keys(['results']),\n",
       " dict_keys(['repeal']),\n",
       " dict_keys(['pascrell']),\n",
       " dict_keys(['season']),\n",
       " dict_keys(['surrogates']),\n",
       " dict_keys(['offshore']),\n",
       " dict_keys(['get']),\n",
       " dict_keys(['microchip']),\n",
       " dict_keys(['carry']),\n",
       " dict_keys(['missed']),\n",
       " dict_keys(['treasurers']),\n",
       " dict_keys(['charter']),\n",
       " dict_keys(['jorge']),\n",
       " dict_keys(['overrule']),\n",
       " dict_keys(['ri']),\n",
       " dict_keys(['badly']),\n",
       " dict_keys(['almost']),\n",
       " dict_keys(['havent']),\n",
       " dict_keys(['kindergartners']),\n",
       " dict_keys(['known']),\n",
       " dict_keys(['accepting']),\n",
       " dict_keys(['scope']),\n",
       " dict_keys(['expanding']),\n",
       " dict_keys(['hassans']),\n",
       " dict_keys(['pottery']),\n",
       " dict_keys(['starts']),\n",
       " dict_keys(['saysrand']),\n",
       " dict_keys(['sums']),\n",
       " dict_keys(['mean']),\n",
       " dict_keys(['previous']),\n",
       " dict_keys(['qaida']),\n",
       " dict_keys(['jana']),\n",
       " dict_keys(['unmatched']),\n",
       " dict_keys(['negotiate']),\n",
       " dict_keys(['seems']),\n",
       " dict_keys(['scott']),\n",
       " dict_keys(['battlefield']),\n",
       " dict_keys(['packers']),\n",
       " dict_keys(['media']),\n",
       " dict_keys(['raise']),\n",
       " dict_keys(['industry']),\n",
       " dict_keys(['brand']),\n",
       " dict_keys(['groping']),\n",
       " dict_keys(['worker']),\n",
       " dict_keys(['ipab']),\n",
       " dict_keys(['watt']),\n",
       " dict_keys(['presidents']),\n",
       " dict_keys(['promote']),\n",
       " dict_keys(['nader']),\n",
       " dict_keys(['dan']),\n",
       " dict_keys(['committing']),\n",
       " dict_keys(['even']),\n",
       " dict_keys(['locks']),\n",
       " dict_keys(['reed']),\n",
       " dict_keys(['confirm']),\n",
       " dict_keys(['originally']),\n",
       " dict_keys(['payne']),\n",
       " dict_keys(['may']),\n",
       " dict_keys(['fingerprints']),\n",
       " dict_keys(['indeed']),\n",
       " dict_keys(['brags']),\n",
       " dict_keys(['crime']),\n",
       " dict_keys(['yeah']),\n",
       " dict_keys(['educating']),\n",
       " dict_keys(['omnibus']),\n",
       " dict_keys(['enacting']),\n",
       " dict_keys(['earmarks']),\n",
       " dict_keys(['relocated']),\n",
       " dict_keys(['adult']),\n",
       " dict_keys(['privilege']),\n",
       " dict_keys(['votes']),\n",
       " dict_keys(['estate']),\n",
       " dict_keys(['near']),\n",
       " dict_keys(['bailey']),\n",
       " dict_keys(['mica']),\n",
       " dict_keys(['bargained']),\n",
       " dict_keys(['hired']),\n",
       " dict_keys(['anybodys']),\n",
       " dict_keys(['lebanon']),\n",
       " dict_keys(['party']),\n",
       " dict_keys(['populist']),\n",
       " dict_keys(['drilling']),\n",
       " dict_keys(['convention']),\n",
       " dict_keys(['rising']),\n",
       " dict_keys(['paying']),\n",
       " dict_keys(['comply']),\n",
       " dict_keys(['prayer']),\n",
       " dict_keys(['abolishing']),\n",
       " dict_keys(['weapon']),\n",
       " dict_keys(['motors']),\n",
       " dict_keys(['uri']),\n",
       " dict_keys(['curbelo']),\n",
       " dict_keys(['suspicion']),\n",
       " dict_keys(['usa']),\n",
       " dict_keys(['billionaire']),\n",
       " dict_keys(['trump']),\n",
       " dict_keys(['borrow']),\n",
       " dict_keys(['medicare']),\n",
       " dict_keys(['domes']),\n",
       " dict_keys(['youd']),\n",
       " dict_keys(['severity']),\n",
       " dict_keys(['camden']),\n",
       " dict_keys(['delinquent']),\n",
       " dict_keys(['traditions']),\n",
       " dict_keys(['jobless']),\n",
       " dict_keys(['biggest']),\n",
       " dict_keys(['cracked']),\n",
       " dict_keys(['support']),\n",
       " dict_keys(['recipient']),\n",
       " dict_keys(['thanks']),\n",
       " dict_keys(['awarded']),\n",
       " dict_keys(['adjusting']),\n",
       " dict_keys(['fallen']),\n",
       " dict_keys(['reform']),\n",
       " dict_keys(['welfare']),\n",
       " dict_keys(['bisexual']),\n",
       " dict_keys(['thibaut']),\n",
       " dict_keys(['slavery']),\n",
       " dict_keys(['ridership']),\n",
       " dict_keys(['via']),\n",
       " dict_keys(['void']),\n",
       " dict_keys(['im']),\n",
       " dict_keys(['albany']),\n",
       " dict_keys(['amazing']),\n",
       " dict_keys(['rebecca']),\n",
       " dict_keys(['fails']),\n",
       " dict_keys(['implement']),\n",
       " dict_keys(['machado']),\n",
       " dict_keys(['aphotograph']),\n",
       " dict_keys(['leads']),\n",
       " dict_keys(['systematically']),\n",
       " dict_keys(['college']),\n",
       " dict_keys(['booker']),\n",
       " dict_keys(['release']),\n",
       " dict_keys(['eu']),\n",
       " dict_keys(['asking']),\n",
       " dict_keys(['ambitions']),\n",
       " dict_keys(['dnc']),\n",
       " dict_keys(['jets']),\n",
       " dict_keys(['zack']),\n",
       " dict_keys(['ends']),\n",
       " dict_keys(['subcommittee']),\n",
       " dict_keys(['hampshire']),\n",
       " dict_keys(['for']),\n",
       " dict_keys(['morgancarroll']),\n",
       " dict_keys(['muslim']),\n",
       " dict_keys(['creators']),\n",
       " dict_keys(['wanted']),\n",
       " dict_keys(['school']),\n",
       " dict_keys(['intervene']),\n",
       " dict_keys(['cleveland']),\n",
       " dict_keys(['kitzhaber']),\n",
       " dict_keys(['pupil']),\n",
       " dict_keys(['interviewers']),\n",
       " dict_keys(['your']),\n",
       " dict_keys(['immigrants']),\n",
       " dict_keys(['florida']),\n",
       " dict_keys(['family']),\n",
       " dict_keys(['gallup']),\n",
       " dict_keys(['crime']),\n",
       " dict_keys(['maddox']),\n",
       " dict_keys(['preparation']),\n",
       " dict_keys(['managed']),\n",
       " dict_keys(['independent']),\n",
       " dict_keys(['pocketed']),\n",
       " dict_keys(['insurance']),\n",
       " dict_keys(['purposely']),\n",
       " dict_keys(['read']),\n",
       " dict_keys(['cuyahoga']),\n",
       " dict_keys(['critical']),\n",
       " dict_keys(['car']),\n",
       " dict_keys(['sayspeter']),\n",
       " dict_keys(['woodruff']),\n",
       " dict_keys(['penny']),\n",
       " dict_keys(['probation']),\n",
       " dict_keys(['increase']),\n",
       " dict_keys(['lost']),\n",
       " dict_keys(['ceiling']),\n",
       " dict_keys(['limiting']),\n",
       " dict_keys(['udalls']),\n",
       " dict_keys(['defined']),\n",
       " dict_keys(['valle']),\n",
       " dict_keys(['levees']),\n",
       " dict_keys(['too']),\n",
       " dict_keys(['trillion']),\n",
       " dict_keys(['increase']),\n",
       " dict_keys(['prohibiting']),\n",
       " dict_keys(['whatis']),\n",
       " dict_keys(['making']),\n",
       " dict_keys(['gun']),\n",
       " dict_keys(['misadventures']),\n",
       " dict_keys(['remind']),\n",
       " dict_keys(['wage']),\n",
       " dict_keys(['adds']),\n",
       " dict_keys(['declining']),\n",
       " dict_keys(['positions']),\n",
       " dict_keys(['acts']),\n",
       " dict_keys(['stood']),\n",
       " dict_keys(['best']),\n",
       " dict_keys(['flying']),\n",
       " dict_keys(['gingrich']),\n",
       " dict_keys(['angel']),\n",
       " dict_keys(['maurice']),\n",
       " dict_keys(['advanced']),\n",
       " dict_keys(['inmates']),\n",
       " dict_keys(['midwestern']),\n",
       " dict_keys(['teamster']),\n",
       " dict_keys(['suspend']),\n",
       " dict_keys(['cigars']),\n",
       " dict_keys(['prices']),\n",
       " dict_keys(['smoking']),\n",
       " dict_keys(['nursing']),\n",
       " dict_keys(['unique']),\n",
       " dict_keys(['staffer']),\n",
       " dict_keys(['participating']),\n",
       " dict_keys(['shared']),\n",
       " dict_keys(['hagee']),\n",
       " dict_keys(['filers']),\n",
       " dict_keys(['stock']),\n",
       " dict_keys(['decades']),\n",
       " dict_keys(['rival']),\n",
       " dict_keys(['elected']),\n",
       " dict_keys(['gaylord']),\n",
       " dict_keys(['play']),\n",
       " dict_keys(['interview']),\n",
       " dict_keys(['lose']),\n",
       " dict_keys(['financing']),\n",
       " dict_keys(['schools']),\n",
       " dict_keys(['embryonic']),\n",
       " dict_keys(['military']),\n",
       " dict_keys(['recount']),\n",
       " dict_keys(['christopher']),\n",
       " dict_keys(['care']),\n",
       " dict_keys(['towards']),\n",
       " dict_keys(['switching']),\n",
       " dict_keys(['synthetic']),\n",
       " dict_keys(['incompetence']),\n",
       " dict_keys(['longest']),\n",
       " dict_keys(['millions']),\n",
       " dict_keys(['debt']),\n",
       " dict_keys(['hani']),\n",
       " dict_keys(['precipitation']),\n",
       " dict_keys(['yes']),\n",
       " dict_keys(['submitted']),\n",
       " dict_keys(['update']),\n",
       " dict_keys(['vietnamese']),\n",
       " dict_keys(['opinion']),\n",
       " dict_keys(['referendum']),\n",
       " dict_keys(['thesis']),\n",
       " dict_keys(['southerners']),\n",
       " dict_keys(['triggering']),\n",
       " dict_keys(['opening']),\n",
       " dict_keys(['dane']),\n",
       " dict_keys(['legislators']),\n",
       " dict_keys(['fiorina']),\n",
       " dict_keys(['multiplied']),\n",
       " dict_keys(['platform']),\n",
       " dict_keys(['crime']),\n",
       " dict_keys(['hybrid']),\n",
       " dict_keys(['security']),\n",
       " dict_keys(['debt']),\n",
       " dict_keys(['scoreboard']),\n",
       " dict_keys(['internment']),\n",
       " dict_keys(['embryonic']),\n",
       " dict_keys(['intact']),\n",
       " dict_keys(['turkey']),\n",
       " dict_keys(['bid']),\n",
       " dict_keys(['dying']),\n",
       " dict_keys(['leffingwell']),\n",
       " dict_keys(['contradictory']),\n",
       " dict_keys(['losses']),\n",
       " dict_keys(['belonged']),\n",
       " dict_keys(['assassination']),\n",
       " dict_keys(['department']),\n",
       " dict_keys(['bunked']),\n",
       " dict_keys(['monitor']),\n",
       " dict_keys(['vaccinated']),\n",
       " dict_keys(['flying']),\n",
       " dict_keys(['class']),\n",
       " dict_keys(['gains']),\n",
       " dict_keys(['electorate']),\n",
       " dict_keys(['information']),\n",
       " dict_keys(['sic']),\n",
       " dict_keys(['we']),\n",
       " dict_keys(['shoe']),\n",
       " dict_keys(['toilets']),\n",
       " dict_keys(['renacci']),\n",
       " dict_keys(['liebermans']),\n",
       " dict_keys(['expenses']),\n",
       " dict_keys(['powers']),\n",
       " dict_keys(['roughly']),\n",
       " dict_keys(['ceos']),\n",
       " dict_keys(['delano']),\n",
       " dict_keys(['dreamers']),\n",
       " dict_keys(['vetting']),\n",
       " dict_keys(['conservatives']),\n",
       " dict_keys(['lawmaker']),\n",
       " dict_keys(['racked']),\n",
       " dict_keys(['strauss']),\n",
       " dict_keys(['reformers']),\n",
       " dict_keys(['top']),\n",
       " dict_keys(['issue']),\n",
       " dict_keys(['twice']),\n",
       " dict_keys(['storms']),\n",
       " dict_keys(['thousandfold']),\n",
       " dict_keys(['income']),\n",
       " dict_keys(['quarter']),\n",
       " dict_keys(['iran']),\n",
       " dict_keys(['ohios']),\n",
       " dict_keys(['troop']),\n",
       " dict_keys(['induced']),\n",
       " dict_keys(['backlog']),\n",
       " dict_keys(['molesting']),\n",
       " dict_keys(['mccrory']),\n",
       " dict_keys(['blueprint']),\n",
       " dict_keys(['affecting']),\n",
       " dict_keys(['especially']),\n",
       " dict_keys(['subscribers']),\n",
       " dict_keys(['businesses']),\n",
       " dict_keys(['bores']),\n",
       " dict_keys(['spend']),\n",
       " dict_keys(['does']),\n",
       " dict_keys(['abortion']),\n",
       " dict_keys(['serving']),\n",
       " dict_keys(['established']),\n",
       " dict_keys(['notably']),\n",
       " dict_keys(['spouse']),\n",
       " dict_keys(['smaller']),\n",
       " dict_keys(['sleep']),\n",
       " dict_keys(['hed']),\n",
       " dict_keys(['latina']),\n",
       " dict_keys(['brewery']),\n",
       " dict_keys(['threshold']),\n",
       " dict_keys(['invested']),\n",
       " dict_keys(['los']),\n",
       " dict_keys(['jobs']),\n",
       " dict_keys(['upset']),\n",
       " dict_keys(['carcieri']),\n",
       " dict_keys(['deployed']),\n",
       " dict_keys(['district']),\n",
       " dict_keys(['saysvirginia']),\n",
       " dict_keys(['settled']),\n",
       " dict_keys(['you']),\n",
       " dict_keys(['narcotics']),\n",
       " dict_keys(['appalled']),\n",
       " dict_keys(['line']),\n",
       " dict_keys(['potentially']),\n",
       " dict_keys(['telecommunications']),\n",
       " dict_keys(['admit']),\n",
       " dict_keys(['standard']),\n",
       " dict_keys(['wisconsinites']),\n",
       " dict_keys(['french']),\n",
       " dict_keys(['lower']),\n",
       " dict_keys(['farmers']),\n",
       " dict_keys(['loretta']),\n",
       " dict_keys(['hate']),\n",
       " dict_keys(['moving']),\n",
       " dict_keys(['accelerated']),\n",
       " dict_keys(['squat']),\n",
       " dict_keys(['outpaces']),\n",
       " dict_keys(['debt']),\n",
       " dict_keys(['minute']),\n",
       " dict_keys(['gogebic']),\n",
       " dict_keys(['denish']),\n",
       " dict_keys(['youre']),\n",
       " dict_keys(['diesel']),\n",
       " dict_keys(['brave']),\n",
       " dict_keys(['vermont']),\n",
       " dict_keys(['duties']),\n",
       " dict_keys(['corporate']),\n",
       " dict_keys(['referring']),\n",
       " dict_keys(['recouped']),\n",
       " dict_keys(['treated']),\n",
       " dict_keys(['game']),\n",
       " dict_keys(['duvalier']),\n",
       " dict_keys(['solar']),\n",
       " dict_keys(['acceptance']),\n",
       " dict_keys(['border']),\n",
       " dict_keys(['ive']),\n",
       " dict_keys(['passenger']),\n",
       " dict_keys(['egyptians']),\n",
       " dict_keys(['earth']),\n",
       " dict_keys(['follows']),\n",
       " dict_keys(['columbia']),\n",
       " dict_keys(['waive']),\n",
       " dict_keys(['said']),\n",
       " dict_keys(['experts']),\n",
       " dict_keys(['may']),\n",
       " dict_keys(['lobby']),\n",
       " dict_keys(['parallel']),\n",
       " dict_keys(['notifications']),\n",
       " dict_keys(['battered']),\n",
       " dict_keys(['aircraft']),\n",
       " dict_keys(['compliance']),\n",
       " dict_keys(['averagein']),\n",
       " dict_keys(['retrofitting']),\n",
       " dict_keys(['fur']),\n",
       " dict_keys(['highest']),\n",
       " dict_keys(['herald']),\n",
       " dict_keys(['coopers']),\n",
       " dict_keys(['clinton']),\n",
       " dict_keys(['periods']),\n",
       " dict_keys(['several']),\n",
       " dict_keys(['boss']),\n",
       " dict_keys(['babies']),\n",
       " dict_keys(['unemployment']),\n",
       " dict_keys(['disarm']),\n",
       " dict_keys(['moratorium']),\n",
       " dict_keys(['milwaukees']),\n",
       " dict_keys(['trimming']),\n",
       " dict_keys(['gatherings']),\n",
       " dict_keys(['reid']),\n",
       " dict_keys(['pace']),\n",
       " dict_keys(['rubios']),\n",
       " dict_keys(['increasing']),\n",
       " dict_keys(['corleone']),\n",
       " dict_keys(['incumbents']),\n",
       " dict_keys(['ricos']),\n",
       " dict_keys(['state']),\n",
       " dict_keys(['cents']),\n",
       " dict_keys(['republican']),\n",
       " dict_keys(['competitive']),\n",
       " dict_keys(['john']),\n",
       " dict_keys(['dissenter']),\n",
       " dict_keys(['inflationary']),\n",
       " dict_keys(['subway']),\n",
       " dict_keys(['delivered']),\n",
       " dict_keys(['lamar']),\n",
       " dict_keys(['mizzou']),\n",
       " dict_keys(['waived']),\n",
       " dict_keys(['works']),\n",
       " dict_keys(['warner']),\n",
       " dict_keys(['incumbent']),\n",
       " dict_keys(['palestine']),\n",
       " dict_keys(['pay']),\n",
       " dict_keys(['increasingly']),\n",
       " dict_keys(['spray']),\n",
       " dict_keys(['presidency']),\n",
       " dict_keys(['northeastern']),\n",
       " dict_keys(['done']),\n",
       " dict_keys(['diane']),\n",
       " dict_keys(['destructive']),\n",
       " dict_keys(['counties']),\n",
       " dict_keys(['bipartisan']),\n",
       " dict_keys(['correctional']),\n",
       " dict_keys(['commander']),\n",
       " dict_keys(['gdp']),\n",
       " dict_keys(['ganley']),\n",
       " dict_keys(['toomey']),\n",
       " dict_keys(['marijuana']),\n",
       " dict_keys(['alive']),\n",
       " dict_keys(['trump']),\n",
       " dict_keys(['relationships']),\n",
       " dict_keys(['subsidy']),\n",
       " dict_keys(['lamp']),\n",
       " dict_keys(['disclose']),\n",
       " dict_keys(['growth']),\n",
       " dict_keys(['howell']),\n",
       " dict_keys(['moderate']),\n",
       " dict_keys(['medicare']),\n",
       " dict_keys(['deleted']),\n",
       " dict_keys(['tiny']),\n",
       " dict_keys(['boxers']),\n",
       " dict_keys(['entitlements']),\n",
       " dict_keys(['want']),\n",
       " dict_keys(['taxes']),\n",
       " dict_keys(['garland']),\n",
       " dict_keys(['incumbent']),\n",
       " dict_keys(['letting']),\n",
       " dict_keys(['bear']),\n",
       " dict_keys(['jfk']),\n",
       " dict_keys(['disenfranchised']),\n",
       " dict_keys(['bomb']),\n",
       " dict_keys(['consult']),\n",
       " dict_keys(['except']),\n",
       " dict_keys(['newsom']),\n",
       " dict_keys(['overseen']),\n",
       " dict_keys(['bailouts']),\n",
       " dict_keys(['recommends']),\n",
       " dict_keys(['rocap']),\n",
       " dict_keys(['jerusalem']),\n",
       " dict_keys(['draft']),\n",
       " dict_keys(['eisenhower']),\n",
       " dict_keys(['rate']),\n",
       " dict_keys(['senior']),\n",
       " dict_keys(['officers']),\n",
       " dict_keys(['recommending']),\n",
       " dict_keys(['staircase']),\n",
       " dict_keys(['turnout']),\n",
       " dict_keys(['about']),\n",
       " dict_keys(['lives']),\n",
       " dict_keys(['privilege']),\n",
       " dict_keys(['gration']),\n",
       " dict_keys(['pretrial']),\n",
       " dict_keys(['initials']),\n",
       " dict_keys(['characteristic']),\n",
       " dict_keys(['club']),\n",
       " dict_keys(['lt']),\n",
       " dict_keys(['parked']),\n",
       " dict_keys(['unsealed']),\n",
       " dict_keys(['constituent']),\n",
       " dict_keys(['venezuela']),\n",
       " dict_keys(['wildfires']),\n",
       " dict_keys(['citizens']),\n",
       " dict_keys(['depetro']),\n",
       " dict_keys(['sprawling']),\n",
       " dict_keys(['states']),\n",
       " dict_keys(['noncitizens']),\n",
       " dict_keys(['troops']),\n",
       " dict_keys(['earned']),\n",
       " dict_keys(['cantors']),\n",
       " dict_keys(['turbine']),\n",
       " dict_keys(['clears']),\n",
       " dict_keys(['condemned']),\n",
       " dict_keys(['graduation']),\n",
       " dict_keys(['theocracy']),\n",
       " dict_keys(['saying']),\n",
       " dict_keys(['boundaries']),\n",
       " dict_keys(['economies']),\n",
       " dict_keys(['helicopters']),\n",
       " dict_keys(['sherrod']),\n",
       " dict_keys(['barretts']),\n",
       " dict_keys(['whoomp']),\n",
       " dict_keys(['concealed']),\n",
       " dict_keys(['uninterrupted']),\n",
       " dict_keys(['his']),\n",
       " dict_keys(['franchised']),\n",
       " dict_keys(['autism']),\n",
       " dict_keys(['wisconsinites']),\n",
       " dict_keys(['position']),\n",
       " dict_keys(['shoots']),\n",
       " dict_keys(['streetcar']),\n",
       " dict_keys(['bluest']),\n",
       " dict_keys(['residential']),\n",
       " dict_keys(['trumpets']),\n",
       " dict_keys(['consecutive']),\n",
       " dict_keys(['coach']),\n",
       " dict_keys(['lobbyists']),\n",
       " dict_keys(['expanding']),\n",
       " dict_keys(['remains']),\n",
       " dict_keys(['farming']),\n",
       " dict_keys(['alcohol']),\n",
       " dict_keys(['scan']),\n",
       " dict_keys(['xl']),\n",
       " dict_keys(['gay']),\n",
       " dict_keys(['speech']),\n",
       " dict_keys(['guest']),\n",
       " dict_keys(['participate']),\n",
       " dict_keys(['care']),\n",
       " dict_keys(['present']),\n",
       " dict_keys(['spankers']),\n",
       " dict_keys(['taxed']),\n",
       " dict_keys(['flores']),\n",
       " dict_keys(['michelle']),\n",
       " dict_keys(['doubled']),\n",
       " dict_keys(['govt']),\n",
       " dict_keys(['exceeded']),\n",
       " dict_keys(['medicine']),\n",
       " dict_keys(['assumption']),\n",
       " dict_keys(['siphon']),\n",
       " dict_keys(['islamabad']),\n",
       " dict_keys(['thing']),\n",
       " dict_keys(['unnecessarily']),\n",
       " dict_keys(['bail']),\n",
       " dict_keys(['ponzi']),\n",
       " dict_keys(['costly']),\n",
       " dict_keys(['ever']),\n",
       " dict_keys(['planet']),\n",
       " dict_keys(['fines']),\n",
       " dict_keys(['iron']),\n",
       " dict_keys(['rating']),\n",
       " dict_keys(['asthma']),\n",
       " dict_keys(['century']),\n",
       " dict_keys(['loaning']),\n",
       " dict_keys(['making']),\n",
       " dict_keys(['players']),\n",
       " dict_keys(['pending']),\n",
       " dict_keys(['victors']),\n",
       " dict_keys(['handpicked']),\n",
       " dict_keys(['owns']),\n",
       " dict_keys(['sununu']),\n",
       " dict_keys(['gonna']),\n",
       " dict_keys(['cutbacks']),\n",
       " dict_keys(['defer']),\n",
       " dict_keys(['rises']),\n",
       " dict_keys(['complying']),\n",
       " dict_keys(['shuttle']),\n",
       " dict_keys(['isnt']),\n",
       " dict_keys(['rank']),\n",
       " dict_keys(['fatalities']),\n",
       " dict_keys(['nidal']),\n",
       " dict_keys(['contribution']),\n",
       " dict_keys(['wwe']),\n",
       " dict_keys(['latin']),\n",
       " dict_keys(['stamps']),\n",
       " dict_keys(['six']),\n",
       " dict_keys(['miamis']),\n",
       " dict_keys(['average']),\n",
       " dict_keys(['experienced']),\n",
       " dict_keys(['job']),\n",
       " dict_keys(['pulling']),\n",
       " dict_keys(['bidder']),\n",
       " dict_keys(['rug']),\n",
       " dict_keys(['engage']),\n",
       " dict_keys(['pledge']),\n",
       " dict_keys(['risen']),\n",
       " dict_keys(['ribble']),\n",
       " dict_keys(['injecting']),\n",
       " dict_keys(['pride']),\n",
       " dict_keys(['social']),\n",
       " dict_keys(['containment']),\n",
       " dict_keys(['stamped']),\n",
       " dict_keys(['leonard']),\n",
       " dict_keys(['curbside']),\n",
       " ...]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=[]\n",
    "for i in range(0, len(mydat)):\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([mydat[i]]))\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    keywords=extract_topn_from_vector(feature_name,sorted_items,1)\n",
    "    res.append(keywords)\n",
    "\n",
    "[word.keys() for word in res]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
