{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# pos tagging for fake news statement\n",
    "## use pos-tagging to build features\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import libraries\n",
    "\n",
    "import util\n",
    "from nlp_util import NLP_Task\n",
    "import preprocessing\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "tr_file, va_file, te_file = util.load_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n"
     ]
    }
   ],
   "source": [
    "nlpTask = NLP_Task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dict = util.tsv_to_dict(tsv_file=tr_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_dict = util.tsv_to_dict(tsv_file=va_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10240"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_dict['statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=set(stopwords.words('english'))\n",
    "text_no_stops=[]\n",
    "for elm in range(0, len(tr_dict['statement'])):\n",
    "    res=' '.join([i for i in tr_dict['statement'][elm].lower().split() if i not in stop])\n",
    "    text_no_stops.append(res)\n",
    "\n",
    "## remove the punctuation\n",
    "def rm_punct(sentence):\n",
    "    flushed_punct = set(string.punctuation)\n",
    "    res = ''.join(x for x in sentence if x not in flushed_punct)\n",
    "    return res\n",
    "\n",
    "text_no_punct=[]\n",
    "for i in range(0, len(tr_dict['statement'])):\n",
    "    res=rm_punct(tr_dict['statement'][i])\n",
    "    text_no_punct.append(res)\n",
    "\n",
    "## here is how to clean up non-letter symbols in statement columns\n",
    "all_text = []\n",
    "for i in range(0, len(tr_dict['statement'])):\n",
    "    patt = re.sub('[^a-zA-Z]', ' ', tr_dict['statement'][i])\n",
    "    res = ' '.join(str(patt).lower().split())\n",
    "    all_text.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for statement in tr_dict['statement']:\n",
    "    review = re.sub('[^a-zA-Z]',' ',statement)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = ''.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'D:\\\\UIC\\\\Fall 2018\\\\Statistical NLP\\\\Project\\\\jurat-aldo-project\\\\cs-521-project.git\\\\source\\\\CS-521-PROJECT\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n",
      "Extracting POS Tags\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "unigram_pos, bigrams_pos, trigram_pos = preprocessing.extract_POS(tr_dict['statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_list =list()\n",
    "for pos in unigram_pos:\n",
    "    for p in pos:\n",
    "        if p not in unigram_list:\n",
    "            unigram_list.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_list[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n",
      "Extracting POS Tags\n",
      "Stringify\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#unigram_pos_va, bigrams_pos_va, trigram_pos_va = preprocessing.extract_POS(statements=va_dict['statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_no_punct_va=[]\n",
    "for i in range(0, len(va_dict['statement'])):\n",
    "    res=rm_punct(va_dict['statement'][i])\n",
    "    text_no_punct_va.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n",
      "Extracting POS Tags\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "unigram_pos_va, bigrams_pos_va, trigram_pos_va = preprocessing.extract_POS(statements=va_dict['statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labels_tr = np.array(preprocessing.create_labels(labels=tr_dict['label'],label_values={'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}))\n",
    "binary_labels_va = np.array(preprocessing.create_labels(labels=va_dict['label'],label_values={'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use countVectorizor to build postag output as features for training ML model\n",
    "## uni_vocabulary: vocabulary=['cd','jjr','vb','jjs','nnp','in','vbp','to','rb','vbg','md','jj','dt']\n",
    "## bi_vocabulary: vocabulary=['NNP NNP', 'IN DT', 'JJR IN', 'CD NN', 'IN CD', 'CD NNS', 'DT JJS']\n",
    "## tri_vocabulary: ['NNP NNP NNP','CD NN IN','VBZ NNP NNP','IN DT NN','IN DT JJ','NN IN DT','<s> VBZ NNP','JJR IN CD','NNP NNP VBD','DT JJ CD','NNS IN DT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_pos_str = [\" \".join(x) for x in unigram_pos]\n",
    "bigram_pos_str = [\" \".join(x) for x in bigrams_pos]\n",
    "trigram_pos_str = [\" \".join(x) for x in trigram_pos]\n",
    "## use countVectorizor to build postag output as features for training ML model\n",
    "#['cd','jjr','vb','jjs','nnp','in','vbp','to','rb','vbg','md','jj','dt']\n",
    "cv_uni = CountVectorizer()\n",
    "#cv = CountVectorizer()\n",
    "pos_uni_feats = cv_uni.fit_transform(unigram_pos_str).toarray()\n",
    "#pos_big_feats = cv.fit_transform(bigram_pos_str).toarray()\n",
    "#pos_trig_feats = cv.fit_transform(trigram_pos_str).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_uni.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_pos_va_str = [\" \".join(x) for x in unigram_pos_va]\n",
    "bigram_pos_va_str = [\" \".join(x) for x in bigrams_pos_va]\n",
    "trigram_pos_va_str = [\" \".join(x) for x in trigram_pos_va]\n",
    "## use countVectorizor to build postag output as features for training ML model\n",
    "cv_va = CountVectorizer(vocabulary=cv_uni.get_feature_names())\n",
    "pos_uni_feats_va = cv_va.fit_transform(unigram_pos_va_str).toarray()\n",
    "#pos_big_feats_va = cv.fit_transform(bigram_pos_va_str).toarray()\n",
    "#pos_trig_feats_va = cv.fit_transform(trigram_pos_va_str).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_va.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 8126 is out of bounds for axis 0 with size 8126",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-f04a5dfe3c2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtr_indexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_labels_tr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mva_indexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_labels_va\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_uni_feats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtr_indexes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fit does nothing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpos_uni_feats\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_uni_feats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtr_indexes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# preprocessing.pos_vectors(['CD','JJR','VB','JJS','NNP','IN','VBP','TO','RB','VBG','MD','JJ','DT'],unigram_pos)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtransformer_va\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_uni_feats_va\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mva_indexes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fit does nothing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 8126 is out of bounds for axis 0 with size 8126"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "transformer = Binarizer().fit(pos_uni_feats[tr_indexes]) # fit does nothing.\n",
    "pos_uni_feats =  transformer.transform(pos_uni_feats[tr_indexes])# preprocessing.pos_vectors(['CD','JJR','VB','JJS','NNP','IN','VBP','TO','RB','VBG','MD','JJ','DT'],unigram_pos)\n",
    "transformer_va = Binarizer().fit(pos_uni_feats_va[va_indexes]) # fit does nothing.\n",
    "pos_uni_feats_va = transformer.transform(pos_uni_feats_va[va_indexes])# preprocessing.pos_vectors(['CD','JJR','VB','JJS','NNP','IN','VBP','TO','RB','VBG','MD','JJ','DT'], unigram_pos_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_indexes = [i for i,x in enumerate(binary_labels_tr) if x!=0]\n",
    "va_indexes = [i for i,x in enumerate(binary_labels_va) if x!=0]\n",
    "X_train = pos_uni_feats[tr_indexes]\n",
    "X_test =  pos_uni_feats_va[va_indexes]\n",
    "y_train = binary_labels_tr[tr_indexes]\n",
    "y_test = binary_labels_va[va_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using own version of pos vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing POS tags and creating vectors\n",
      "Finished\n",
      "Processing POS tags and creating vectors\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "tr_indexes = [i for i,x in enumerate(binary_labels_tr) if x!=0]\n",
    "va_indexes = [i for i,x in enumerate(binary_labels_va) if x!=0]\n",
    "#transformer = Binarizer().fit(pos_uni_feats[tr_indexes]) # fit does nothing.\n",
    "pos_uni_feats = preprocessing.pos_vectors(unigram_list[1:],unigram_pos)\n",
    "pos_uni_feats_va = preprocessing.pos_vectors(unigram_list[1:], unigram_pos_va)\n",
    "X_train = pos_uni_feats[tr_indexes]\n",
    "X_test =  pos_uni_feats_va[va_indexes]\n",
    "y_train = binary_labels_tr[tr_indexes]\n",
    "y_test = binary_labels_va[va_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6283783783783784\n",
      "[0.58653846 0.53846154 0.56730769 0.625      0.60576923 0.625\n",
      " 0.6407767  0.59223301 0.66019417 0.66019417]\n"
     ]
    }
   ],
   "source": [
    "m_clf = DecisionTreeClassifier(max_depth=5,criterion='entropy')\n",
    "m_clf.fit(X_train, y_train)\n",
    "predicted = m_clf.predict(X_test)\n",
    "print(m_clf.score(X=X_test, y=y_test))\n",
    "print(cross_val_score(m_clf, X_test, y_test, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_indexes = [i for i,x in enumerate(binary_labels_tr) if x!=0]\n",
    "va_indexes = [i for i,x in enumerate(binary_labels_va) if x!=0]\n",
    "X_train = pos_uni_feats[tr_indexes]\n",
    "X_test =  pos_uni_feats_va[va_indexes]\n",
    "y_train = binary_labels_tr[tr_indexes]\n",
    "y_test = binary_labels_va[va_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Different Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of LogisticRegression: 0.6236486486486488\n",
      "accuracy metrics for LogisticRegression:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.55      0.41      0.47       420\n",
      "          1       0.66      0.78      0.71       616\n",
      "\n",
      "avg / total       0.62      0.63      0.61      1036\n",
      "\n",
      "Accuracy score of SGDClassifier: 0.6214285714285716\n",
      "accuracy metrics for SGDClassifier:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.58      0.33      0.42       420\n",
      "          1       0.65      0.83      0.73       616\n",
      "\n",
      "avg / total       0.62      0.63      0.60      1036\n",
      "\n",
      "Accuracy score of MultinomialNB: 0.6219111969111969\n",
      "accuracy metrics for MultinomialNB:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.54      0.53      0.53       420\n",
      "          1       0.68      0.69      0.68       616\n",
      "\n",
      "avg / total       0.62      0.62      0.62      1036\n",
      "\n",
      "Accuracy score of RandomForestClassifier: 0.5615830115830116\n",
      "accuracy metrics for RandomForestClassifier:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.45      0.56      0.50       420\n",
      "          1       0.64      0.54      0.58       616\n",
      "\n",
      "avg / total       0.57      0.55      0.55      1036\n",
      "\n",
      "Accuracy score of SVC: 0.6227799227799228\n",
      "accuracy metrics for SVC:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.56      0.39      0.46       420\n",
      "          1       0.66      0.79      0.72       616\n",
      "\n",
      "avg / total       0.62      0.63      0.61      1036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Need to Provide Xtrain, Ytrain, Xtest, Ytest\n",
    "Xtrain = X_train\n",
    "Ytrain = y_train\n",
    "Xtest = X_test\n",
    "Ytest = y_test\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "# ,'class_weight':{1:.9, 2:.5, 3:.01}\n",
    "params = [{}, {'loss': 'log', 'penalty': 'l2', 'max_iter':1000},{},{},{}]\n",
    "Models = [LogisticRegression, linear_model.SGDClassifier, MultinomialNB, RandomForestClassifier, SVC]\n",
    "accuracy_list = list()\n",
    "accuracy_metrics = list()\n",
    "for param, Model in zip(params, Models):\n",
    "    total = 0\n",
    "    for train_indices, test_indices in kf.split(X_train):\n",
    "        train_X = Xtrain[train_indices, :]; train_Y = Ytrain[train_indices]\n",
    "        test_X = Xtrain[test_indices, :]; test_Y = Ytrain[test_indices]\n",
    "        reg = Model(**param)\n",
    "        reg.fit(train_X, train_Y)\n",
    "        predictions = reg.predict(Xtest)\n",
    "        total += accuracy_score(Ytest, predictions)\n",
    "    \n",
    "    accuracy = total / kf.n_splits\n",
    "    reg = Model(**param)\n",
    "    reg.fit(X_train, y_train)\n",
    "    predictions = reg.predict(X_test)\n",
    "    accuracy_list.append((Model.__name__, accuracy))\n",
    "    accuracy_metrics.append((Model.__name__, classification_report(y_test, predictions)))\n",
    "    \n",
    "for i, value in enumerate(accuracy_list):\n",
    "    print(\"Accuracy score of {0}: {1}\".format(value[0],value[1]))\n",
    "    print(\"accuracy metrics for {0}:\\n{1}\".format(accuracy_metrics[i][0], accuracy_metrics[i][1]))\n",
    "    \n",
    "#print(\"Accuracy score of {0}: {1}: {2}: {3}: {4}\".format(accuracy_list[0], accuracy_list[]))\n",
    "#print(\"accuracy metrics for {0}: {1}: {2}: {3}: {4}\".format(Model.__name__, classification_report(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
