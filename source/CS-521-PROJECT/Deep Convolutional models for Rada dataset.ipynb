{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model for small sized fake news data (Rada data)\n",
    "\n",
    "data source is taken from Rada paper: http://web.eecs.umich.edu/~mihalcea/downloads/fakeNewsDatasets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os,sys\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## access dataset\n",
    "for root, dirs, files in os.walk(\"celebrityDataset/\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            allfile=os.path.join(root, file)\n",
    "            print(allfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rename file extension from given fakenews dataset (Rada data)\n",
    "folder1 = 'fakeNewsDataset/fake/'\n",
    "for filename in os.listdir(folder1):\n",
    "    infilename = os.path.join(folder1,filename)\n",
    "    if not os.path.isfile(infilename): \n",
    "        continue\n",
    "    oldbase = os.path.splitext(filename)\n",
    "    newname = infilename.replace('.fake.txt', '.txt')\n",
    "    output = os.rename(infilename, newname)\n",
    "    \n",
    "## rename file extension in legit news\n",
    "folder2 = 'fakeNewsDataset/legit/'\n",
    "for filename in os.listdir(folder2):\n",
    "    infilename = os.path.join(folder2,filename)\n",
    "    if not os.path.isfile(infilename): \n",
    "        continue\n",
    "    oldbase = os.path.splitext(filename)\n",
    "    newname = infilename.replace('.legit.txt', '.txt')\n",
    "    output = os.rename(infilename, newname)\n",
    "    \n",
    "## rename file extension in celebreity dataset\n",
    "folder3 = 'celebrityDataset/fake/'\n",
    "for filename in os.listdir(folder3):\n",
    "    infilename = os.path.join(folder3,filename)\n",
    "    if not os.path.isfile(infilename): \n",
    "        continue\n",
    "    oldbase = os.path.splitext(filename)\n",
    "    newname = infilename.replace('fake.txt', '.txt')\n",
    "    output = os.rename(infilename, newname)\n",
    "    \n",
    "folder4 = 'celebrityDataset/legit/'\n",
    "for filename in os.listdir(folder4):\n",
    "    infilename = os.path.join(folder4,filename)\n",
    "    if not os.path.isfile(infilename): \n",
    "        continue\n",
    "    oldbase = os.path.splitext(filename)\n",
    "    newname = infilename.replace('legit.txt', '.txt')\n",
    "    output = os.rename(infilename, newname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import fake news\n",
    "fake_news_1= glob.glob('fakeNewsDataset/fake/*.txt')\n",
    "fake_news_1 = pd.concat(pd.read_csv(file, header=None, sep='\\n', quoting=3, skip_blank_lines = True, encoding='utf-8').T for file in fake_news_1).reset_index(drop=True).fillna('')\n",
    "fake_news_1 = pd.DataFrame({'headline': fake_news_1[0], 'context': fake_news_1.loc[:, 1:3].apply(' '.join, axis=1)})\n",
    "fake_news_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_2 = glob.glob('celebrityDataset/fake/*.txt')\n",
    "fake_news_2 = pd.concat(pd.read_csv(file, header=None, sep='\\n', quoting=3, skip_blank_lines = True, encoding='utf-8').T for file in fake_news_2).reset_index(drop=True).fillna('')\n",
    "fake_news_2 = pd.DataFrame({'headline': fake_news_2[0], 'context': fake_news_2.loc[:, 1:3].apply(' '.join, axis=1)})\n",
    "fake_news_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legit_news_1 = glob.glob('fakeNewsDataset//legit/*.txt')\n",
    "legit_news_1 = pd.concat(pd.read_csv(file, header=None, sep='\\n', quoting=3, skip_blank_lines = True, encoding='utf-8').T for file in legit_news_1).reset_index(drop=True).fillna('')\n",
    "legit_news_1 = pd.DataFrame({'headline': legit_news_1[0], 'context': legit_news_1.loc[:, 1:3].apply(' '.join, axis=1)})\n",
    "legit_news_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legit_news_2 = glob.glob('celebrityDataset/legit//*.txt')\n",
    "legit_news_2 = pd.concat(pd.read_csv(file, header=None, sep='\\n', quoting=3, skip_blank_lines = True, encoding='utf-8').T for file in legit_news_2).reset_index(drop=True).fillna('')\n",
    "legit_news_2 = pd.DataFrame({'headline': legit_news_2[0], 'context': legit_news_2.loc[:, 1:3].apply(' '.join, axis=1)})\n",
    "legit_news_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## name entity recognition extraction\n",
    "\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "            else:\n",
    "                continue\n",
    "    return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "fake_news_1['label'], fake_news_2['label']='fake', 'fake'\n",
    "legit_news_1['label'], legit_news_2['label']='legit', 'legit'\n",
    "\n",
    "rada_data=pd.concat([fake_news_1, fake_news_2, legit_news_1, legit_news_2])\n",
    "rada_data['NER']=[get_continuous_chunks(x) for x in rada_data.context]\n",
    "\n",
    "binary_labels = {'fake':0, 'legit':1}\n",
    "\n",
    "def one_hot_label(label):\n",
    "    return to_categorical(multi_labels_dict[x], num_classes=6)\n",
    "\n",
    "rada_data['bi_label']=rada_data['label'].apply(lambda x: binary_labels[x])\n",
    "rada_data=shuffle(rada_data)\n",
    "\n",
    "## preprocessed data\n",
    "rada_data['word_ids'] = rada_data['context'].apply(pre_process_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train data on CNN model\n",
    "\n",
    "### tokenize fake news statement and build vocabulary\n",
    "vocab_dict={}\n",
    "\n",
    "tokenizer = Tokenizer(num_words=50000)\n",
    "tokenizer.fit_on_texts(all_fakenews['context'])\n",
    "vocab_dict=tokenizer.word_index\n",
    "cPickle.dump(tokenizer.word_index, open(\"vocab.p\",\"wb\"))\n",
    "print(\"vocab dictionary is created\")\n",
    "print(\"saved vocan dictionary to pickle file\")\n",
    "\n",
    "def pre_process_statement(statement):\n",
    "    text = text_to_word_sequence(statement)\n",
    "    val = [0] * 10\n",
    "    val = [vocab_dict[t] for t in text if t in vocab_dict] #Replace unk words with 0 index\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "val_size=0.2\n",
    "labels=to_categorical(rada_data['bi_label'], num_classes=2)\n",
    "random_state = np.random.randint(1000)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(rada_data, labels, test_size=val_size, random_state=random_state)\n",
    "\n",
    "x_train=x_train['word_ids']\n",
    "x_val=x_val['word_ids']\n",
    "X_train = sequence.pad_sequences(x_train, maxlen=32, padding='post',truncating='post')\n",
    "X_val = sequence.pad_sequences(x_val, maxlen=32, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parameter list\n",
    "\n",
    "## parameter list\n",
    "\n",
    "vocab_length = len(vocab_dict.keys())\n",
    "hidden_dims = 50 #Has to be same as EMBEDDING_DIM\n",
    "lstm_size = 100\n",
    "num_steps = 32\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "#Hyperparams for CNN\n",
    "kernel_sizes = [3,4,5]\n",
    "filter_size = 128\n",
    "#Meta data related hyper params\n",
    "# num_party = 6\n",
    "# num_state = 51\n",
    "# num_context = 12\n",
    "# num_job = 11\n",
    "# num_sub = 14\n",
    "# num_speaker = 21\n",
    "\n",
    "##\n",
    "max_features = len(tokenizer.word_index)+1\n",
    "Embedding_dims=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create embedding layer\n",
    "\n",
    "### add embedding layer\n",
    "num_words=len(vocab_dict)+1\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    embeddings_index = {}\n",
    "    f = open(gloveFile, encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-300])\n",
    "        coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "glove_model = loadGloveModel('glove.6B.300d.txt')\n",
    "\n",
    "def build_glove_embedding_layers():\n",
    "    embed_matrix=np.zeros((max_features, Embedding_dims))\n",
    "    for word, indx in tokenizer.word_index.items():\n",
    "        if indx >= max_features:\n",
    "            continue\n",
    "        if word in glove_model:\n",
    "            embed_vec=glove_model[word]\n",
    "            if embed_vec is not None:\n",
    "                embed_matrix[indx]=embed_vec\n",
    "    return embed_matrix\n",
    "\n",
    "embedding_weights=build_glove_embedding_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compile cnn model\n",
    "\n",
    "## keras dependencies\n",
    "\n",
    "from keras.layers import Concatenate, Input, MaxPooling1D\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences # To make vectors the same size. \n",
    "# from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, MaxPool1D, Conv2D\n",
    "from keras.layers import concatenate, Concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, CSVLogger\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import matplotlib.pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_arr = []\n",
    "statement_input = Input(shape=(32,), dtype='int32', name='main_input')\n",
    "x = Embedding(vocab_length+1,Embedding_dims,weights=[embedding_weights],input_length=32,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "# x = Embedding(output_dim=hidden_size, input_dim=vocab_length+1, input_length=num_steps)(statement_input) #Train embeddings from scratch\n",
    "\n",
    "for kernel in kernel_sizes:\n",
    "    x_1 = Conv1D(filters=filter_size,kernel_size=kernel, activation='relu')(x)\n",
    "    x_1 = GlobalMaxPool1D()(x_1)\n",
    "    kernel_arr.append(x_1)\n",
    "\n",
    "conv_in = concatenate(kernel_arr)\n",
    "conv_in = Dropout(0.6)(conv_in)\n",
    "conv_in = Dense(hidden_dims, activation='relu')(conv_in)\n",
    "conv_in = Dense(128, activation='relu')(conv_in)\n",
    "main_output = Dense(2, activation='softmax', name='main_output')(conv_in)\n",
    "\n",
    "model = Model(inputs=[statement_input], outputs=[main_output])\n",
    "model.summary()\n",
    "\n",
    "### compile CNN model\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.2)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "# tb = TensorBoard()\n",
    "# csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "# filepath= \"weights.best.hdf5\"\n",
    "# checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "#                                              monitor='val_categorical_accuracy', \n",
    "#                                              verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history= model.fit({'main_input': x_train},\n",
    "                   {'main_output': y_train},epochs=num_epochs, batch_size=batch_size,\n",
    "                   validation_data=(x_val,y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
