{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pos tagging for fake news statement\n",
    "## use pos-tagging to build features\n",
    "\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "## set up stanfordcorenlp server\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "# Importing the dataset\n",
    "dataset=pd.read_csv('train.tsv',delimiter='\\t',encoding='utf-8')\n",
    "dataset.columns=['statement_ID','label','statement','subject','speaker','job_title',\n",
    "           'state_info','pantry_affiliation','barely_true_cnt','false_cnt',\n",
    "           'half_true_cnt','mostly_true_cnt','pants_on_fire_cnt','context']\n",
    "\n",
    "\n",
    "dataset.head()\n",
    "\n",
    "total_rows = len(dataset.index)\n",
    "cols = list(dataset.columns.values)\n",
    "\n",
    "# Cleaning the texts\n",
    "corpus = []\n",
    "for i in range(0, total_rows):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['statement'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "train_data=corpus\n",
    "## define key-value\n",
    "data_dict=dict()\n",
    "\n",
    "data_dict['ids'] = train_data[['ID']].values[:,0]\n",
    "data_dict['labels'] = train_data[['Label']].values[:,0]\n",
    "data_dict['statements'] = train_data[['Statement']].values[:,0]\n",
    "data_dict['subjects'] = train_data[['Subject']].values[:,0]\n",
    "data_dict['contexts'] = train_data[['Context']].values[:,0]\n",
    "# Process each sentence\n",
    "# Obtain the Part Of Speech for every word in the statement\n",
    "# Get POS bigram way\n",
    "# Get POS trigram way\n",
    "\n",
    "word_pos_list = list()\n",
    "pos_list = list()\n",
    "unigram_pos_list=list()\n",
    "bigram_pos_list = list()\n",
    "trigram_pos_list = list()\n",
    "\n",
    "for sent_id, txt in enumerate(data_dict['text']):\n",
    "    print('Procesing sentence number {0}: {1}'.format(sent_id,txt))\n",
    "    output = nlp.annotate(txt, properties={\n",
    "      'annotators': 'tokenize,pos,parse',\n",
    "      'outputFormat': 'json'\n",
    "      })\n",
    "    result_word_pos = str()\n",
    "    result_pos = list()\n",
    "    unigram_pos=list()\n",
    "    bigram_pos = list()\n",
    "    trigram_pos = list()\n",
    "    for o in output['sentences']:\n",
    "        result_pos.append('<s>')\n",
    "        for t in o['tokens']:\n",
    "            result_word_pos += '{0}/{1} '.format(t['word'],t['pos'])\n",
    "            result_pos.append('{0}'.format(t['pos']))\n",
    "        for rpIndex, rp in enumerate(result_pos):\n",
    "            if rpIndex==len(result_pos):\n",
    "                unigram_pos.append('{0}').format(rp, result_pos[rpIndex+1])\n",
    "            if rpIndex < len(result_pos)-1 and len(result_pos)>=2:\n",
    "                bigram_pos.append('{0} {1}'.format(rp, result_pos[rpIndex+1]))\n",
    "            if rpIndex < len(result_pos)-2 and len(result_pos)>=3:\n",
    "                trigram_pos.append('{0} {1} {2}'.format(rp, result_pos[rpIndex+1], result_pos[rpIndex+2]))\n",
    "\n",
    "    word_pos_list.append(result_word_pos)\n",
    "    pos_list.append(result_pos)\n",
    "    unigram_pos_list.append(unigram_pos)\n",
    "    bigram_pos_list.append(bigram_pos)\n",
    "    trigram_pos_list.append(trigram_pos)\n",
    "\n",
    "##\n",
    "data_dict['word_pos'] = word_pos_list\n",
    "data_dict['pos'] = pos_list\n",
    "data_dict['unigram'] = unigram_pos_list\n",
    "data_dict['bigrams'] = bigram_pos_list\n",
    "data_dict['trigrams'] = trigram_pos_list\n",
    "\n",
    "##\n",
    "dataset['Word POS'] = data_dict['word_pos']\n",
    "dataset['POS'] = data_dict['pos']\n",
    "dataset['unigram_pos'] = data_dict['unigram']\n",
    "dataset['bigrams_pos'] = data_dict['bigrams']\n",
    "dataset['trigram_pos'] = data_dict['trigrams']\n",
    "\n",
    "## use countVectorizor to build postag output as features for training ML model\n",
    "cv = CountVectorizer()\n",
    "\n",
    "pos_uni_feats = cv.fit_transform(dataset['unigram_pos']).toarray()\n",
    "pos_big_feats = cv.fit_transform(dataset['bigram_pos']).toarray()\n",
    "pos_trig_feats = cv.fit_transform(dataset['trigram_pos']).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(pos_uni_feats,\n",
    "        dataset['class'],train_size=0.8, random_state=123)\n",
    "\n",
    "## train bigram_pos features on logistic regression\n",
    "LR = LogisticRegression()\n",
    "LR = LR.fit(X=X_train, y=y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "print(\"accuracy metrics for logistic regression classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "### train features on multinomial naive bayes classifier\n",
    "nb_cly=OneVsRestClassifier(MultinomialNB()).fit(X=X_train, y=y_train)\n",
    "y_pred=nb_cly.predict(X_test)\n",
    "print(\"accuracy metrics for Multinomial naive bayes classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "### train features on SVM\n",
    "\n",
    "svm_clf=SVC(C=1, kernel='rbf', degree=3, gamma='auto', random_state=None)\n",
    "svm_clf=svm_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=svm_clf.predict(X_test)\n",
    "print(\"accuracy metrics for support vector machine classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "### train features on SGD\n",
    "sgd_clf = linear_model.SGDClassifier(max_iter=1000)\n",
    "sgd_clf=sgd_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=sgd_clf.predict(X_test)\n",
    "print(\"accuracy metrics for stochastic gardient desent classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "### train features on random forest\n",
    "randFor_clf=RandomForestClassifier(n_estimators=2, criterion='gini', max_features='auto', class_weight={1:.9, 2:.5, 3:.01})\n",
    "randFor_clf=randFor_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=randFor_clf.predict(X_test)\n",
    "print(\"accuracy metrics for random forest classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "##----------- More compact solution for training several emprical ML classfier model -----------##\n",
    "### use kfold cross validation\n",
    "## Kforld cross validation on training set\n",
    "kf = KFold(len(X_train), numFolds=10, shuffle=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(pos_uni_feats,\n",
    "        dataset['class'],train_size=0.8, random_state=123)\n",
    "Y=dataset['class']\n",
    "\n",
    "params = [{}, {\"loss\": \"log\", \"penalty\": \"l2\", 'n_iter':1000},class_weight={1:.9, 2:.5, 3:.01}]\n",
    "Models = [LogisticRegression, SGDClassifier, MultinomialNB, RandomForestClassifier, SVC]\n",
    "\n",
    "for param, Model in zip(params, Models):\n",
    "    total = 0\n",
    "    for train_indices, test_indices in kf:\n",
    "        train_X = X_train[train_indices, :]; train_Y = y_train[train_indices]\n",
    "        test_X = X_test[test_indices, :]; test_Y = y_test[test_indices]\n",
    "        reg = Model(**param)\n",
    "        reg.fit(train_X, train_Y)\n",
    "        predictions = reg.predict(test_X)\n",
    "        total += accuracy_score(test_Y, predictions)\n",
    "\n",
    "    accuracy = total / numFolds\n",
    "    print(\"Accuracy score of {0}: {1}: {2}: {3}: {4}\".format(Model.__name__, accuracy))\n",
    "    print(\"accuracy metrics for {0}: {1}: {2}: {3}: {4}\".format(Model.__name__, classification_report(y_test, predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
