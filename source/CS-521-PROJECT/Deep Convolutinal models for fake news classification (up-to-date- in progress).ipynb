{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras.utils\n",
    "import nlp_util\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv1D, Dense, Input, Lambda, LSTM\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "import _pickle as cPickle\n",
    "\n",
    "from keras.layers import Concatenate, Input, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences # To make vectors the same size. \n",
    "# from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, MaxPool1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, CSVLogger, EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load dataset\n",
    "train_file=pd.read_csv('train.tsv', sep='\\t', header=None, encoding='utf-8')\n",
    "test_file=pd.read_csv('test.tsv', sep='\\t', header=None, encoding='utf-8')\n",
    "va_file=pd.read_csv('valid.tsv', sep='\\t', header=None, encoding='utf-8')\n",
    "\n",
    "column_names = ['Id', 'Label','Statement','Subject','Speaker','Speaker Job','State Info','Party','BT','FC','HT','MT','PF','Context']\n",
    "train_file.columns, test_file.columns, va_file.columns=column_names, column_names, column_names\n",
    "\n",
    "train_data = train_file[train_file.columns[~train_file.columns.isin(['Id','BT','FC','HT','MT','PF'])]]\n",
    "test_data = test_file[test_file.columns[~test_file.columns.isin(['Id','BT','FC','HT','MT','PF'])]]\n",
    "val_data = va_file[va_file.columns[~va_file.columns.isin(['Id','BT','FC','HT','MT','PF'])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jvret\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\jvret\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\jvret\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "multi_labels_dict = {'false':0, 'true':1,'pants-fire':2,'barely-true':3,'half-true':4,'mostly-true':5}\n",
    "binary_labels = {'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}\n",
    "\n",
    "\n",
    "def one_hot_label(label):\n",
    "    return to_categorical(multi_labels_dict[x], num_classes=6)\n",
    "\n",
    "train_data['multi_label']=train_data['Label'].apply(lambda x: multi_labels_dict[x])\n",
    "test_data['multi_label']=test_data['Label'].apply(lambda x: multi_labels_dict[x])\n",
    "val_data['multi_label']=val_data['Label'].apply(lambda x: multi_labels_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barack-obama': 0, 'donald-trump': 1, 'hillary-clinton': 2, 'mitt-romney': 3, 'scott-walker': 4, 'john-mccain': 5, 'rick-perry': 6, 'chain-email': 7, 'marco-rubio': 8, 'rick-scott': 9, 'ted-cruz': 10, 'bernie-s': 11, 'chris-christie': 12, 'facebook-posts': 13, 'charlie-crist': 14, 'newt-gingrich': 15, 'jeb-bush': 16, 'joe-biden': 17, 'blog-posting': 18, 'paul-ryan': 19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jvret\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\jvret\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\jvret\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "speakers= ['barack-obama', 'donald-trump', 'hillary-clinton', 'mitt-romney', \n",
    "                'scott-walker', 'john-mccain', 'rick-perry', 'chain-email', \n",
    "                'marco-rubio', 'rick-scott', 'ted-cruz', 'bernie-s', 'chris-christie', \n",
    "                'facebook-posts', 'charlie-crist', 'newt-gingrich', 'jeb-bush', \n",
    "                'joe-biden', 'blog-posting','paul-ryan']\n",
    "\n",
    "speaker_dict={}\n",
    "for cnt, speaker in enumerate(speakers):\n",
    "    speaker_dict[speaker]=cnt\n",
    "print(speaker_dict)\n",
    "\n",
    "def speaker_projection(speaker):\n",
    "    if isinstance(speaker, str):\n",
    "        speaker=speaker.lower()\n",
    "        matched=[s for s in speakers if s in speaker]\n",
    "        if len(matched)>0:\n",
    "            return speaker_dict[matched[0]]\n",
    "        else:\n",
    "            return len(speakers)\n",
    "        \n",
    "##\n",
    "train_data['speaker_id']=train_data['Speaker'].apply(speaker_projection)\n",
    "test_data['speaker_id']=test_data['Speaker'].apply(speaker_projection)\n",
    "val_data['speaker_id']=val_data['Speaker'].apply(speaker_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jvret\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\jvret\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\jvret\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "## Map job\n",
    "job_list = ['president', 'u.s. senator', 'governor', 'president-elect', 'presidential candidate', \n",
    "                'u.s. representative', 'state senator', 'attorney', 'state representative', 'congress', 'others']\n",
    "\n",
    "\n",
    "job_dict = {'president':0, 'u.s. senator':1, 'governor':2, 'president-elect':3, 'presidential candidate':4, \n",
    "            'u.s. representative':5, 'state senator':6, 'attorney':7, 'state representative':8, 'congress':9, 'others':10}\n",
    "\n",
    "## Map job\n",
    "\n",
    "def job_projection(job):\n",
    "    if isinstance(job, str):\n",
    "        job=job.lower()\n",
    "        matched_job=[j for j in job_list if j in job]\n",
    "        if len(matched_job)>0:\n",
    "            return job_dict[matched_job[0]]\n",
    "        else:\n",
    "            return 10\n",
    "    else:\n",
    "        return 10\n",
    "\n",
    "## job projection output\n",
    "\n",
    "train_data['job_id']=train_data['Speaker Job'].apply(job_projection)\n",
    "test_data['job_id']=test_data['Speaker Job'].apply(job_projection)\n",
    "val_data['job_id']=val_data['Speaker Job'].apply(job_projection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jvret\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\jvret\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\jvret\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "### Map political parties\n",
    "party_dict={'republican':0, 'democrat':1, 'none':2, 'organization':3, 'newsmaker':4, 'rest':5}\n",
    "\n",
    "def map_political_party(party):\n",
    "    if party in party_dict:\n",
    "        return party_dict[party]\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "##\n",
    "train_data['party_id']=train_data['Party'].apply(map_political_party)\n",
    "test_data['party_id']=test_data['Party'].apply(map_political_party)\n",
    "val_data['party_id']=val_data['Party'].apply(map_political_party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jvret\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\jvret\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\jvret\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "## Map states\n",
    "all_states = ['Alabama','Alaska','Arizona','Arkansas','California','Colorado',\n",
    "              'Connecticut','Delaware','Florida','Georgia','Hawaii','Idaho', \n",
    "              'Illinois','Indiana','Iowa','Kansas','Kentucky','Louisiana',\n",
    "              'Maine' 'Maryland','Massachusetts','Michigan','Minnesota',\n",
    "              'Mississippi', 'Missouri','Montana','Nebraska','Nevada',\n",
    "              'New Hampshire','New Jersey','New Mexico','New York',\n",
    "              'North Carolina','North Dakota','Ohio',    \n",
    "              'Oklahoma','Oregon','Pennsylvania','Rhode Island',\n",
    "              'South  Carolina','South Dakota','Tennessee','Texas','Utah',\n",
    "              'Vermont','Virginia','Washington','West Virginia',\n",
    "              'Wisconsin','Wyoming']\n",
    "\n",
    "\n",
    "states_dict = {'wyoming': 48, 'colorado': 5, 'washington': 45, 'hawaii': 10, \n",
    "               'tennessee': 40, 'wisconsin': 47, 'nevada': 26, 'north dakota': 32, \n",
    "               'mississippi': 22, 'south dakota': 39, 'new jersey': 28, 'oklahoma': 34, \n",
    "               'delaware': 7, 'minnesota': 21, 'north carolina': 31, 'illinois': 12, \n",
    "               'new york': 30, 'arkansas': 3, 'west virginia': 46, 'indiana': 13, \n",
    "               'louisiana': 17, 'idaho': 11, 'south  carolina': 38, 'arizona': 2, \n",
    "               'iowa': 14, 'mainemaryland': 18, 'michigan': 20, 'kansas': 15, \n",
    "               'utah': 42, 'virginia': 44, 'oregon': 35, 'connecticut': 6, 'montana': 24, \n",
    "               'california': 4, 'massachusetts': 19, 'rhode island': 37, 'vermont': 43, \n",
    "               'georgia': 9, 'pennsylvania': 36, 'florida': 8, 'alaska': 1, 'kentucky': 16,\n",
    "               'nebraska': 25, 'new hampshire': 27, 'texas': 41, 'missouri': 23, 'ohio': 33,\n",
    "               'alabama': 0, 'new mexico': 29, 'rest':50}\n",
    "\n",
    "\n",
    "def state_projection(state):\n",
    "    if isinstance(state, str):\n",
    "        state=state.lower()\n",
    "        if state in states_dict:\n",
    "            return states_dict[state]\n",
    "        else:\n",
    "            if 'washington' in state:\n",
    "                return states_dict['washington']\n",
    "            else:\n",
    "                return 50\n",
    "    else:\n",
    "        return 50\n",
    "    \n",
    "## state mapping output:\n",
    "train_data['state_id']=train_data['State Info'].apply(state_projection)\n",
    "test_data['state_id']=test_data['State Info'].apply(state_projection)\n",
    "val_data['state_id']=val_data['State Info'].apply(state_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## map subject\n",
    "subject_list = ['health','tax','immigration','election','education',\n",
    "    'candidates-biography','economy','gun','jobs','federal-budget','energy','abortion','foreign-policy']\n",
    "\n",
    "subject_dict = {'health':0,'tax':1,'immigration':2,'election':3,'education':4,\n",
    "                'candidates-biography':5,'economy':6,'gun':7,'jobs':8,'federal-budget':9,\n",
    "                'energy':10,'abortion':11,'foreign-policy':12, 'others':13}\n",
    "\n",
    "## mapping subject\n",
    "def subject_projection(subject):\n",
    "    if isinstance(subject, str):\n",
    "        subject=subject.lower()\n",
    "        matched_subject=[subj for subj in subject_list if subj in subject]\n",
    "        \n",
    "        if len(matched_subject)>0:\n",
    "            return subject_dict[matched_subject[0]]\n",
    "        else:\n",
    "            return 13\n",
    "    else:\n",
    "        return 13\n",
    "    \n",
    "##\n",
    "train_data['subject_id']=train_data['Subject'].apply(subject_projection)\n",
    "test_data['subject_id']=test_data['Subject'].apply(subject_projection)\n",
    "val_data['subject_id']=val_data['Subject'].apply(subject_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Context mapping\n",
    "Context_list = ['news release','interview','tv','radio',\n",
    "                'campaign','news conference','press conference','press release',\n",
    "                'tweet','facebook','email']\n",
    "\n",
    "Context_dict = {'news release':0,'interview':1,'tv':2,'radio':3,\n",
    "                'campaign':4,'news conference':5,'press conference':6,'press release':7,\n",
    "                'tweet':8,'facebook':9,'email':10, 'others':11}\n",
    "\n",
    "def Context_projection(context):\n",
    "    if isinstance(context, str):\n",
    "        context=context.lower()\n",
    "        matched_context=[cntx for cntx in Context_list if cntx in context]\n",
    "        if len(matched_context)>0:\n",
    "            return Context_dict[matched_context[0]]\n",
    "        else:\n",
    "            return 11\n",
    "    else:\n",
    "        return 11\n",
    "    \n",
    "## context projection output\n",
    "train_data['context_id']=train_data['Context'].apply(Context_projection)\n",
    "test_data['context_id']=test_data['Context'].apply(Context_projection)\n",
    "val_data['context_id']=val_data['Context'].apply(Context_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab dictionary is created\n",
      "saved vocan dictionary to pickle file\n"
     ]
    }
   ],
   "source": [
    "### tokenize fake news statement and build vocabulary\n",
    "vocab_dict={}\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(train_data['Statement'])\n",
    "vocab_dict=tokenizer.word_index\n",
    "cPickle.dump(tokenizer.word_index, open(\"vocab.p\",\"wb\"))\n",
    "print(\"vocab dictionary is created\")\n",
    "print(\"saved vocan dictionary to pickle file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data preprocessing\n",
    "\n",
    "# def preprocessing_txt(dataset):\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     corpus=[]\n",
    "#     for elm in range(0, len(dataset.index)):\n",
    "#         res=' '.join([i for i in dataset['Statement'][elm].lower().split() if i not in stop_words])\n",
    "#         res=re.sub(\"</?.*?>\",\" <> \",dataset['Statement'][elm])    # remove tags\n",
    "#         res=re.sub(\"(\\\\d|\\\\W)+\",\" \",dataset['Statement'][elm])        # remove special characte\n",
    "#         res=re.sub(r'@([A-Za-z0-9_]+)', \"\",dataset['Statement'][elm])  # remove twitter handler\n",
    "#         res=re.sub('(\\r)+', \"\", dataset['Statement'][elm])            # remove newline character\n",
    "#         res=re.sub('[^\\x00-\\x7F]+', \"\", dataset['Statement'][elm])    # remove non-ascii characters\n",
    "#         res=''.join(x for x in dataset['Statement'][elm] if x not in set(string.punctuation))   ## remove punctuation\n",
    "#         corpus.append(res)\n",
    "#     return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    vocab_length = len(vocab_dict.keys())\n",
    "    hidden_size = 200 #Has to be same as EMBEDDING_DIM\n",
    "    lstm_size = 100\n",
    "    num_steps = 50\n",
    "    num_epochs = 30\n",
    "    batch_size = 64\n",
    "    #Hyperparams for CNN\n",
    "    kernel_sizes = [2,3,5,7]\n",
    "    filter_size = 128\n",
    "    #Meta data related hyper params\n",
    "    num_party = 6\n",
    "    num_state = 51\n",
    "    num_context = 12\n",
    "    num_job = 11\n",
    "    num_sub = 14\n",
    "    num_speaker = 21\n",
    "    embedding_dims=300\n",
    "    max_features = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    }
   ],
   "source": [
    "### create embedding layer\n",
    "num_words=len(vocab_dict)+1\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    embeddings_index = {}\n",
    "    f = open(gloveFile, encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-300])\n",
    "        coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "glove_model = loadGloveModel('glove.6B.300d.txt')\n",
    "\n",
    "def build_glove_embedding_layers():\n",
    "    embed_matrix=np.zeros((max_features, embedding_dims))\n",
    "    for word, indx in tokenizer.word_index.items():\n",
    "        if indx >= max_features:\n",
    "            continue\n",
    "        if word in glove_model:\n",
    "            embed_vec=glove_model[word]\n",
    "            if embed_vec is not None:\n",
    "                embed_matrix[indx]=embed_vec\n",
    "    return embed_matrix\n",
    "\n",
    "embedding_weights=build_glove_embedding_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data preprocessing\n",
    "def preprocessing_txt_keras(statement):\n",
    "    txt=text_to_word_sequence(statement)\n",
    "    val=[0]*64\n",
    "    val=[vocab_dict[t] for t in txt if t in vocab_dict]   ##replace unknown words with zero index\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training instances list\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "### remove stopwords fitst\n",
    "train_data['Statement'] = list(map(' '.join, train_data['Statement'].apply(lambda x: [item for item in x.lower().split() if item not in stop])))\n",
    "test_data['Statement'] = list(map(' '.join, test_data['Statement'].apply(lambda x: [item for item in x.lower().split() if item not in stop])))\n",
    "val_data['Statement'] = list(map(' '.join, val_data['Statement'].apply(lambda x: [item for item in x.lower().split() if item not in stop])))\n",
    "\n",
    "train_data['word_id']=train_data['Statement'].apply(preprocessing_txt_keras)\n",
    "test_data['word_id']=test_data['Statement'].apply(preprocessing_txt_keras)\n",
    "val_data['word_id']=val_data['Statement'].apply(preprocessing_txt_keras)\n",
    "\n",
    "x_train=train_data['word_id']\n",
    "x_test=test_data['word_id']\n",
    "x_val=val_data['word_id']\n",
    "\n",
    "y_train=train_data['multi_label']\n",
    "y_val=val_data['multi_label']\n",
    "\n",
    "##\n",
    "x_train=sequence.pad_sequences(x_train, maxlen=num_steps, padding='post', truncating='post')\n",
    "y_train=to_categorical(y_train, num_classes=6)\n",
    "x_val=sequence.pad_sequences(x_val, maxlen=num_steps, padding='post', truncating='post')\n",
    "y_val=to_categorical(y_val, num_classes=6)\n",
    "x_test=sequence.pad_sequences(x_test, maxlen=num_steps, padding='post', truncating='post')\n",
    "\n",
    "## meta data preparation\n",
    "tr_party=to_categorical(train_data['party_id'], num_classes=num_party)\n",
    "tr_state=to_categorical(train_data['state_id'], num_classes=num_state)\n",
    "tr_cont=to_categorical(train_data['context_id'], num_classes=num_context)\n",
    "tr_job=to_categorical(train_data['job_id'], num_classes=num_job)\n",
    "tr_subj=to_categorical(train_data['subject_id'], num_classes=num_sub)\n",
    "# tr_speaker=to_categorical(train_data['speaker_id'], num_classes=num_speaker)\n",
    "\n",
    "# ## put all metadata of train data together in one stack\n",
    "# x_train_metadata=np.hstack(tr_party, tr_state, tr_job, tr_subj, tr_speaker, tr_cont)\n",
    "x_train_metadata=np.hstack((tr_party, tr_state, tr_cont,tr_job, tr_subj))\n",
    "\n",
    "\n",
    "# #********************************************************************************#\n",
    "val_party=to_categorical(val_data['party_id'], num_classes=num_party)\n",
    "val_state=to_categorical(val_data['state_id'], num_classes=num_state)\n",
    "val_cont=to_categorical(val_data['context_id'], num_classes=num_context)\n",
    "val_job=to_categorical(val_data['job_id'], num_classes=num_job)\n",
    "val_subj=to_categorical(val_data['subject_id'], num_classes=num_sub)\n",
    "# val_speaker=to_categorical(val_data['speaker_id'], num_classes=num_speaker)\n",
    "\n",
    "\n",
    "# ## put all metadata of train data together in one stack\n",
    "# x_val_metadata=np.hstack(val_party, val_state, val_job, val_subj, val_speaker, val_cont)\n",
    "x_val_metadata=np.hstack((val_party, val_state, val_cont,val_job, val_subj, ))\n",
    "\n",
    "# #********************************************************************************#\n",
    "te_party=to_categorical(test_data['party_id'], num_classes=num_party)\n",
    "te_state=to_categorical(test_data['state_id'], num_classes=num_state)\n",
    "te_cont=to_categorical(test_data['context_id'], num_classes=num_context)\n",
    "te_job=to_categorical(test_data['job_id'], num_classes=num_job)\n",
    "te_subj=to_categorical(test_data['subject_id'], num_classes=num_sub)\n",
    "# te_speaker=to_categorical(test_data['speaker_id'], num_classes=num_speaker)\n",
    "\n",
    "# ## put all metadata of train data together in one stack\n",
    "x_test_metadata=np.hstack((te_party, te_state, te_cont,te_job, te_subj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 50, 300)      3722700     main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 49, 128)      76928       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 48, 128)      115328      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 46, 128)      192128      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 44, 128)      268928      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 49, 128)      0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 48, 128)      0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 46, 128)      0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 44, 128)      0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (Global (None, 128)          0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_26 (Global (None, 128)          0           dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_27 (Global (None, 128)          0           dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_28 (Global (None, 128)          0           dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 512)          0           global_max_pooling1d_25[0][0]    \n",
      "                                                                 global_max_pooling1d_26[0][0]    \n",
      "                                                                 global_max_pooling1d_27[0][0]    \n",
      "                                                                 global_max_pooling1d_28[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 512)          0           concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "aux_input (InputLayer)          (None, 94)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 100)          51300       dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 64)           6080        aux_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 164)          0           dense_13[0][0]                   \n",
      "                                                                 dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 6)            990         concatenate_14[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 4,434,382\n",
      "Trainable params: 711,682\n",
      "Non-trainable params: 3,722,700\n",
      "__________________________________________________________________________________________________\n",
      "Train on 10240 samples, validate on 1284 samples\n",
      "Epoch 1/20\n",
      "10240/10240 [==============================] - 51s 5ms/step - loss: 1.8157 - categorical_accuracy: 0.2044 - val_loss: 1.7490 - val_categorical_accuracy: 0.2321\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.23209, saving model to weights.best.hdf5\n",
      "Epoch 2/20\n",
      "10240/10240 [==============================] - 47s 5ms/step - loss: 1.7314 - categorical_accuracy: 0.2314 - val_loss: 1.7299 - val_categorical_accuracy: 0.2321\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.23209\n",
      "Epoch 3/20\n",
      "10240/10240 [==============================] - 50s 5ms/step - loss: 1.7195 - categorical_accuracy: 0.2398 - val_loss: 1.7161 - val_categorical_accuracy: 0.2477\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.23209 to 0.24766, saving model to weights.best.hdf5\n",
      "Epoch 4/20\n",
      "10240/10240 [==============================] - 48s 5ms/step - loss: 1.7092 - categorical_accuracy: 0.2498 - val_loss: 1.7101 - val_categorical_accuracy: 0.2578\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.24766 to 0.25779, saving model to weights.best.hdf5\n",
      "Epoch 5/20\n",
      "10240/10240 [==============================] - 49s 5ms/step - loss: 1.7029 - categorical_accuracy: 0.2577 - val_loss: 1.7097 - val_categorical_accuracy: 0.2609\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.25779 to 0.26090, saving model to weights.best.hdf5\n",
      "Epoch 6/20\n",
      "10240/10240 [==============================] - 51s 5ms/step - loss: 1.6941 - categorical_accuracy: 0.2593 - val_loss: 1.7070 - val_categorical_accuracy: 0.2593\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.26090\n",
      "Epoch 7/20\n",
      "10240/10240 [==============================] - 51s 5ms/step - loss: 1.6887 - categorical_accuracy: 0.2644 - val_loss: 1.7015 - val_categorical_accuracy: 0.2617\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.26090 to 0.26168, saving model to weights.best.hdf5\n",
      "Epoch 8/20\n",
      "10240/10240 [==============================] - 55s 5ms/step - loss: 1.6849 - categorical_accuracy: 0.2679 - val_loss: 1.7011 - val_categorical_accuracy: 0.2648\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.26168 to 0.26480, saving model to weights.best.hdf5\n",
      "Epoch 9/20\n",
      "10240/10240 [==============================] - 50s 5ms/step - loss: 1.6794 - categorical_accuracy: 0.2672 - val_loss: 1.7018 - val_categorical_accuracy: 0.2484\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.26480\n",
      "Epoch 10/20\n",
      "10240/10240 [==============================] - 49s 5ms/step - loss: 1.6707 - categorical_accuracy: 0.2790 - val_loss: 1.7058 - val_categorical_accuracy: 0.2586\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.26480\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "kernel_arr = []\n",
    "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "x = Embedding(vocab_length+1,embedding_dims,weights=[embedding_weights],input_length=num_steps,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "# x = Embedding(output_dim=hidden_size, input_dim=vocab_length+1, input_length=num_steps)(statement_input) #Train embeddings from scratch\n",
    "\n",
    "for kernel in kernel_sizes:\n",
    "    x_1 = Conv1D(filters=filter_size,kernel_size=kernel)(x)\n",
    "    x_1 = Dropout(0.6)(x_1)\n",
    "    x_1 = GlobalMaxPool1D()(x_1)\n",
    "    kernel_arr.append(x_1)\n",
    "\n",
    "conv_in = keras.layers.concatenate(kernel_arr)\n",
    "conv_in = Dropout(0.6)(conv_in)\n",
    "conv_in = Dense(100, activation='relu')(conv_in)\n",
    "\n",
    "#Meta input\n",
    "meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "x_meta = Dense(64, activation='relu')(meta_input)\n",
    "x = keras.layers.concatenate([conv_in, x_meta])\n",
    "\n",
    "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
    "model = Model(inputs=[statement_input, meta_input], outputs=[main_output])\n",
    "\n",
    "#************************************************************************#\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.2)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "filepath= \"weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history= model.fit({'main_input': x_train, 'aux_input': x_train_metadata},\n",
    "                   {'main_output': y_train},epochs=20, batch_size=batch_size,\n",
    "                   validation_data=({'main_input': x_val, 'aux_input': x_val_metadata},{'main_output': y_val}),\n",
    "                  callbacks=[tb,csv_logger,checkpoint, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accu_curve=plt.figure()\n",
    "plt.plot(history.history['categorical_accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_categorical_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Accuracy',fontsize=12)\n",
    "plt.title('Accuracy Curves : CNN',fontsize=12)\n",
    "accu_curve.savefig('accuracy_cnn_improved.png')\n",
    "plt.show()\n",
    "##^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##\n",
    "loss_curve = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Loss',fontsize=12)\n",
    "plt.title('Loss Curves :CNN',fontsize=12)\n",
    "loss_curve.savefig('loss_cnn_improved.png')\n",
    "loss_curve.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1267/1267 [==============================] - 2s 2ms/step\n",
      "[[0.2339262  0.08675115 0.18008143 0.18238442 0.19402519 0.12283161]\n",
      " [0.21698949 0.14054851 0.04024055 0.15862547 0.21648441 0.22711153]\n",
      " [0.37754112 0.05388047 0.2438285  0.18233532 0.09576442 0.04665018]\n",
      " ...\n",
      " [0.26225013 0.11336583 0.07458698 0.17128097 0.17038243 0.20813368]\n",
      " [0.15930183 0.23672773 0.06315657 0.1558833  0.23714441 0.14778607]\n",
      " [0.21353503 0.12819347 0.03141575 0.15193172 0.2270546  0.24786939]]\n"
     ]
    }
   ],
   "source": [
    "### Make prediction\n",
    "from keras.models import load_model\n",
    "#Load a pre-trained model if any\n",
    "model1 = load_model('weights.best.hdf5')\n",
    "preds = model1.predict([x_test,x_test_metadata], batch_size=batch_size, verbose=1)\n",
    "print(np.array(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils.np_utils import accuracy\n",
    "# acc = accuracy(y_test, np.round(np.array(preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  C-LSTM model for fake news classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## C-LSTM model implementation (trail version)\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.2)\n",
    "\n",
    "\n",
    "kernel_arr = []\n",
    "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "x = Embedding(vocab_length+1,embedding_dims,weights=[embedding_weights],input_length=num_steps,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "# x = Embedding(output_dim=hidden_size, input_dim=vocab_length+1, input_length=num_steps)(statement_input) #Train embeddings from scratch\n",
    "\n",
    "for kernel in kernel_sizes:\n",
    "    x_1 = Conv1D(filters=filter_size,kernel_size=kernel)(x)\n",
    "    x_1 = GlobalMaxPool1D()(x_1)\n",
    "    x_1 = Flatten()(x_1)\n",
    "    #x_1 = Dropout(0.8)(x_1)\n",
    "    kernel_arr.append(x_1)\n",
    "    \n",
    "cnn_feats_maps = keras.layers.concatenate(kernel_arr)\n",
    "sent_encoder = LSTM(lstm_size,dropout=0.25)(cnn_feats_maps)\n",
    "# sent_encoder = Bidirectional(LSTM(128, return_sequences=False))(cnn_feats_maps)\n",
    "fc_layer = Dense(128, activation=\"relu\")(sent_encoder)\n",
    "conv_in = Dropout(0.6)(fc_layer)\n",
    "conv_in = Dense(128, activation='relu')(conv_in)\n",
    "\n",
    "###Meta input\n",
    "meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "x_meta_ = Dense(64, activation='relu')(meta_input)\n",
    "x_ = keras.layers.concatenate([conv_in, x_meta_])\n",
    "\n",
    "### Define model\n",
    "main_output = Dense(6, activation='softmax', name='main_output')(x_)\n",
    "model_CLSTM = Model(inputs=[statement_input, meta_input], outputs=[main_output])\n",
    "\n",
    "## compile model\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.2)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_CLSTM.compile(optimizer=sgd,\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "model_CLSTM.summary()\n",
    "\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "filepath= \"weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history_clstm= model_CLSTM.fit({'main_input': x_train, 'aux_input': x_train_metadata},\n",
    "                               {'main_output': y_train},epochs=num_epochs, batch_size=batch_size,\n",
    "                               validation_data=({'main_input': x_val, 'aux_input': x_val_metadata},{'main_output': y_val}),\n",
    "                               callbacks=[tb,csv_logger,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character level CNN model for fake news classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense, Concatenate, Embedding, Flatten\n",
    "from keras.layers import AlphaDropout\n",
    "from keras.layers import ThresholdedReLU\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameter list\n",
    "conv_layers=[[256,7,3],\n",
    "            [256,7,3],\n",
    "            [256,7,None],\n",
    "            [256,7,None],\n",
    "            [256,7,None],\n",
    "            [256,7,3]]\n",
    "\n",
    "fully_connected_layers=[1024,1024]\n",
    "\n",
    "nb_class=6\n",
    "input_size=32\n",
    "alphabet_size=69\n",
    "embedding_size=64\n",
    "droprate=0.4\n",
    "threshold=1e-6\n",
    "\n",
    "## build CNN model with character embedding\n",
    "\n",
    "sequence_input = Input(shape=(input_size,), name='sent_input', dtype='int64')\n",
    "x = Embedding(alphabet_size+1, embedding_size, input_length=input_size)(sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from math import sqrt\n",
    "\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 69 from 1 for 'conv2d_14/convolution' (op: 'Conv2D') with input shapes: [?,69,1,32], [7,69,32,256].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1628\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1629\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 69 from 1 for 'conv2d_14/convolution' (op: 'Conv2D') with input shapes: [?,69,1,32], [7,69,32,256].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-ab4572a3ee89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     x=layers.Conv2D(filters=cl[0],\n\u001b[0;32m     17\u001b[0m                    \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                    kernel_initializer='random_uniform')(x)\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mstdval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[0;32m    172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[0;32m   3648\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3649\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3650\u001b[1;33m         data_format=tf_data_format)\n\u001b[0m\u001b[0;32m   3651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'channels_first'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtf_data_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'NHWC'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution\u001b[1;34m(input, filter, padding, strides, dilation_rate, name, data_format)\u001b[0m\n\u001b[0;32m    778\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         data_format=data_format)\n\u001b[1;32m--> 780\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         name=self.name)\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         \u001b[1;34m\"Conv2D\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[0;32m   1044\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    788\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m                 instructions)\n\u001b[1;32m--> 488\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[0;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3272\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3273\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3274\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3276\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1790\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1791\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1792\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1794\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative dimension size caused by subtracting 69 from 1 for 'conv2d_14/convolution' (op: 'Conv2D') with input shapes: [?,69,1,32], [7,69,32,256]."
     ]
    }
   ],
   "source": [
    "# convs=[]\n",
    "# for nm_filters, filter_width in conv_layers:\n",
    "#     conv_in=Convolution1D(filters=nm_filters,\n",
    "#                           kernel_size=filter_width,\n",
    "#                           activation='tanh')(x)\n",
    "#     l_pool = GlobalMaxPool1D()(conv_in)\n",
    "#     l_flatten = Flatten(l_pool)\n",
    "#     convs.append(l_flatten)\n",
    "\n",
    "conc_l=tf.concat([tf.zeros([1, alphabet_size]), \n",
    "                  tf.one_hot(list(range(alphabet_size)), alphabet_size, 1.0, 0.0)],0,  name='conc_l')\n",
    "x=layers.Lambda(lambda i: K.gather(conc_l, K.cast(i, 'int64')))(sequence_input)\n",
    "x=layers.Lambda(lambda i: K.expand_dims(i,-1))(x)\n",
    "\n",
    "for cl in conv_layers:\n",
    "    x=layers.Conv2D(filters=cl[0],\n",
    "                   kernel_size=(cl[1], x.get_shape()[2].value),\n",
    "                   kernel_initializer='random_uniform')(x)\n",
    "    \n",
    "    stdval=1/(sqrt(cl[0]*cl[1]))\n",
    "    b=K.random_uniform([cl[0]], minval=-stdval, maxval=stdval)\n",
    "    x=layers.Lambda(lambda i: K.bias_add(i,b))(x)\n",
    "    x=layers.ThresholdedReLU(threshold)(x)\n",
    "    if cl[-1] is not None:\n",
    "        x=layers.MaxPool2D(pool_size=(cl[-1],1))(x)\n",
    "    x=layers.Lambda(lambda i: K.permute_dimensions(i, [0,1,3,2]))(x)\n",
    "    \n",
    "x=Flatten()(x)\n",
    "for fl in fully_connected_layers:\n",
    "    x=layers.Dense(fl)(x)\n",
    "    x=ThresholdedReLU(threshold)(x)\n",
    "    x=Dropout(droprate)(x)\n",
    "\n",
    "model_output=Dense(6, activation='softmax')(x)\n",
    "Model_charcnn=Model(inputs=sequence_input, outputs=model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.2)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "charcnn_model.compile(optimizer=sgd,\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['categorical_accuracy'])\n",
    "\n",
    "charcnn_model.summary()\n",
    "\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "filepath= \"weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history_charcnn= charcnn_model.fit({'main_input': x_train, 'aux_input': x_train_metadata},\n",
    "                                   {'main_output': y_train},epochs=num_epochs, batch_size=batch_size,\n",
    "                                   validation_data=({'main_input': x_val, 'aux_input': x_val_metadata},{'main_output': y_val}),\n",
    "                                   callbacks=[tb,csv_logger,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### character level classification (simple version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers=[[256, 7, 3],\n",
    "             [256, 7, 3],\n",
    "             [256, 3, -1],\n",
    "             [256, 3, -1],\n",
    "             [256, 3, -1],\n",
    "             [256, 3, 3]]\n",
    "\n",
    "fully_connected_layers=[1024, 1024]\n",
    "dropout_p=0.5\n",
    "threshold=1e-06\n",
    "input_size=300\n",
    "alphabet='abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:\\'\"/\\\\|_@#$%^&*~`+-=<>()[]{}'\n",
    "batch_size=128\n",
    "checkpoint_every=100\n",
    "epochs=5000\n",
    "evaluate_every=100\n",
    "batch_size=128\n",
    "checkpoint_every=100\n",
    "epochs=5000\n",
    "evaluate_every=100\n",
    "embedding_size=128\n",
    "alphabet_size=69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sent_input (InputLayer)      (None, 1014)              0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 1014, 128)         8960      \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1008, 256)         229632    \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1008, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 336, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 330, 256)          459008    \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 330, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 110, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 108, 256)          196864    \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 108, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 106, 256)          196864    \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 106, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 104, 256)          196864    \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 104, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 102, 256)          196864    \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 102, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 34, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 8704)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1024)              8913920   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 6)                 6150      \n",
      "=================================================================\n",
      "Total params: 11,454,726\n",
      "Trainable params: 11,454,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(input_size,), name='sent_input', dtype='int64')  # shape=(?, 1014)\n",
    "# Embedding layer\n",
    "conv = Embedding(alphabet_size+1, embedding_size, input_length=input_size)(inputs)\n",
    "# Conv \n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    conv = Conv1D(filter_num, filter_size)(conv) \n",
    "    conv = Activation('relu')(conv)\n",
    "    if pooling_size != -1:\n",
    "        conv = MaxPooling1D(pool_size=pooling_size)(conv) # Final shape=(None, 34, 256)\n",
    "x = Flatten()(conv) # (None, 8704)\n",
    "# Fully connected layers \n",
    "for dense_size in fully_connected_layers:\n",
    "    x = Dense(dense_size, activation='relu')(x) # dense_size == 1024\n",
    "    x = Dropout(dropout_p)(x)\n",
    "model_output = Dense(6, activation='softmax')(x)\n",
    "# Build model\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.2)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# model.compile(optimizer=adam, loss=loss) # Adam, categorical_crossentropy\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent CNN model for fake news classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.syn0.shape[0]\n",
    "word2vec.syn0.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = gensim.models.Word2Vec.load(\"word2vec.gensim\")\n",
    "embeddings = np.zeros((word2vec.syn0.shape[0] + 1, word2vec.syn0.shape[1]), dtype = \"float32\")\n",
    "embeddings[:word2vec.syn0.shape[0]] = word2vec.syn0\n",
    "\n",
    "MAX_TOKENS = word2vec.syn0.shape[0]\n",
    "embedding_dim = word2vec.syn0.shape[1]\n",
    "hidden_dim_1 = 200\n",
    "hidden_dim_2 = 100\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "document = Input(shape = (None, ), dtype = \"int32\")\n",
    "left_context = Input(shape = (None, ), dtype = \"int32\")\n",
    "right_context = Input(shape = (None, ), dtype = \"int32\")\n",
    "\n",
    "embedder = Embedding(MAX_TOKENS + 1, embedding_dim, weights = [embeddings], trainable = False)\n",
    "doc_embedding = embedder(document)\n",
    "l_embedding = embedder(left_context)\n",
    "r_embedding = embedder(right_context)\n",
    "\n",
    "forward = LSTM(hidden_dim_1, return_sequences = True)(l_embedding)\n",
    "backward = LSTM(hidden_dim_1, return_sequences = True, go_backwards = True)(r_embedding)\n",
    "\n",
    "# Keras returns the output sequences in reverse order.\n",
    "backward = Lambda(lambda x: backend.reverse(x, axes = 1))(backward)\n",
    "together = concatenate([forward, doc_embedding, backward], axis = 2)\n",
    "\n",
    "semantic = Conv1D(hidden_dim_2, kernel_size = 1, activation = \"tanh\")(together)\n",
    "\n",
    "## define customized maxpooling layer\n",
    "pool_rnn = Lambda(lambda x: backend.max(x, axis = 1), output_shape = (hidden_dim_2, ))(semantic)\n",
    "model_output = Dense(NUM_CLASSES, input_dim = hidden_dim_2, activation = \"softmax\")(pool_rnn)\n",
    "\n",
    "## define and compile RCNN mode\n",
    "RCNN_model = Model(inputs = [document, left_context, right_context], outputs = model_output)\n",
    "model.compile(optimizer = \"adadelta\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.2)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "RCNN_model.compile(optimizer=sgd,\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['categorical_accuracy'])\n",
    "\n",
    "RCNN_model.summary()\n",
    "\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "filepath= \"weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history_rcnn= RCNN_model.fit({'main_input': x_train, 'aux_input': x_train_metadata},\n",
    "                             {'main_output': y_train},epochs=num_epochs, batch_size=batch_size,\n",
    "                             validation_data=({'main_input': x_val, 'aux_input': x_val_metadata},{'main_output': y_val}),\n",
    "                             callbacks=[tb,csv_logger,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is some example text.\"\n",
    "text = text.strip().lower().translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "tokens = text.split()\n",
    "tokens = [word2vec.vocab[token].index if token in word2vec.vocab else MAX_TOKENS for token in tokens]\n",
    "\n",
    "doc_as_array = np.array([tokens])\n",
    "# We shift the document to the right to obtain the left-side contexts.\n",
    "left_context_as_array = np.array([[MAX_TOKENS] + tokens[:-1]])\n",
    "# We shift the document to the left to obtain the right-side contexts.\n",
    "right_context_as_array = np.array([tokens[1:] + [MAX_TOKENS]])\n",
    "\n",
    "target = np.array([NUM_CLASSES * [0]])\n",
    "target[0][3] = 1\n",
    "\n",
    "history = model.fit([doc_as_array, left_context_as_array, right_context_as_array], target, epochs = 1, verbose = 0)\n",
    "loss = history.history[\"loss\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
